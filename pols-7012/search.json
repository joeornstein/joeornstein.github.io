{
  "articles": [
    {
      "path": "index.html",
      "title": "Introduction to Political Methodology",
      "description": "How to learn from data\n",
      "author": [
        {
          "name": "Joe Ornstein",
          "url": "https://joeornstein.github.io/"
        }
      ],
      "contents": "\r\n\r\nContents\r\nOverview\r\nGetting\r\nStarted\r\nData Analysis (R and\r\nRStudio)\r\nOPTIONAL: Version Control\r\n(git)\r\nOPTIONAL: Reference\r\nManagement (Zotero)\r\nOPTIONAL: Social\r\nAnnotation (Hypothesis)\r\nOPTIONAL: Memory Aid (Anki)\r\nOPTIONAL: Note-Taking\r\nSoftware\r\n\r\n\r\nOverview\r\nIn this course, you will learn the math and coding skills you need to\r\nconduct social science research. In the first half of the semester,\r\nwe’ll get comfortable working with data: cleaning it up, visualizing it,\r\nand summarizing it. In the second half of the semester, we’ll build\r\nstatistical models to help us interpret our data, communicate\r\nuncertainty, and make causal claims. A key theme of the course is that,\r\nto deeply understand your data, you must do both.\r\nAll the course materials you need will be available on this website.\r\nCheck the Schedule tab for an overview of each week’s topics and\r\nassignments. On the navbar, you can also find links to read the syllabus, schedule an office hours\r\nappointment, read your\r\nclassmates’ comments on the readings, and visit\r\nthe code repository.\r\nGetting Started\r\nThere is a lot of software to install for this class, but in the\r\ninstructions below I’ve tried to make it as straightforward as possible.\r\n(And everything is free!) Before the end of our first week, please\r\ncomplete the following steps.\r\n\r\nData Analysis (R and RStudio)\r\nFor starters, you will need R. The most current version\r\nis available here. To\r\ndownload, click link for your operating system (i.e. Mac, Windows,\r\nLinux), then follow the instructions on the page.\r\nR is the programming language itself, but it comes with\r\na pretty unfriendly interface.\r\n\r\nYou don’t want to do all your typing in that lonely black box.\r\nInstead, you want RStudio, a piece of software that\r\nmakes R much prettier. Download the RStudio\r\nDesktop version here.\r\nOPTIONAL: Version Control\r\n(git)\r\nProfessional coders use a version control system\r\ncalled git to keep track of their code. If you’re on a Mac,\r\nyour computer probably already has it installed.1 If\r\nnot, or you use a PC, follow the installation instructions here.\r\nOnce git is installed, it will want you to introduce\r\nyourself. To do so, follow these three steps:\r\nIn RStudio, click Tools > Shell. This will open\r\nyour computer’s shell, a program that\r\nruns other programs.\r\nType and enter\r\ngit config --global user.name 'Jane Doe' (substituting your\r\nname for Jane Doe).\r\nType and enter\r\ngit config --global user.email 'jane@example.com'\r\n(substituting your email for that fake email address).\r\nUsing git to track changes in your code involves a bit\r\nof struggle on the front end, but it is well worth it (Bryan 2017). I’ll walk you through the\r\nbasics in class.\r\nOPTIONAL: Reference\r\nManagement (Zotero)\r\nThis software makes it blindingly simple to keep track of and format\r\nyour citations. It also plays really nicely with RStudio. Download it here and follow the setup\r\ninstructions. The Chrome\r\nextension is particularly useful; anything you find on the Internet\r\ncan be saved to your library with one click.\r\nAfter Zotero is installed, create a username and password (if you\r\nclick the green sync button at the top right, it will prompt you to\r\ncreate an account).\r\nOPTIONAL: Social\r\nAnnotation (Hypothesis)\r\nI have created a group for our class on a social\r\nannotation platform called Hypothesis. That way, everyone will\r\nbe able to post comments/questions/concerns/memes directly into the\r\nreading assignments. Follow these steps to get started:\r\nClick the book icon in the top-right corner of the course website.\r\nThis will take you to our Hypothesis\r\nGroup Page.\r\nIf you don’t already have an account, you will be prompted to create\r\none. Make your username something like firstname_lastname\r\nso we know who is posting!\r\nHypothesis works best with Google Chrome. If you don’t have it,\r\ninstall it here.\r\nInstall the Chrome\r\nextension. When you do, it should take you to a welcome page with\r\ninstructions on how to log in, pin the extension, and start annotating\r\npages!\r\n\r\nGo to the syllabus and\r\nadd your questions/comments/memes. Whenever you annotate a reading, just\r\nmake sure that you are posting to the Group instead of Public (see the\r\ndropdown menu at the top of the extension).\r\nOPTIONAL: Memory Aid (Anki)\r\nA lot of grad school – particularly the methods courses – involves\r\nrote memorization of things like formulas and code syntax. This is the\r\nboring and frustrating part, but there are ways to make it somewhat less\r\nboring and frustrating. Do yourself a favor and take 20 minutes and play\r\nthrough Case (2018). If you’re\r\nconvinced, download Anki. It’s\r\nnot the prettiest spaced repetition software out there, but it is free,\r\nstreamlined, highly customizable, and has a large, dedicated\r\nfanbase.\r\nOPTIONAL: Note-Taking\r\nSoftware\r\nWhile we’re talking software, consider how you want to organize all\r\nthe notes you generate in grad school. Information is going to be coming\r\nat you like a firehouse, and you’ll want a system that you can trust as\r\nyour “external brain”.2 I highly recommend reading Ahrens (2017), a book\r\nthat is much more fascinating than a book about note-taking should\r\nrightfully be.\r\n\r\n\r\nAhrens, Sönke. 2017. How to Take Smart Notes: One Simple Technique\r\nto Boost Writing, Learning and Thinkingfor Students,\r\nAcademics and Nonfiction Book Writers.\r\n\r\n\r\nBryan, Jennifer. 2017. “Excuse Me, Do You Have a Moment to Talk\r\nabout Version Control?” https://doi.org/10.7287/peerj.preprints.3159v2.\r\n\r\n\r\nCase, Nicky. 2018. “How To Remember Anything Forever-Ish.”\r\nhttps://ncase.me/remember/.\r\n\r\n\r\nI think? If you’re on a Mac and you can’t get git to\r\nwork, let me know!↩︎\r\nDon’t be like I was in grad school and write all your\r\nnotes in a bunch of random notebooks that you throw in the trash at the\r\nend of the semester; so much wasted effort!\r\n\r\n↩︎\r\n",
      "last_modified": "2023-08-16T15:27:21-04:00"
    },
    {
      "path": "week-01.html",
      "title": "Week 1",
      "description": "Tell stories with data",
      "author": [],
      "contents": "\r\n\r\nContents\r\nReadings\r\nOverview\r\nIntroductions\r\nProblem Set\r\n\r\nReadings\r\nTeacup\r\nGiraffes (Intro to R Module)\r\nHealy Preface\r\n(and complete the package installation instructions at the bottom)\r\nHealy Chapter\r\n2\r\nR4DS Chapter\r\n1\r\nOverview\r\nThis week, we will familiarize ourselves with a suite of software\r\nthat allows us to learn lessons from data in a transparent, reproducible\r\nfashion. Make sure you’ve completed everything from the Getting Started section on the\r\nhome page.\r\n\r\n\r\n\r\n\r\nIntroductions\r\nOn our first day, we will be collecting and analyzing some data about\r\nthe class itself. Please complete the following survey.1\r\n\r\n\r\n\r\n\r\nProblem Set\r\nOur first problem set will ensure that all the software is\r\nfunctioning on your machine. If you can complete all of the following\r\nsteps, then you’re in good shape!\r\nDownload the folder from the class GitHub repository and open\r\nintro-to-political-methodology.Rproj.\r\nOpen problem-sets/problem-set-01.R.\r\nUpdate the author and date in the metadata at the top. (You’re\r\nthe author. The date is today.)\r\nUpdate the script so that it filters the data for your name and\r\nsex (or some other name and sex of your choice) instead of the male\r\n“Joseph”s.\r\nMake sure you have the tinytex package installed.\r\nThis is the library you need to convert RMarkdown documents into PDFs.\r\nTo install, enter these two lines into your RStudio console.\r\n\r\n\r\ninstall.packages('tinytex')\r\ntinytex::install_tinytex()\r\n\r\n\r\n\r\nClick ‘Compile Report’ (Ctrl + Shift + K) and output to\r\nPDF.\r\nSubmit the resulting PDF document to eLC at least 24 hours before\r\nour next class.2\r\nIf the embedded survey isn’t working for some reason,\r\nclick here.↩︎\r\nAll problem sets are due by the following\r\nweek’s class (so this one is due before our second class). Feel free to\r\nwork together on problem sets, but I encourage you to type out the\r\nsolutions yourself instead of copying-pasting from others. You can’t\r\nlearn this stuff without doing it.\r\n\r\n↩︎\r\n",
      "last_modified": "2023-08-16T15:27:22-04:00"
    },
    {
      "path": "week-02.html",
      "title": "Week 2",
      "description": "Describe your data",
      "author": [],
      "contents": "\r\n\r\nContents\r\nReading\r\nOptional\r\nPractice\r\n\r\nOverview\r\nVisualizing Distributions\r\nSummarizing Distributions\r\nProblem Set\r\n\r\nReading\r\nTeacup\r\nGiraffes: Mean, Median, and Mode\r\nThe\r\nEffect Chapter 3 (skip section 3.5 for now)\r\nOptional Practice\r\nThe RStudio Primers\r\nare a set of helpful interactive data science tutorials from the people\r\nwho brought you RStudio and the tidyverse. If you’re\r\nlooking for additional practice, I recommend giving them a try!\r\nProgramming\r\nBasics\r\nWork with\r\nData\r\nOverview\r\nThis week, we introduce a tremendously fundamental concept in\r\nstatistics, called the variable. What is a variable? Well, it’s\r\na thing that varies across observations. Reason #1 why we’re\r\nusing data to understand the world (i.e. a many\r\nstory approach) is that it allows us to study variation – how things\r\nchange from case to case.\r\nIn a tidy dataset, variables are the columns. For example, in the\r\nbabynames dataset, the column named n contains\r\nthe number of babies born in the United States in a given year with a\r\ngiven name and sex.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(babynames)\r\n\r\nbabynames\r\n\r\n\r\n# A tibble: 1,924,665 × 5\r\n    year sex   name          n   prop\r\n   <dbl> <chr> <chr>     <int>  <dbl>\r\n 1  1880 F     Mary       7065 0.0724\r\n 2  1880 F     Anna       2604 0.0267\r\n 3  1880 F     Emma       2003 0.0205\r\n 4  1880 F     Elizabeth  1939 0.0199\r\n 5  1880 F     Minnie     1746 0.0179\r\n 6  1880 F     Margaret   1578 0.0162\r\n 7  1880 F     Ida        1472 0.0151\r\n 8  1880 F     Alice      1414 0.0145\r\n 9  1880 F     Bertha     1320 0.0135\r\n10  1880 F     Sarah      1288 0.0132\r\n# ℹ 1,924,655 more rows\r\n\r\nHow do we make sense of all that information? The n\r\ncolumn alone contains 1924665 numbers! Broadly speaking, there are two\r\nways to do it: visualization and summarization.\r\nVisualizing Distributions\r\nThe first way to make sense of all that data is to create a visual\r\nrepresentation of the distribution, depicting how often\r\ndifferent values occur.\r\n\r\n\r\n# take the babynames dataset\r\nbabynames |> \r\n  # keep only babies born in the year 2000\r\n  filter(year == 2000) |> \r\n  # pipe that dataset into a ggplot with the variable n on the x-axis\r\n  ggplot(mapping = aes(x = n)) +\r\n  # add a *histogram* geometry\r\n  geom_histogram(color = 'black') +\r\n  # make the x-axis a *logarithmic* scale\r\n  scale_x_log10()\r\n\r\n\r\n\r\n\r\nA few things to note here:\r\nA histogram is a type of visualization where the height of each bar\r\ncorresponds to the number of observations that fall into a certain\r\nrange. I hope you agree that this is much more legible than looking\r\nthrough 1924665 numbers.\r\nI plotted this histogram on a logarithmic scale, which\r\nmeans that each increment is a multiple of the previous increment\r\n(e.g. 10 is followed by 100, then 1,000, then 10,000).\r\nThis particular distribution is heavy tailed. There are\r\nthousands of names that appear less than a ten times, but only a handful\r\nof names that appear more than 10,000 times.\r\nSummarizing Distributions\r\nThe second way to make sense of data is with statistics. A\r\nstatistic is a number computed from data that summarizes or describes\r\nthe distribution in some way.\r\nFor example, how many total babies were born in the United States in\r\nthe year 2000?\r\n\r\n\r\nbabynames |> \r\n  filter(year == 2000) |> \r\n  summarize(total_babies = sum(n))\r\n\r\n\r\n# A tibble: 1 × 1\r\n  total_babies\r\n         <int>\r\n1      3778079\r\n\r\nWhat was the median and mean value of n that year?\r\n\r\n\r\nbabynames |> \r\n  filter(year == 2000) |> \r\n  summarize(median_popularity = median(n),\r\n            mean_popularity = mean(n))\r\n\r\n\r\n# A tibble: 1 × 2\r\n  median_popularity mean_popularity\r\n              <int>           <dbl>\r\n1                11            127.\r\n\r\nEach of these summary statistics has a slightly different\r\ninterpretation. The median tells us how many babies were born\r\nwith the name in the exact middle of the popularity distribution that\r\nyear. (In this case, there were 11 babies born with the name “Miraya”.)\r\nThe interpretation of the mean is bit more complicated, but\r\nhere’s a mental image I like. Imagine we took every baby born in the\r\nyear 2000, sorted them by popularity of their name, and placed them on a\r\nmassive see-saw. The mean is where you would place the fulcrum\r\nto exactly balance the babies on the left with the babies on the right.\r\nBecause this distribution is so heavy tailed, that balance point is\r\naround 127.\r\nWhich is the right way to describe the distribution? Well,\r\nneither. They’re just different ways to describe the distribution, and\r\nwe may want one or the other depending on what question we’re\r\nasking.\r\nUsing the group_by() function, we can compute summary\r\nstatistics for different groups in the data.\r\n\r\n\r\nbabynames |> \r\n  filter(year == 2000) |> \r\n  group_by(sex) |> \r\n  summarize(median_popularity = median(n),\r\n            mean_popularity = mean(n))\r\n\r\n\r\n# A tibble: 2 × 3\r\n  sex   median_popularity mean_popularity\r\n  <chr>             <dbl>           <dbl>\r\n1 F                    11            103.\r\n2 M                    11            162.\r\n\r\nTake a moment to ponder this result. What might it mean that the\r\nmedian for females and males is the same, but the mean for males is\r\nlarger?\r\nProblem Set\r\nIn our class folder, open RStudio with the project file\r\nintro-to-political-methodology.Rproj, then open\r\nproblem-sets/problem-set-02.R.\r\nIn this problem set, we’ll summarize the distribution of survey\r\nrespondents in the 2020 Cooperative Election Study (CES). First, what is\r\nthe median age of respondents?\r\nWhat is the median age of respondents who Strongly Approve of\r\nTrump’s performance as president? What’s the median age for each level\r\nof the trump_approval variable?\r\nWhat’s the average age of people who watched TV news in the last\r\n24 hours? Those who didn’t?\r\nAre respondents who view religion as Very Important more or less\r\nlikely to support a ban on assault rifles than those who view religion\r\nas Not Important At All?\r\nSee what else you can discover in the dataset, reporting at least\r\nthree other summary statistics.\r\nCompile the report to PDF (Ctrl + Shift + K) and submit the PDF\r\nat least 24 hours before our next class.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-16T15:27:28-04:00"
    },
    {
      "path": "week-03.html",
      "title": "Week 3",
      "description": "Wrangle your data",
      "author": [],
      "contents": "\r\n\r\nContents\r\nReading\r\nOptional\r\nPractice\r\n\r\nOverview\r\nImporting\r\nThe Data\r\nRecoding\r\nVariables\r\nJoining\r\nDatasets\r\nSelecting Variables\r\n\r\nProblem Set\r\n\r\nReading\r\nTeacup\r\nGiraffes: Spread of the Data\r\nWrangling\r\nPenguins\r\nR4DS Chapter 9:\r\nWrangle\r\nR4DS Chapter 11:\r\nImport (skip sections 11.3 and 11.4)\r\nR4DS Chapter 12:\r\nTidy Data\r\nOptional Practice\r\nTidy Your Data\r\n(RStudio Primer)\r\nI’m assigning what I think are the crucial chapters of R4DS, but if\r\nyou really want to get deep into R programming, feel free\r\nto look over the chapters we skipped. In particular, the chapter on\r\n“Strings” if you want to work with text data, and “Dates & Times” if\r\nyou want to work with time series.\r\nOverview\r\nUp to now, we’ve been working with pretty tidy datasets. Every column\r\nis a variable, every row is an observation, and every value is where it\r\nshould be. But things are not always this way. More often than you’re\r\ngoing to like, data comes to you an unruly mess, and you’ll need to tidy\r\nit up before you can even start to explore it.\r\nThis week, we’ll learn some of the most important functions in the\r\ntidyverse for data importing, wrangling, and\r\nsummarizing.\r\n\r\nIn your last problem set, you\r\nworked with a tidied up version of the Cooperative Election Study (CES)\r\nfrom 2020. Let’s look at the steps I took to import and clean up that\r\ndataset.\r\nImporting The Data\r\nThe CES is a survey of a representative sample of 50,000+ American\r\nadults, conducted every election year by a large consortium of\r\nuniversities. The data going back to 2006 is available here. Go to that site and\r\ndownload the 2020 dataset, a 180 megabyte monster called\r\nCES20_Common_OUTPUT_vv.csv.\r\nThe .csv extension at the end of the file means that it\r\nis a “comma separated value” file. This is a file where all of the\r\nvalues are separated by commas. Looks something like this:\r\nname, age, party\r\n'Joe', 35, 'Bull Moose'\r\n'Fiona', 6, 'Birthday'\r\n'Avery', 3, 'Know Nothing'\r\nIt’s a nice standardized way to represent datasets, and fortunately\r\nfor us, the read_csv() function allows us to read such\r\nfiles into R. Let’s create an object called\r\nces_raw from that raw dataset.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\nces_raw <- read_csv('data/raw/CES20_Common_OUTPUT_vv.csv')\r\n\r\n\r\n\r\nThis dataset has a lot of information.\r\n\r\n\r\ndim(ces_raw)\r\n\r\n\r\n[1] 61000   717\r\n\r\n61,000 rows and 717 columns, to be precise. The first few rows and\r\ncolumns look like this:\r\n\r\n\r\nhead(ces_raw)\r\n\r\n\r\n# A tibble: 6 × 717\r\n   ...1    caseid commonweight commonpostweight vvweight vvweight_post\r\n  <dbl>     <dbl>        <dbl>            <dbl>    <dbl>         <dbl>\r\n1     1    1.23e9        0.783            0.666    0.851         0.607\r\n2     2    1.23e9        1.34             1.44    NA            NA    \r\n3     3    1.23e9        0.406            0.342   NA            NA    \r\n4     4    1.23e9        0.958            0.822    1.04          1.00 \r\n5     5    1.23e9        0.195            0.162   NA            NA    \r\n6     6    1.23e9        1.06             0.880    1.15          0.988\r\n# ℹ 711 more variables: tookpost <dbl>, CCEStake <dbl>,\r\n#   birthyr <dbl>, gender <dbl>, educ <dbl>, race <dbl>,\r\n#   race_other <chr>, hispanic <dbl>, CC20_hisp_1 <dbl>,\r\n#   CC20_hisp_2 <dbl>, CC20_hisp_3 <dbl>, CC20_hisp_4 <dbl>,\r\n#   CC20_hisp_5 <dbl>, CC20_hisp_6 <dbl>, CC20_hisp_7 <dbl>,\r\n#   CC20_hisp_8 <dbl>, CC20_hisp_9 <dbl>, CC20_hisp_10 <dbl>,\r\n#   CC20_hisp_11 <dbl>, CC20_hisp_12 <dbl>, CC20_hisp_t <chr>, …\r\n\r\nRecoding Variables\r\nNow that we have the dataset loaded into memory, let’s find the\r\nvariables we’re interested in keeping and clean them up a bit. For\r\nexample, gender is coded as a 1 or 2.\r\n\r\n\r\nces_raw |> \r\n  count(gender)\r\n\r\n\r\n# A tibble: 2 × 2\r\n  gender     n\r\n   <dbl> <int>\r\n1      1 25791\r\n2      2 35209\r\n\r\nLet’s make this more readable. The codebook (available at the same\r\nsite where you downloaded the data) says that 1 is male and 2 is female.\r\nWe can create a new version of the gender column using the\r\nmutate() and if_else() functions.\r\n\r\n\r\nces_clean <- ces_raw |> \r\n  mutate(gender = if_else(gender == 1, 'Male', 'Female'))\r\n\r\nces_clean |> \r\n  count(gender)\r\n\r\n\r\n# A tibble: 2 × 2\r\n  gender     n\r\n  <chr>  <int>\r\n1 Female 35209\r\n2 Male   25791\r\n\r\nWhat about the education variable (educ)?\r\n\r\n\r\nces_clean |> \r\n  count(educ)\r\n\r\n\r\n# A tibble: 6 × 2\r\n   educ     n\r\n  <dbl> <int>\r\n1     1  1983\r\n2     2 16618\r\n3     3 13330\r\n4     4  6539\r\n5     5 14152\r\n6     6  8378\r\n\r\nThat’s a lot of categories to clean up. We can combine the\r\nmutate() function with case_when() to recode a\r\nbunch of different values at the same time.\r\n\r\n\r\nces_clean <- ces_clean |> \r\n  mutate(educ = case_when(educ == 1 ~ 'No HS',\r\n                          educ == 2 ~ 'High school graduate',\r\n                          educ == 3 ~ 'Some college',\r\n                          educ == 4 ~ '2-year',\r\n                          educ == 5 ~ '4-year',\r\n                          educ == 6 ~ 'Post-grad'))\r\nces_clean |> \r\n  count(educ)\r\n\r\n\r\n# A tibble: 6 × 2\r\n  educ                     n\r\n  <chr>                <int>\r\n1 2-year                6539\r\n2 4-year               14152\r\n3 High school graduate 16618\r\n4 No HS                 1983\r\n5 Post-grad             8378\r\n6 Some college         13330\r\n\r\nNow the variable has the correct labels, but notice that the order in\r\nthat table is kind of weird. We tend to think of education as an\r\nordinal variable, because there’s a natural order from\r\nleast to most educated. But R doesn’t know that yet. It\r\njust thinks of educ as a character variable, and lists them\r\nin alphabetical order. We can tell R about the variable\r\norder by recoding educ as a factor.\r\n\r\n\r\nces_clean <- ces_clean |> \r\n  mutate(educ = factor(educ,\r\n                       levels = c('No HS', \r\n                                  'High school graduate',\r\n                                  'Some college',\r\n                                  '2-year',\r\n                                  '4-year',\r\n                                  'Post-grad')))\r\nces_clean |> \r\n  count(educ)\r\n\r\n\r\n# A tibble: 6 × 2\r\n  educ                     n\r\n  <fct>                <int>\r\n1 No HS                 1983\r\n2 High school graduate 16618\r\n3 Some college         13330\r\n4 2-year                6539\r\n5 4-year               14152\r\n6 Post-grad             8378\r\n\r\nMuch better!\r\nThere’s a variable in CES called birthyr. What if we\r\nwanted to know the respondents age in 2020 instead? The\r\nmutate() function can help us here too.\r\n\r\n\r\nces_clean |> \r\n  count(birthyr)\r\n\r\n\r\n# A tibble: 78 × 2\r\n   birthyr     n\r\n     <dbl> <int>\r\n 1    1925     2\r\n 2    1926     6\r\n 3    1927    10\r\n 4    1928    12\r\n 5    1929    23\r\n 6    1930    23\r\n 7    1931    42\r\n 8    1932    55\r\n 9    1933    76\r\n10    1934   109\r\n# ℹ 68 more rows\r\n\r\nces_clean <- ces_clean |> \r\n  mutate(age = 2020 - birthyr)\r\n\r\nces_clean |> \r\n  count(birthyr, age)\r\n\r\n\r\n# A tibble: 78 × 3\r\n   birthyr   age     n\r\n     <dbl> <dbl> <int>\r\n 1    1925    95     2\r\n 2    1926    94     6\r\n 3    1927    93    10\r\n 4    1928    92    12\r\n 5    1929    91    23\r\n 6    1930    90    23\r\n 7    1931    89    42\r\n 8    1932    88    55\r\n 9    1933    87    76\r\n10    1934    86   109\r\n# ℹ 68 more rows\r\n\r\nJoining Datasets\r\nHere’s another messy feature of this dataset. Every respondent has a\r\nvariable called inputstate, representing the state where\r\nthey live.\r\n\r\n\r\nces_clean |> \r\n  count(inputstate)\r\n\r\n\r\n# A tibble: 51 × 2\r\n   inputstate     n\r\n        <dbl> <int>\r\n 1          1   947\r\n 2          2   115\r\n 3          4  1463\r\n 4          5   536\r\n 5          6  5035\r\n 6          8  1061\r\n 7          9   642\r\n 8         10   240\r\n 9         11   197\r\n10         12  4615\r\n# ℹ 41 more rows\r\n\r\nBut it’s a number! Not, like, the actual name of the state, which\r\nwould be more useful to me. I don’t know all 51 state FIPS\r\ncodes by heart, but fortunately I have a dataset that includes every\r\nstate’s name and FIPS code.\r\n\r\n\r\nstate_fips <- tidycensus::fips_codes |> \r\n  mutate(inputstate = as.numeric(state_code)) |> \r\n  select(inputstate, state_name) |> \r\n  unique()\r\n\r\nhead(state_fips)\r\n\r\n\r\n    inputstate state_name\r\n1            1    Alabama\r\n68           2     Alaska\r\n104          4    Arizona\r\n119          5   Arkansas\r\n194          6 California\r\n252          8   Colorado\r\n\r\nAnd I can join that information with the CES data\r\nlike this.\r\n\r\n\r\nces_clean <- ces_clean |> \r\n  left_join(state_fips, by = 'inputstate')\r\n\r\nces_clean |> \r\n  count(inputstate, state_name)\r\n\r\n\r\n# A tibble: 51 × 3\r\n   inputstate state_name               n\r\n        <dbl> <chr>                <int>\r\n 1          1 Alabama                947\r\n 2          2 Alaska                 115\r\n 3          4 Arizona               1463\r\n 4          5 Arkansas               536\r\n 5          6 California            5035\r\n 6          8 Colorado              1061\r\n 7          9 Connecticut            642\r\n 8         10 Delaware               240\r\n 9         11 District of Columbia   197\r\n10         12 Florida               4615\r\n# ℹ 41 more rows\r\n\r\nSelecting Variables\r\nFinally, we can use the select() function to keep only\r\nthe columns we want, and drop the columns we don’t want.\r\n\r\n\r\nces_clean <- ces_clean |> \r\n  select(caseid, gender, educ, age, state_name)\r\n\r\nhead(ces_clean)\r\n\r\n\r\n# A tibble: 6 × 5\r\n      caseid gender educ           age state_name   \r\n       <dbl> <chr>  <fct>        <dbl> <chr>        \r\n1 1232318697 Male   2-year          54 Connecticut  \r\n2 1231394915 Female Post-grad       65 Florida      \r\n3 1232451503 Female 4-year          74 Iowa         \r\n4 1232494791 Female 4-year          58 Massachusetts\r\n5 1232494829 Male   4-year          53 Illinois     \r\n6 1232473675 Male   Some college    59 Ohio         \r\n\r\nAnd you can save your cleaned up version of the data to an .RData\r\nfile with the save() function.\r\n\r\n\r\nsave(ces_clean, file = 'data/ces-clean.RData')\r\n\r\n\r\n\r\nProblem Set\r\nJust like last week, we’re going to be summarizing different\r\nvariables from the CES dataset. But this time, you’ll need to use the\r\ncodebook to identify which variables contain the information you want,\r\ntidy them up, and then summarize. Write an R script that performs these\r\ntasks, compile a PDF report, and submit the report.\r\nHere are the questions I’d like you to answer:\r\nAre labor union members more likely to be Democrats or\r\nRepublicans?\r\nWhat is the median age of people with landline phones, compared\r\nto those who only have cell phones?\r\nAre people who read the newspaper more likely to correctly answer\r\nthe questions about who controls the US House of Representatives and US\r\nSenate?\r\nWhich religious groups are the most and least likely to support\r\nmaking abortion illegal in all circumstances?\r\nPick one other variable you find interesting and describe\r\nit.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-16T15:27:44-04:00"
    },
    {
      "path": "week-04.html",
      "title": "Week 4",
      "description": "Visualize your data",
      "author": [],
      "contents": "\r\n\r\nContents\r\nReading\r\nOptional\r\nOverview\r\nProblem Set\r\n\r\nReading\r\nTeacup\r\nGiraffes, The Normal Distribution\r\nHealy Chapter\r\n3\r\nHealy Chapter\r\n4\r\nHealy Chapter 5\r\nRead up on how to join information from two\r\ndataframes together. left_join() is the most useful. Here’s\r\na good\r\nexplanation, and here’s an interactive\r\nprimer.\r\nOptional\r\nHealy Chapter 1: A\r\nnice review of the evidence on what makes an effective\r\nvisualization.\r\nIf you want to learn how to make maps in ggplot, Healy Chapter 7 is a great\r\nresource.\r\nFor more practice with ggplot, this chapter in\r\nHadley Wickham’s book is a nice resource.1\r\nThe ggplot\r\ncheat sheet is a useful reference.\r\nOverview\r\nThe brain is hardwired to detect patterns in images, and a\r\nwell-crafted data visualization can take advantage of that fact to\r\ncommunicate lots of information in an aesthetically pleasing way. And\r\nit’s about more than communication. Charts can reveal patterns in data\r\nthat summary statistics alone might miss, as the Datasaurus\r\nDozen artfully reveals…\r\n\r\nThis week, we get deeper into building visualizations with the\r\nggplot2 package (a part of the tidyverse). It\r\nwill take some time to learn all of the function syntax, so be patient\r\nwith yourself. Once you get the hang of it, you’ll have an endlessly\r\nflexible tool for exploring and communicating patterns in your data.\r\nLet’s get started.\r\nProblem Set\r\nUsing one of the datasets we’ve been working with in class, submit a\r\ndata visualization using ggplot. Your submission\r\nshould:\r\nCommunicate some interesting result\r\nUse more than one aesthetic\r\nBe easy to interpret at a glance (you can include a short caption –\r\nlabs(caption = ...) – if you would like).\r\nBe reproducible; submit your figure as a .png\r\nfile, plus the .R script that produced it.\r\nConsider this a friendly competition! I will look through the\r\nsumbmissions and pick my favorites, and their creators will receive\r\nhonor, glory, and adoration.\r\n\r\nHadley is the author of ggplot, so he knows\r\na trick or two.↩︎\r\n",
      "last_modified": "2023-08-16T15:27:45-04:00"
    },
    {
      "path": "week-06.html",
      "title": "Week 6",
      "description": "Building our first statistical model, plus all the calculus you need to make it go",
      "author": [],
      "contents": "\r\n\r\nContents\r\nReading\r\nOverview\r\nSlides\r\nProblem Set\r\n\r\nReading\r\nThe\r\nJoy of X, Chapters 17 and 18.\r\nMoore\r\n& Siegel, Chapter 5.\r\nThe\r\nEffect, Chapter 4 (Describing Relationships)\r\nAllison\r\nHorst Explains Derivatives.\r\nOverview\r\nThis week, we introduce the linear model, the\r\nworkhorse of empirical social science. In its most basic form, we take\r\ntwo variables from our dataset and try to find the “line of best fit” –\r\nthe line that best describes the relationship between them. For example,\r\nthe line of best fit below suggests that places where infant mortality\r\nis high also tend to be places where fertility is high, and vice\r\nversa:\r\n\r\n\r\nggplot(data = swiss,\r\n       mapping = aes(x = Fertility,\r\n                     y = Infant.Mortality)) +\r\n  geom_point() +\r\n  # the line of best fit\r\n  geom_smooth(method = 'lm', se = FALSE) +\r\n  theme_bw() +\r\n  labs(title = 'The Relationship Between Fertility and Infant Mortality',\r\n       subtitle = 'Swiss Provinces (1888)')\r\n\r\n\r\n\r\n\r\nBut where did that line of best fit come from? What does it even mean\r\nto be the “best fit”? To answer that question, we first need a language\r\nthat will help us talk about how to minimize error or\r\nmaximize model fit. That language is differential calculus.\r\n\r\nCome with me on a journey.\r\nSlides\r\n\r\n\r\n\r\n\r\n(Full Screen Version)\r\nProblem Set\r\nFor the midterm mini-conference next week, please prepare a 10 minute\r\npresentation about a dataset you find interesting (feel free to use one\r\nof the datasets we’ve looked at in class, or strike out on your own).\r\nYour presentation should describe each step in your analysis pipeline:\r\nimporting, tidying, summarizing, and visualizing. Submit the code you\r\nused in your analysis to eLC (both the R script and a PDF report from\r\nthe script).\r\nPlease send me the title of your presentation at least two days in\r\nadvance so I can create the mini-conference program.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-16T15:27:48-04:00"
    },
    {
      "path": "week-07.html",
      "title": "Week 7",
      "description": "Midterm Mini-Conference",
      "author": [],
      "contents": "\r\n\r\nContents\r\nProblem Set\r\n\r\nThis week you’ll present a project that follows the entire data\r\nanalysis pipeline from start to finish: finding the dataset, importing\r\nit, cleaning it up, summarizing it, and building visualizations.\r\n\r\nIf you’re looking for additional practice with the skills we’ve\r\nlearned over the past six weeks, I highly recommend the RStudio primers, a set of\r\ninteractive online tutorials. To review the functions we’ve covered so\r\nfar, check out “The Basics”, “Work With Data”, “Visualize Data”, and\r\n“Tidy Your Data”. The other primers are more advanced R\r\nfunctions – you may try them if you’re looking for an extra\r\nchallenge.\r\nProblem Set\r\nPractice a bit with calculus here. Complete and submit to\r\neLC in any format you like.\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-16T15:27:49-04:00"
    },
    {
      "path": "week-08.html",
      "title": "Week 8",
      "description": "Correlation does not imply causation (but it's a good place to start)",
      "author": [],
      "contents": "\r\n\r\nContents\r\nReading\r\nSlides\r\nProblem Set\r\n\r\nReading\r\nOur guide to causal inference is The Effect, by Nick\r\nHuntington-Klein. The whole book is great, but start with:\r\nChapter\r\n6: Causal Diagrams\r\nChapter\r\n7: Drawing Causal Diagrams\r\n\r\nSlides\r\n\r\n\r\n\r\n\r\n(Full Screen Version)\r\nProblem Set\r\nConsider a research question you’re interested in and draw a DAG to\r\nrepresent the web of relevant causal relationships - R or\r\nby hand are both acceptable. What are the backdoor paths between your\r\ntreatment and outcome (i.e. alternative explanations)? What variables\r\nmust you condition on to close them?\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-16T15:27:50-04:00"
    },
    {
      "path": "week-09.html",
      "title": "Week 9",
      "description": "Closing The Back Doors\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nReadings\r\nOverview\r\nSlides\r\n3D Plots\r\n\r\nMidterm\r\nSurvey\r\nProblem Set\r\n\r\nReadings\r\nThe\r\nEffect, Chapter 8: Closing Back Doors\r\nMoore\r\n& Siegel,) Chapter 12. Ignore sections 12.3.6 and 12.3.7 on\r\ncomputing determinants and inverses by hand. We have computers that do\r\nthat for us now.\r\nOverview\r\nThis week, we discuss the linear model as a general purpose tool for\r\nconditioning on confounding variables and recovering causal estimands.\r\nWe will highlight the strengths and limitations of this approach, and\r\nshow how to estimate the model’s parameters using matrix algebra.\r\n\r\nSlides\r\n\r\n\r\n\r\n\r\n(Full Screen Version)\r\n3D Plots\r\nSome three-dimensional plots illustrating the “plane of best\r\nfit”.\r\n\r\n\r\n\r\n\r\n\r\n\r\nMidterm Survey\r\n\r\n\r\n\r\n\r\nProblem Set\r\nIn class, we showed that GDP per capita was partly confounding\r\nthe observed relationship between democracy and corruption. Can you\r\nthink of any other back door paths that need to be closed? What variable\r\nwould you need to measure and condition upon to close that path? Try\r\nyour hand at finding a dataset with that variable, merging it with the\r\ncountry-level corruption dataset we created in\r\nR/week-09/cleanup-data.R, and adding it to the linear\r\nmodel. (You may find the countrycode\r\npackage useful for that merge.)\r\nCohn et al. (2019) left wallets\r\nin cities around the world to see how many of them would be returned.\r\nIt’s a very ambitious study. You can read about it here\r\nand you can find the replication data in our repository at\r\ndata/cohn-2019/. You want the “behavioral data”; check out\r\nthe codebook to see what all the variables mean. The experimenters\r\nvaried a few things at random, including the type of institution they\r\nleft it at (public, bank, etc.) and how much money was in the wallet.\r\nYour task is this: I want to know what fraction of wallets were returned\r\nin each country when they were left at public institutions (this could\r\nbe another measure of public corruption; if bureaucrats just tend to\r\nsteal wallets instead of returning them). Are government officials less\r\nlikely to steal wallets in countries that have been democracies for\r\nlonger periods of time (c_PIV_years_democracy)? What’s the\r\nslope of that relationship? And can you interpret that relationship as\r\ncausal? Why or why not?\r\nOptional Challenge: Replicate Figure 1 in the\r\nCohn et al. (2019) paper using\r\nggplot.\r\n\r\n\r\n\r\nCohn, Alain, Michel André Maréchal, David Tannenbaum, and Christian\r\nLukas Zünd. 2019. “Civic Honesty Around the Globe.”\r\nScience 365 (6448): 70–73. https://doi.org/10.1126/science.aau8712.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-16T15:28:07-04:00"
    },
    {
      "path": "week-10.html",
      "title": "Week 10",
      "description": "Finding Front Doors\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nReadings\r\nFurther\r\nReading\r\n\r\nOverview\r\nProblem Set\r\n\r\nReadings\r\nThe\r\nEffect, Chapter 9 (Finding Front Doors)\r\nFurther Reading\r\nIf you’re interested in learning more about these methods and don’t\r\nwant to wait until I teach a standalone class on causal inference, check\r\nout chapters 19 and 20 in Huntington-Klein (2021).\r\nOverview\r\nThis week, the instrumental variable approach to causal inference.\r\nWe’ll talk experiments, two-stage least squares, and regression\r\ndiscontinuity designs.\r\nProblem Set\r\nIn a knitted R script (or Rmd), complete the following problems:\r\nUsing the replication data from the Cohn et al. (2019) wallet experiment, estimate the\r\naverage treatment effect of money for (a) men only, (b) women only, (c)\r\npublic institutions only, (d) people who seemed to understand what the\r\nexperimenter was saying. Do there seem to be heterogeneous treatment\r\neffects? Can we interpret these estimates as causal? Why or why\r\nnot?\r\nEstimate the incumbency advantage/disadvantage in other (non-Brazil)\r\nLatin American countries from the Klašnja and Titiunik (2017) dataset. Plot the data, faceting\r\nby country.\r\n\r\n\r\n\r\nCohn, Alain, Michel André Maréchal, David Tannenbaum, and Christian\r\nLukas Zünd. 2019. “Civic Honesty Around the Globe.”\r\nScience 365 (6448): 70–73. https://doi.org/10.1126/science.aau8712.\r\n\r\n\r\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\r\nResearch Design and Causality. S.l.: CHAPMAN & HALL CRC. https://theeffectbook.net/index.html.\r\n\r\n\r\nKlašnja, Marko, and Rocio Titiunik. 2017. “The Incumbency Curse:\r\nWeak Parties, Term Limits, and Unfulfilled Accountability.”\r\nThe American Political Science Review 111 (1): 129–48. https://doi.org/10.1017/S0003055416000575.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2023-08-16T15:28:09-04:00"
    },
    {
      "path": "week-11.html",
      "title": "Week 11",
      "description": "Sampling Distributions\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nReadings\r\nOverview\r\nClass Notes\r\nDrawing a\r\nSample\r\nThe Sampling Distribution\r\nExpected\r\nValue\r\nStandard\r\nError\r\nThe Central Limit Theorem\r\nMargin of Error and\r\nConfidence Intervals\r\nThe Law of Large Numbers\r\n\r\nProblem Set\r\n\r\nReadings\r\nTeacup\r\nGiraffes: Module 6\r\nThe\r\nJoy of X, Chapters 22 and 23\r\nSeeing Theory,\r\nsome nice interactive explanations of concepts from probability\r\ntheory\r\nFor an (optional) deeper dive into the mathematics of probability\r\ntheory:\r\nMoore\r\n& Siegel, Chapters 9-11\r\nOverview\r\nOkay. So we have an estimate we computed from our data. Maybe we’ve\r\neven done enough theoretical legwork to believe that our estimate\r\nreflects a causal relationship. We’re feeling pretty good about\r\nourselves.\r\nBut we’re not done yet, because that estimate we calculated is based\r\non limited information about the world. Maybe our data only represents a\r\nsample from a much larger population. There’s no guarantee that the\r\nestimate we get from that sample is that same one we would get if we\r\ncould observe everyone in the population. And even if we’ve collected\r\ndata from the entire population, don’t forget about the Fundamental Problem of Causal Inference – we can\r\nnever know with certainty what would have happened under some\r\ncounterfactual treatment regime.1 So we need a way to\r\nexpress our uncertainty about the estimates we produced. How likely is\r\nit that our result was just caused by a weird sample?\r\nThis week, we discuss the fundamental concept underlying all of\r\nstatistical inference2 – the sampling\r\ndistribution.\r\nClass Notes\r\n\r\n\r\n# load the CES dataset\r\nload('data/ces-2020/cleaned-CES.RData')\r\n\r\n# recode TV news viewership as a 0-1 variable\r\nces <- mutate(ces, tv_news_24h = as.numeric(tv_news_24h == 'Yes'))\r\n\r\n\r\n\r\nWhat percent of this population (CES respondents) watched TV news in\r\nthe past 24 hours?\r\n\r\n\r\nmean(ces$tv_news_24h)\r\n\r\n\r\n[1] 0.6058689\r\n\r\nDrawing a Sample\r\nBut suppose we just had a sample from this population, and\r\nwe were trying to estimate that population mean? How weird can that\r\nsample get? And how far can a sample statistic get from the true value?\r\nTo see, let’s play around with drawing samples of CES respondents.\r\n\r\n\r\n# pull a sample\r\nsample_data <- slice_sample(ces, n = 100)\r\n\r\n# what percent of the sample watched TV news recently?\r\nmean(sample_data$tv_news_24h)\r\n\r\n\r\n[1] 0.54\r\n\r\nYour result may differ! (It’s a random sample, so there’s\r\nsome…randomness involved.) But the estimate I got was 54%. Not terrible,\r\nbut not terribly accurate either.\r\nHere’s a custom function that draws a sample and returns the percent\r\nof respondents in that sample that watch TV news. You should run that\r\nfunction a few times to see how much variability we get from sample to\r\nsample.\r\n\r\n\r\nget_sample_mean <- function(sample_size = 100){\r\n  ces %>% \r\n    # draw a sample\r\n    slice_sample(n = sample_size) %>%\r\n    # pull the TV news variable\r\n    pull(tv_news_24h) %>% \r\n    # return the mean\r\n    mean\r\n}\r\n\r\nget_sample_mean()\r\n\r\n\r\n[1] 0.58\r\n\r\nget_sample_mean()\r\n\r\n\r\n[1] 0.61\r\n\r\nget_sample_mean(sample_size = 200)\r\n\r\n\r\n[1] 0.615\r\n\r\nget_sample_mean(sample_size = 10)\r\n\r\n\r\n[1] 0.6\r\n\r\nThe Sampling Distribution\r\nNow here’s the question that motivates basically all of statistics.\r\nHow weird can our sample estimates get? On average, do they\r\ntend to be close to the truth, or can they be pretty far off the mark?\r\nTo address this question, let’s introduce the sampling\r\ndistribution – the distribution of outcomes we would expect to\r\nobserve if we repeatedly drew samples and computed the estimate.3\r\n\r\n\r\n# we'll use the replicate() function to repeat that sampling process a large number of times\r\nsampling_distribution <- replicate( 25000, get_sample_mean() )\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe object sampling_distribution is a vector with 25,000\r\nsample means. Each one was computed from an independent random sample\r\nfrom the CES dataset. The first sample estimate I got was 54%, but what\r\ndoes this set of 25,000 estimates look like?\r\n\r\n\r\nhead(sampling_distribution)\r\n\r\n\r\n[1] 0.62 0.60 0.66 0.57 0.54 0.53\r\n\r\nggplot(mapping = aes(x = sampling_distribution)) +\r\n  geom_histogram(color = 'black', binwidth = 0.01) +\r\n  theme_minimal() +\r\n  labs(x = 'Estimate', y = 'Number of Samples',\r\n       title = 'The Sampling Distribution of the Mean')\r\n\r\n\r\n\r\n\r\nIf I draw a sample of 100 people from a population where 60% have\r\nsome characteristic, then this is the range of outcomes I should expect\r\nto observe. Most of the time, we’ll get a sample estimate between 55%\r\nand 65%. An estimate smaller than 50% or larger than 70% is unlikely,\r\nbut possible.4\r\nExpected Value\r\nWhat is the mean value of the sampling distribution?\r\n\r\n\r\nmean(sampling_distribution)\r\n\r\n\r\n[1] 0.6060284\r\n\r\nThis is the expected value of our sample statistic –\r\nthe average value that you would expect to get if you repeatedly sampled\r\nfrom the population. In this case, notice how close the expected value\r\nis to the truth! We say that an estimator is unbiased\r\nif its expected value equals the true population parameter.\r\nSo we just demonstrated that the mean of a random sample is an\r\nunbiased estimator of the population mean. No big deal. Just the\r\nfoundation of all statistics and polling. That if I draw 100 people at\r\nrandom and ask them a question, their responses will, on average, give\r\nus a good approximation of what all 300 million Americans would say.\r\nBut averages\r\ncan be deceiving! We don’t just want to know whether we’ll be right\r\non average. We want to know how far from the truth we might end\r\nup with our particular sample.\r\nStandard Error\r\nWhat is the standard deviation of our sampling distribution?5\r\n\r\n\r\nsd(sampling_distribution)\r\n\r\n\r\n[1] 0.04884307\r\n\r\nThe standard deviation of a sampling distribution is is called the\r\nstandard error, and it’s a very important for\r\nunderstanding hypothesis tests. The larger the standard error, the wider\r\nthe range of sample statistics that could have been computed from our\r\npopulation, and the less certain we should be about our particular\r\nvalue.\r\nThe Central Limit Theorem\r\nNext, notice the shape of the sampling distribution in the\r\nfigure above. It’s this nice bell curve you may have seen before, called\r\na “normal” distribution (aka a Gaussian distribution).6 Why\r\nis the sampling distribution normally shaped? Well that’s one of the\r\nmost fascinating and magical theorems in all of statistics: the\r\nCentral Limit Theorem. The sampling distribution of the\r\nmean is approximately normally distributed – as long as you have a\r\nsufficiently large7 sample size.\r\nIntuition: some samples will, by chance, contain an\r\nunrepresentatively large number of TV watchers. Some will contain an\r\nunrepresentatively small number of TV watchers. But most of the time,\r\nthe TV watchers and the non-TV watchers will cancel each other out, such\r\nthat the mass of the distribution is centered around the truth.\r\nThere are a bunch of nice simulations\r\nthat showing the Central Limit Theorem in action. In class, you’ll get\r\nto play around with one.\r\nNormal distributions are nice. They allow us to say precisely how far\r\na sample estimate is likely to deviate from the truth. For example, we\r\nknow that about 68% of a normal distribution falls within 1 standard\r\ndeviation of the mean.\r\n\r\n\r\nexpected_value <- mean(sampling_distribution)\r\nstandard_error <- sd(sampling_distribution) \r\nnum_draws <- 25000\r\n\r\nwithin_1sd <- sum(sampling_distribution > expected_value - standard_error &\r\n      sampling_distribution < expected_value + standard_error)\r\n\r\nwithin_1sd / num_draws\r\n\r\n\r\n[1] 0.69112\r\n\r\nAnd about 95% of observations fall within two standard\r\ndeviations.\r\n\r\n\r\nwithin_2sd <- sum(sampling_distribution > expected_value - 2*standard_error &\r\n                        sampling_distribution < expected_value + 2*standard_error)\r\n\r\nwithin_2sd / num_draws\r\n\r\n\r\n[1] 0.95992\r\n\r\nAnd about 99.7% fall within 3 standard deviations. It’s super rare to\r\nget an observation that far away.\r\n\r\n\r\nwithin_3sd <- sum(sampling_distribution > expected_value - 3*standard_error &\r\n                        sampling_distribution < expected_value + 3*standard_error)\r\n\r\nwithin_3sd / num_draws\r\n\r\n\r\n[1] 0.9984\r\n\r\nMargin of Error and\r\nConfidence Intervals\r\nSo if I draw a sample of 100 individuals, then 95% of the time, my\r\nsample proportion will be within…\r\n\r\n\r\n2*sd(sampling_distribution)\r\n\r\n\r\n[1] 0.09768615\r\n\r\n…of the truth. This is what pollsters call the margin of\r\nerror.\r\nAnother way of describing this sampling variability is with a\r\nconfidence interval. You construct a confidence\r\ninterval by taking your sample estimate plus or minus 2 standard errors.\r\nBecause the sample mean is within 2 standard deviations of the truth 95%\r\nof the time, the 95% confidence interval will contain the true value\r\napproximately 95% of the time.\r\nThe Law of Large Numbers\r\nWhat happens to the standard error when you increase your sample\r\nsize? Intuitively, you’re getting more data, so you should be more and\r\nmore confident in your estimate.\r\nLet’s draw a sampling distribution with 400 instead of 100\r\npeople.\r\n\r\n\r\nsampling_distribution <- replicate(25000,\r\n                                   get_sample_mean(sample_size = 400))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nggplot(mapping = aes(x = sampling_distribution)) +\r\n  geom_histogram(color = 'black', binwidth = 0.01) +\r\n  theme_minimal() +\r\n  labs(x = 'Estimate', y = 'Number of Samples',\r\n       title = 'The Sampling Distribution of the Mean')\r\n\r\n\r\n\r\n\r\nCheck out that sampling variability! Previously, we got values as\r\nhigh as 0.8 and as low as 0.4. Now the highest and lowest values are\r\nmore like 0.7 and 0.5.\r\nNote that our estimate is still unbiased (centered around the\r\ntruth):\r\n\r\n\r\nmean(sampling_distribution)\r\n\r\n\r\n[1] 0.6059747\r\n\r\nBut the standard error is half as large.\r\n\r\n\r\nsd(sampling_distribution)\r\n\r\n\r\n[1] 0.02415349\r\n\r\nThis is a general fact about the standard error of the mean. If the\r\npopulation standard deviation equals \\(\\sigma\\), then\r\n\\[\r\n\\text{Standard Error} = \\frac{\\sigma}{\\sqrt{n}}\r\n\\]\r\nFor those interested in the derivation, it’s in the footnotes.8\r\nIf the population distribution has higher variance, so too will your\r\nsampling distribution, because you can draw a wider range of samples.\r\nAnd if you draw a larger sample, the variance of the sampling\r\ndistribution decreases. More precisely, the standard error decreases\r\nwith the square root of \\(n\\). If you\r\nquadruple your sample size, you cut standard error in half.\r\nIn the limit, as your sample size gets bigger and bigger, the\r\nstandard error approaches (but never quite reaches) zero. This is the\r\nLaw of Large Numbers. Bigger datasets mean more\r\nconfidence in your estimate. Formally, we say that the sample mean\r\nconverges in probability to its expected value.\r\nProblem Set\r\nImagine we were interested in studying whether American men were more\r\nor less likely than American women to support import tariffs on Chinese\r\ngoods.\r\nLoad data/ces-2020/cleaned-CES.RData.\r\nWhat percent of CES respondents support tariffs on China?\r\nWhat percent of women support tariffs on China? What percent of men\r\nsupport tariffs on China? What is the difference between these two\r\nvalues (aka the difference in means)?\r\nWrite a function that draws a sample of 100 respondents and returns\r\na sample estimate of the difference between men and women’s support for\r\ntariffs.\r\nUsing that function, generate a sampling distribution for this\r\ndifference in means.\r\nPlot your sampling distribution. Confirm that it is centered around\r\nthe truth.\r\nHow far away from the truth can these sample estimates get? What is\r\nthe maximum value? What is the minimum value? What is the standard\r\ndeviation of the sampling distribution?\r\n\r\n\r\n\r\nAbadie, Alberto, Susan Athey, Guido W. Imbens, and Jeffrey M.\r\nWooldridge. 2020. “Sampling-Based Versus\r\nDesign-Based Uncertainty in Regression Analysis.”\r\nEconometrica 88 (1): 265–96. https://doi.org/10.3982/ECTA12675.\r\n\r\n\r\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\r\nwith Examples in r and Stan. 2nd ed. CRC Texts in Statistical\r\nScience. Boca Raton: Taylor; Francis, CRC Press.\r\n\r\n\r\nFor a thorough review of the distinction between\r\nsampling-based and design-based uncertainty, see Abadie et al. (2020).↩︎\r\nWell, frequentist statistical inference. Bayesians think\r\nabout uncertainty in a different way (McElreath 2020).↩︎\r\nThe sampling distribution is a pure thought experiment,\r\nbecause in real life we can’t actually do this (unless we had a\r\nlot of money to repeat the same survey thousands of times).↩︎\r\nAnd the chance of drawing a sample with 0% is about 1 in\r\nten quintillion-trillion-billion. It will approximately never happen.↩︎\r\nRecall that the variance of a distribution is the average\r\nsquared distance from the mean, and the standard deviation is the\r\nsquare root of variance.↩︎\r\nTechnically\r\nit’s a binomial distribution, but we can reserve that distinction for a\r\nfootnote because binomial distributions are approximately normal when\r\nthe sample size is large enough.↩︎\r\nHow large is “sufficiently large”? I don’t want to give\r\na definitive number, because people will yell at me on Twitter. But, as\r\nyou see in the figure above, 100 is clearly big enough. 50 might even be\r\nokay. Less than 25 is pushing your luck.↩︎\r\nThe sample mean is the sum of your all your sample\r\nvalues \\((X)\\) divided by the sample\r\nsize \\((n)\\).\r\n\\[\\bar{X} =\r\n\\frac{\\sum{X_i}}{n}\\]\r\nThe variance of that statistic is:\r\n\\[\r\n\\text{Var}(\\bar{X}) = \\text{Var}\\left(\\frac{\\sum{X_i}}{n}\\right)\r\n\\]\r\nOne property of variance is that \\(\\text{Var}(ax) = a^2\\text{Var}(X)\\) for any\r\nconstant \\(a\\). If \\(\\text{Var}(X_i) = \\sigma^2\\), then:\r\n\\[\r\n\\text{Var}\\left(\\frac{\\sum{X_i}}{n}\\right) = \\frac{1}{n^2} (n\\sigma) =\r\n\\frac{\\sigma^2}{n}\r\n\\]\r\nThe square root of that variance, \\(\\frac{\\sigma}{\\sqrt{n}}\\), is the standard\r\nerror of th mean.↩︎\r\n",
      "last_modified": "2023-08-16T15:28:14-04:00"
    },
    {
      "path": "week-12.html",
      "title": "Week 12",
      "description": "Statistical Inference, Part I\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nReadings\r\nOverview\r\nClass Notes\r\nStep 1: Compute the test\r\nstatistic\r\nStep 2: Derive the\r\nSampling Distribution\r\nStep\r\n3: Compare the observed statistic to the null distribution\r\n\r\nAppendix\r\n\r\nReadings\r\nKellstedt and\r\nWhitten (2018), chapters 7-8 (Statistical\r\nInference and Bivariate Hypothesis Testing)1\r\nHuntington-Klein\r\n(2021), section 3.5 (this is the section\r\nwe skipped before on Theoretical Distributions).\r\nThere\r\nIs Only One Test (blog post by Allen Downey)\r\nOverview\r\nLast week, we introduced probability theory from the perspective of\r\nsampling. We have some population of interest, and we imagine\r\nall the possible samples that we could draw from the population. With\r\nthis sampling distribution in hand, we have a better sense of how far\r\nfrom the truth a sample estimate might be.\r\nThis week, we turn that question on its head. We are no longer an\r\nomniscient being who can sample ad infinitum from the\r\npopulation. Instead, we are a humble researcher with a single sample.\r\nWhat conclusions can we draw? How confident are we that our sample is\r\nnot way out in the tails of the sampling distribution? That is a task\r\nfor statistical inference.\r\n\r\nClass Notes\r\nEvery statistic has a sampling distribution. When we conduct a\r\nhypothesis test, we compare our observed statistic to\r\nits sampling distribution, to assess whether that statistic is something\r\nwe would have expected due to chance alone. Every hypothesis test\r\nproceeds in three steps:\r\nCompute the test statistic\r\nGenerate the sampling distribution assuming a null\r\nhypothesis\r\nCompare the test statistic with its sampling distribution. This\r\ncomparison will take one of two forms:\r\nA p-value (if the null hypothesis were true, how\r\nsurprising would my test statistic be?)\r\nA confidence interval (what is the set of null\r\nhypotheses for which my test statistic would not be\r\nsurprising?)\r\n\r\nStep 1: Compute the test\r\nstatistic\r\nA statistic can be anything you compute from data! So far we’ve\r\ncomputed statistics like:\r\nThe sample mean2\r\nThe difference in means\r\nVariance\r\nLinear model coefficients\r\nA word on notation: statisticians denote population-level parameters\r\nwith Greek letters.3 So the population mean is typically\r\n\\(\\mu\\), the population standard\r\ndeviation is \\(\\sigma\\), the true\r\naverage treatment effect is \\(\\tau\\),\r\nand the true linear model slope coefficient is \\(\\beta\\). Of course, you can write whatever\r\nGreek letters you like. These are just conventions.\r\nSample statistics get plain old English letters, like \\(b\\) for an estimated slope or \\(s\\) for a the standard deviation of a\r\nsample. Alternatively, they might get little hats on top of Greek\r\nletters, like \\(\\hat{\\beta}\\), to show\r\nthat they are estimates of the population-level parameter we care\r\nabout.\r\nAs a running example, let’s draw a sample from CES and compute the\r\ndifference in support for tariffs on Chinese goods between men and\r\nwomen.\r\n\r\n\r\nload('data/ces-2020/cleaned-CES.RData')\r\n\r\nsample_data <- ces %>% \r\n  filter(!is.na(china_tariffs)) %>% \r\n  mutate(pro_tariff = as.numeric(china_tariffs == 'Support'),\r\n         female = as.numeric(gender == 'Female')) %>% \r\n  slice_sample(n = 100)\r\n\r\nlinear_model <- lm(pro_tariff ~ female, \r\n                   data = sample_data)\r\n\r\ntest_statistic <- linear_model$coefficients['female']\r\n\r\ntest_statistic\r\n\r\n\r\n    female \r\n0.01550388 \r\n\r\nOur sample estimate (\\(\\hat{\\beta}\\)) suggests that women are 1.55\r\npercentage points more likely than men to support tariffs on China. Is\r\nthat difference “real” or could it have been driven just by sampling\r\nvariability? Let’s find out.\r\nStep 2: Derive the\r\nSampling Distribution\r\nStatisticians have spent centuries doing most of the work here, so\r\nunless you’ve computed a statistic nobody’s ever seen before, chances\r\nare you won’t have to do too much heavy lifting. As we demonstrated in\r\nthe problem set, the sampling distribution of a difference-in-means\r\nstatistic is normally distributed, so let’s work with that.\r\nWe typically represent random events (like the outcomes of a random\r\nsample) with a special type of function called a probability\r\ndistribution function (pdf). These functions have two special\r\nproperties:\r\n\\(f(x) \\geq 0\\)\r\n\\(\\int_{-\\infty}^\\infty f(x)dx =\r\n1\\)\r\nThat first property says that probability can’t be less-than-zero.\r\nEvery outcome must have at least a zero probability of happening. The\r\nsecond property uses integral notation (if you’ve never\r\nseen that before we’ll discuss it in a moment). It says that the sum of\r\nprobabilities for all outcomes equals 1 (something has to\r\nhappen).\r\nThe PDF of a normal distribution looks like this (please don’t\r\ncry).\r\n\\[\r\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\r\ne^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\r\n\\]\r\nwhere \\(\\mu\\) is the mean of the\r\ndistribution, \\(\\sigma\\) is the\r\nstandard deviation, \\(\\pi\\) is the\r\nratio of a circle’s circumference to its diameter, and \\(e\\) is how much money you would have if you\r\nleft a dollar in a bank offering 100% interest continuously compounded\r\nover one year. The fact that both \\(\\pi\\) and \\(e\\) appear in the same equation to describe\r\nthe distribution of a sample mean is honestly one of the most beautiful\r\nthings I know about the universe.4\r\nWe’d like to test the null hypothesis that there is no difference\r\nbetween men and women on support for tariffs \\((H_0: \\beta = 0)\\). So we’ll set the mean\r\nof our null distribution to 0, but what do we use for \\(\\sigma\\)? We don’t know its true value,\r\nbecause we only have a sample. So we’ll estimate it instead.\r\nRecall from the Kellstedt and Whitten (2018) that the\r\nstandard error of a difference in means statistic is \\(\\sqrt{\\frac{\\sigma^2}{n_0} +\r\n\\frac{\\sigma^2}{n_1}}\\). (If you’re curious where that value\r\ncomes from, see the appendix of this page.) We’ll estimate that standard\r\nerror by plugging in the sample variance for \\(\\sigma^2\\).\r\n\r\n\r\nn0 <- sum(sample_data$female == 0)\r\nn1 <- sum(sample_data$female == 1)\r\ns <- sd(sample_data$pro_tariff)\r\nstandard_error <- sqrt(s^2 / n0 + s^2 / n1)\r\n\r\nstandard_error\r\n\r\n\r\n[1] 0.09616614\r\n\r\nWhich means that if the null hypothesis were true, the sampling\r\ndistribution would look like this:\r\n\r\n\r\nx <- seq(-0.5, 0.5, 0.001)\r\nf <- dnorm(x, mean = 0, sd = standard_error)\r\n\r\nggplot() +\r\n  geom_line(mapping = aes(x=x, y=f)) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\n…which should look a lot like the sampling distribution from your\r\nlast problem set!\r\n\r\n\r\nload('data/ces-2020/ps11-sampling-distribution.RData')\r\n\r\nggplot() + \r\n  geom_histogram(mapping = \r\n                   aes(x=sampling_distribution, \r\n                       y = ..density..), \r\n                 color = 'black', fill = 'gray',\r\n                 binwidth=0.02) + \r\n  geom_line(mapping = aes(x=x, y=f),\r\n            color = 'red') +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nStep\r\n3: Compare the observed statistic to the null distribution\r\nThere are two ways to do this:\r\nCompute the probability that you’d observe a statistic as extreme as\r\nyou did if the null hypothesis were true (p-value)\r\nConstruct an interval such that in 95% of samples, it would include\r\nthe truth (confidence interval).\r\nTo understand where p-values come from, let’s take a quick break for\r\nsecond-semester calculus:\r\n\r\n\r\n\r\n\r\n(Full Screen Version)\r\nOkay we’re back.\r\nFormally, our sample statistic is \\(\\hat{\\tau}\\). The sampling distribution of\r\nthis statistic is centered around the true population parameter \\(\\tau\\).\r\n\\[\\hat{\\tau} \\sim \\mathcal{N}\\left(\\tau,\r\n\\frac{\\sigma^2}{n_0} + \\frac{\\sigma^2}{n_1}\\right)\\]\r\nIf \\(\\tau = 0\\), what is the chance\r\nwe would observe a sample statistic as large as we did?\r\n\r\n\r\n\r\nAnd the p-value is equal to that shaded area.\r\n\\[p = \\int_{-\\infty}^{-\\hat{\\tau}}\r\n\\mathcal{N}\\left(0, \\frac{\\sigma^2}{n_0} + \\frac{\\sigma^2}{n_1}\\right) +\r\n\\int_{\\hat{\\tau}}^\\infty \\mathcal{N}\\left(0, \\frac{\\sigma^2}{n_0} +\r\n\\frac{\\sigma^2}{n_1}\\right)\\]\r\nWe can compute that quantity in R with the\r\npnorm() function.\r\n\r\n\r\np <- pnorm(-test_statistic, \r\n           mean = 0, \r\n           sd = standard_error,\r\n           lower.tail = TRUE) +\r\n  pnorm(test_statistic, \r\n        mean = 0, \r\n        sd = standard_error,\r\n        lower.tail = FALSE)\r\n\r\np\r\n\r\n\r\n   female \r\n0.8719204 \r\n\r\nAppendix\r\nTo see why the standard error of the difference-in-means statistic is\r\nequal to \\(\\sqrt{\\frac{\\sigma^2}{n_0} +\r\n\\frac{\\sigma^2}{n_1}}\\), first recall that the variance of one\r\nsample mean is \\(Var(\\bar{X}) =\r\n\\frac{\\sigma^2}{N}\\). Now we draw two independent sample means\r\nand take the difference. What’s the variance of that statistic?\r\n\\[\r\nVar(\\bar{X_1} - \\bar{X_0}) = Var(\\bar{X_1}) + Var(\\bar{X_0}) - 2\r\nCov(\\bar{X_1},\\bar{X_0})\r\n\\]\r\nBecause the two samples are drawn independently, that final\r\ncovariance term equals zero. The standard error is the square root of\r\nthe variance, so:\r\n\\[\r\n\\sqrt{Var(\\bar{X}_1) + Var(\\bar{X}_2)} = \\sqrt{\\frac{\\sigma^2}{n_0} +\r\n\\frac{\\sigma^2}{n_1}}\r\n\\]\r\n\r\n\r\n\r\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\r\nResearch Design and Causality. S.l.: CHAPMAN & HALL CRC. https://theeffectbook.net/index.html.\r\n\r\n\r\nKellstedt, Paul M, and Guy D Whitten. 2018. The Fundamentals of\r\nPolitical Science Research. http://catalog.hathitrust.org/api/volumes/oclc/1021067350.html.\r\n\r\n\r\nIf you’re taking POLS 7010, then I assume you have this\r\nbook. If not, you may borrow it from others in the class or from me. Or\r\nbuy it, I guess. It’s a good book.↩︎\r\nThe reason why statisticians like means as a measure of\r\ncentral tendency is because of Central Limit Theorem! The sampling\r\ndistribution of the mean is normally distributed; no such guarantee for\r\nother statistics like medians or modes.↩︎\r\nBecause mathematicians associate timeless truth and\r\nbeauty with the ancient Greeks.↩︎\r\nAnd that’s saying something, because I recently learned\r\nabout sugar\r\ngliders.↩︎\r\n",
      "last_modified": "2023-08-16T15:28:19-04:00"
    },
    {
      "path": "week-13.html",
      "title": "Week 13",
      "description": "Statistical Inference, Part II\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nConfidence Intervals\r\nA Wrinkle:\r\nT-Tests\r\nSome Points of Caution\r\n1.\r\nP-values and confidence intervals don’t mean what most people think they\r\nmean\r\n2.\r\nDividing results into “significant” and “not significant” is\r\nsilly\r\n3. “Big Data” can be wildly\r\nmisleading\r\n\r\n\r\nConfidence Intervals\r\nPreviously, we introduced the p-value, the\r\nprobability of observing a value as extreme as your test statistic if\r\nthe null hypothesis were true. A complementary concept is the\r\nconfidence interval, which is a way to express the\r\nrange of values that is consistent with your observed statistic. A 95%\r\nconfidence interval, for example, is a set of values such that, in 95%\r\nof random samples, it will contain the true value.\r\nYou can construct a 95% confidence interval by taking your sample\r\nestimate plus and minus 2 standard errors.1 If\r\nthe sampling distribution is normal, then about 95% of the time that\r\nconfidence interval will contain the true population parameter.\r\nTo illustrate, let’s draw samples from CES and compute 95% confidence\r\nintervals for average age. The true average age is:\r\n\r\n\r\nload( here('data/ces-2020/cleaned-CES.RData') )\r\n\r\ntruth <- mean(ces$age)\r\n\r\ntruth\r\n\r\n\r\n[1] 48.38757\r\n\r\nLet’s draw a sample and compute a confidence interval.\r\n\r\n\r\nsample_size <- 100\r\n\r\nsample_ages <- ces |> \r\n  slice_sample(n = sample_size) |> \r\n  pull(age)\r\n\r\nestimate <- mean(sample_ages)\r\n\r\nestimate\r\n\r\n\r\n[1] 45.89\r\n\r\nstandard_error <- sd(sample_ages) / sqrt(sample_size)\r\n\r\nconfidence_interval <- c(estimate - 1.96 * standard_error, \r\n                         estimate + 1.96 * standard_error)\r\n\r\nconfidence_interval\r\n\r\n\r\n[1] 42.3209 49.4591\r\n\r\nIn class, we’ll do this a bunch of times, and see that the confidence\r\ninterval captures the truth 95% of the time!\r\nA Wrinkle: T-Tests\r\nUp to now, we’ve been working with sample sizes that are large enough\r\nfor their sampling distribution to be approximately normal. But the\r\ncentral limit theorem doesn’t hold unless there is a sufficiently large\r\nnumber of observations!\r\nHow small is too small? Well, try computing those confidence\r\nintervals again, but with a sample size of 10.\r\n\r\n\r\nsample_size <- 10\r\n\r\nsample_ages <- ces |> \r\n  slice_sample(n = sample_size) |> \r\n  pull(age)\r\n\r\nestimate <- mean(sample_ages)\r\n\r\nestimate\r\n\r\n\r\n[1] 44.3\r\n\r\nstandard_error <- sd(sample_ages) / sqrt(sample_size)\r\n\r\nconfidence_interval <- c(estimate - 1.96 * standard_error, \r\n                         estimate + 1.96 * standard_error)\r\n\r\nconfidence_interval\r\n\r\n\r\n[1] 30.65159 57.94841\r\n\r\nIf you repeat that enough times, you’ll see that the 95% confidence\r\ninterval only contains the truth about 91% of the time. Our statistical\r\ntest falls apart with small samples!\r\nWilliam\r\nSealy Gosset came across this problem while working at the Guinness\r\nBrewery in the early 1900s, conducting experiments to find the best\r\nvariety of barley. Working with samples too small to rely on normal\r\napproximations, he developed what we now call the Student’s t\r\ntest. 2\r\nIt goes like this. Instead of computing a statistic like the sample\r\nmean or difference-in-means, the researcher instead computes a\r\nt-statistic.\r\n\\[\r\nt = \\frac{\\text{statistic} - H_0}{\\text{standard error}}\r\n\\]\r\nEssentially, the \\(t\\) statistic\r\ntells you how many standard errors your observed statistic is from the\r\nnull hypothesis. Gosset showed that the sampling distribution for this\r\nstatistic follows a Student’s t distribution.\r\nTo demonstrate what that looks like, let’s draw a bunch of samples\r\nfrom CES, compute the \\(t\\) statistic\r\nfor the age of each sample, and see what the sampling distribution of\r\nthat \\(t\\) statistic looks like:\r\n\r\n\r\n# the true average age in the population\r\nmu <- mean(ces$age)\r\n\r\n# this function draws a sample of ages and computes the t statistic (how many standard errors away from the truth is the sample mean?)\r\nget_t_statistic <- function(n = 10){\r\n  \r\n  # draw the sample\r\n  sample_ages <- ces$age[ sample(1:nrow(ces), size = n) ]\r\n  \r\n  # xbar is the sample mean\r\n  xbar <- mean(sample_ages)\r\n  \r\n  # s is the sample standard deviation\r\n  s <- sd(sample_ages)\r\n  \r\n  # return the t statistic\r\n  (xbar - mu) / (s / sqrt(n))\r\n  \r\n}\r\n\r\n# generate a sampling distribution with n=10\r\nsample_size <- 10\r\nsampling_distribution <- replicate(25000,\r\n                                   get_t_statistic(n = sample_size))\r\n\r\np <- ggplot() +\r\n  geom_histogram(mapping = aes(x=sampling_distribution,\r\n                               y=..density..),\r\n                 binwidth = 0.15,\r\n                 color = 'black')\r\np\r\n\r\n\r\n\r\n\r\nThat sampling distribution looks like a bell curve, but let’s overlay\r\na normal distribution…\r\n\r\n\r\n# overlay normal distribution\r\nexpected_value <- mean(sampling_distribution)\r\nstandard_error <- sd(sampling_distribution)\r\n\r\nx <- seq(min(sampling_distribution),\r\n         max(sampling_distribution),\r\n         0.01)\r\nf_norm <- dnorm(x, mean = expected_value,\r\n                sd = standard_error)\r\n\r\np <- p +\r\n  geom_line(mapping = aes(x=x, y=f_norm),\r\n            color = 'red')\r\n\r\np\r\n\r\n\r\n\r\n\r\nThe normal distribution is all wrong! It’s not peaky enough, and\r\nthere are a bunch of outliers that the normal distribution doesn’t\r\nexpect. Now let’s overlay a Student’s \\(t\\) distribution with the dt()\r\nfunction…\r\n\r\n\r\n# overlay a Student's t distribution\r\nx <- seq(min(sampling_distribution),\r\n         max(sampling_distribution),\r\n         0.01)\r\nf_t <- dt(x, df = sample_size - 1)\r\n\r\np <- p +\r\n  geom_line(mapping = aes(x=x, y=f_t),\r\n            color = 'blue')\r\n\r\np\r\n\r\n\r\n\r\n\r\nMuch better.\r\nNote that this distinction between the t distribution and the normal\r\ndistribution only really matters when sample size is small. If we go\r\nback to drawing a sample of 100…\r\n\r\n\r\n# generate a sampling distribution with n=100\r\nsample_size <- 100\r\nsampling_distribution <- replicate(25000,\r\n                                   get_t_statistic(n = sample_size))\r\n\r\np <- ggplot() +\r\n  geom_histogram(mapping = aes(x=sampling_distribution,\r\n                               y=..density..),\r\n                 binwidth = 0.15,\r\n                 color = 'black')\r\n\r\n# overlay normal distribution\r\nexpected_value <- mean(sampling_distribution)\r\nstandard_error <- sd(sampling_distribution)\r\n\r\nx <- seq(min(sampling_distribution),\r\n         max(sampling_distribution),\r\n         0.01)\r\nf_norm <- dnorm(x, mean = expected_value,\r\n                sd = standard_error)\r\n\r\np <- p +\r\n  geom_line(mapping = aes(x=x, y=f_norm),\r\n            color = 'red')\r\n\r\n# overlay a Student's t distribution\r\nx <- seq(min(sampling_distribution),\r\n         max(sampling_distribution),\r\n         0.01)\r\nf_t <- dt(x, df = sample_size - 1)\r\n\r\np <- p +\r\n  geom_line(mapping = aes(x=x, y=f_t),\r\n            color = 'blue')\r\n\r\np\r\n\r\n\r\n\r\n\r\n…both distributions are essentially identical. But researchers tend\r\nto default to a t-test, because it works no matter what your sample size\r\nis.\r\nSome Points of Caution\r\nLet’s close our discussion of hypothesis testing with some notes of\r\ncaution. P-values, confidence intervals, and “statistical significance”\r\nare widely misused, abused, and misunderstood. It’s worth taking a\r\nmoment to consider what these tests can and cannot tell us.\r\n1.\r\nP-values and confidence intervals don’t mean what most people think they\r\nmean\r\nWhen discussing the results of a hypothesis test, it is common even\r\nfor knowledgeable researchers to slip into colloquial language, like\r\n“the p-value is 0.03, which means there’s a 3% chance that the null\r\nhypothesis is true”. Or “I’m 95% sure that the true value lies within\r\nthis confidence interval”. These interpretations seem sensible,\r\nbut they’re wrong in an important-to-understand way.\r\nA p-value tells us the probability of the observing your\r\nstatistic, conditional on the null hypothesis being true. But\r\nthat’s a bit of a bait-and-switch, because it’s not really the thing we\r\nwant to know. Instead, we we want to know the probability the null\r\nhypothesis is true, conditional on observing the statistic you did.\r\nFortunately, Bayes Theorem\r\ntells us how to go from one conditional probability to the other.\r\n\\[\r\nP(H_0 | \\text{statistic}) = \\frac{P(\\text{statistic}|H_0) \\times\r\nP(H_0)}{P(\\text{statistic})}\r\n\\]\r\nThe first term, \\(P(\\text{statistic}|H_0)\\), is your p-value.\r\nThe second term, \\(P(H_0)\\), is your\r\nprior, how likely you think the null hypothesis is before you\r\neven look at your data. The third term, \\(P(\\text{statistic})\\) turns out to be\r\nsurprisingly difficult to compute, which is why Bayesian statistics\r\ndidn’t really take off until we got really powerful personal\r\ncomputers.3 The key takeaway is that not all\r\np-values are created equal. A small p-value in favor of a dodgy\r\nhypothesis can still be considered weak evidence.\r\n\r\n2.\r\nDividing results into “significant” and “not significant” is silly\r\nHumans crave certainty, but a p-value alone cannot give us the\r\ncertainty we crave. It cannot tell us whether our result is “real” or\r\nnot. So over the past century, the scientific community has developed a\r\nconvention of dividing results into “statistically significant” and “not\r\nstatistically significant” based on whether the p-value in a null\r\nhypothesis test is less than 0.05 – that is, if the chance of the null\r\nhypothesis producing our result is less than five percent.4\r\nBut this convention is silly at best, harmful at worst. Silly because\r\nthere’s no meaningful sense in which a study with p=0.049 is\r\n“significant” but a study with p=0.051 is “insignificant”. Imposing an\r\narbitrary threshold does not add clarity to the research prcess, and\r\nwhen combined with publication bias – the tendency of\r\njournals to only publish statistically significant results – the\r\npractice only serves to distort the published literature with wildly\r\nexaggerated claims. Gelman and Carlin (2014) call this “Type M error” and it\r\nworks like this:\r\nSuppose 200 researchers each survey a random sample of 100 Americans\r\nto see if people who use social media are more likely to support an\r\nassault rifle ban. Their study only gets published if their p-value is\r\nless than 0.05.\r\n\r\n\r\nload( here('data/ces-2020/cleaned-CES.RData') )\r\n\r\n# clean up the data\r\nces <- ces |> \r\n  select(social_media_24h, assault_rifle_ban) |> \r\n  # remove rows with missing data\r\n  na.omit() |> \r\n  # convert to 0-1\r\n  mutate(social_media_24h = as.numeric(social_media_24h == 'Yes'),\r\n         assault_rifle_ban = as.numeric(assault_rifle_ban == 'Support'))\r\n\r\n# the true difference in the population\r\nces |> \r\n  group_by(social_media_24h) |> \r\n  summarize(pct_support_ban = mean(assault_rifle_ban))\r\n\r\n\r\n# A tibble: 2 × 2\r\n  social_media_24h pct_support_ban\r\n             <dbl>           <dbl>\r\n1                0           0.641\r\n2                1           0.644\r\n\r\nThe true difference is basically zero; social media users are 0.3%\r\nmore likely to support an assault rifle ban. But our 200 scientists get\r\nresults all over the map:\r\n\r\n\r\nresults <- replicate(200,\r\n                     ces |> \r\n                       slice_sample(n = 100) |> \r\n                       lm(assault_rifle_ban ~ social_media_24h, data = _) |> \r\n                       coef() |> \r\n                       last())\r\n\r\np <- ggplot(mapping = aes(x=results)) + \r\n  geom_histogram(color = 'black') +\r\n  theme_minimal() +\r\n  labs(x='Estimate',\r\n       y = 'Number of Studies')\r\n\r\np\r\n\r\n\r\n\r\n\r\nThe standard error of the difference-in-means estimator is roughly\r\n0.1, so any estimate greater than 0.2 or less than -0.2 will be deemed\r\n“statistically significant” and published. And we’ll have roughly five\r\nstudies saying that social media makes you 20% more liberal, and five\r\nstudies saying that social media makes you 20% more conservative.\r\nThis example is somewhat contrived, but the replication\r\ncrisis in psychology (Button et al. 2013; Loken and Gelman 2017) has highlighted\r\nhow small sample sizes and a statistical significance requirement for\r\npublication have resulted in a literature littered with extreme claims\r\nthat fail to replicate.\r\nTo make matters worse, Hauer (2004) describes how the dominance of\r\nstatistical significance testing has imposed enormous costs when applied\r\nlife-and-death questions like “does increasing speed limits cause more\r\nauto fatalities?”:\r\n\r\nThe problem is clear. Researchers obtain real data which, while\r\nnoisy, time and again point in a certain direction. However, instead of\r\nsaying: “here is my estimate of the safety effect, here is its\r\nprecision, and this is how what I found relates to previous findings”,\r\nthe data is processed by NHST, and the researcher says, correctly but\r\npointlessly: “I cannot be sure that the safety effect is not zero”.\r\nOccasionally, the researcher adds, this time incorrectly and\r\nunjustifiably, a statement to the effect that: “since the result is not\r\nstatistically significant, it is best to assume the safety effect to be\r\nzero”. In this manner, good data are drained of real content, the\r\ndirection of empirical conclusions reversed, and ordinary human and\r\nscientific reasoning is turned on its head for the sake of a venerable\r\nritual.\r\n\r\nMany scientists think the problem is that 0.05 is too high a\r\nthreshold, and advocate using something like 0.005 instead (Benjamin et al. 2018). With respect to\r\nthe 72 authors of that paper I just cited (many of whom I deeply\r\nadmire), my own preference is to abandon statistical significance rather\r\nthan redefine it. Reducing the threshold to 0.005 solves neither the\r\nharms discussed by Hauer\r\n(2004) nor the\r\nType M distortions discussed by Gelman and Carlin (2014). Those can only be solved by\r\nresearch transparency and human judgment.\r\n\r\n3. “Big Data” can be wildly\r\nmisleading\r\nIn November 2022, Elon Musk conducted the following Twitter poll.\r\n\r\nWith over 15 million votes, the poll estimated that 51.8% of Twitter\r\nusers wanted Trump back on the platform. Let’s put a 95% confidence\r\ninterval around that estimate.\r\n\r\n\r\n# a bit over 15 million respondents\r\nn <- 15085458\r\n\r\n# here's that poll as a vector\r\npoll <- c(\r\n  rep(1, n * 0.518),\r\n  rep(0, n * 0.482)\r\n)\r\n\r\n\r\n# approximately normal sampling distribution,\r\n# mean = 51.8\r\n# standard error = sd / sqrt(n) \r\nestimate <- mean(poll)\r\nsd <- sd(poll)\r\nse <- sd / sqrt(n)\r\n\r\n# the 95% confidence interval is plus/minus 1.96 standard errors\r\nconfidence_interval <- c(\r\n  estimate - 1.96*se,\r\n  estimate + 1.96*se\r\n)\r\n\r\nconfidence_interval\r\n\r\n\r\n[1] 0.5177479 0.5182522\r\n\r\nOur 95% confidence interval ranges from 51.77% to 51.83%. By the\r\nstandards of the data we’ve been working with in class, this is an\r\nincredibly precise estimate! With giant sample sizes come teeny\r\ntiny standard errors.\r\nBut, what does that actually mean? Does it mean we’re 95% confident\r\nthat somewhere between 51.77% and 51.83% of Twitter users support\r\nreinstating Donald Trump?\r\nHere’s where insisting on the pedantic definition of confidence\r\nintervals really starts to come in handy. A confidence interval doesn’t\r\nsay “I’m 95% confident the value is in here”. It says “if we draw a\r\nbunch of random samples from the population, then the confidence\r\ninterval will contain the true value 95% of the time”. But there’s no\r\nreason to believe that Musk’s poll is a random sample. Certain people\r\nwere more likely than others to see and respond to the poll, and it’s\r\nunlikely that the people who responded are perfectly representative of\r\nthe population of Twitter users.\r\nWhat inferences can we make about the population using this sample?\r\nThe surprising answer is…basically nothing! Because we have no way of\r\nknowing whether Trump supporters were more or less likely to answer\r\nMusk’s poll (and by how much), even a sample size as large as 15 million\r\ndoesn’t permit us to make valid inferences beyond the sample. In fact, a\r\ntruly random sample of 1,000 Twitter users would do a much\r\nbetter job estimating what the broader population thinks than this\r\nnon-representative poll of 15 million.\r\nThe key takeaway here is that data quality matters far more\r\nthan data quantity. For more on this “Big Data Paradox”, I\r\nhighly recommend the Bradley et al. (2021) paper on how surveys of millions\r\nof social media users wildly mis-estimated the COVID-19 vaccination\r\nrate, while a carefully-constructed random sample of 1,000 performed\r\nmuch better. For a more technical (but still delightful) description of\r\nthe trouble with Big Data, see Meng (2018).\r\n\r\n\r\n\r\n\r\nBenjamin, Daniel J., James O. Berger, Magnus Johannesson, Brian A.\r\nNosek, E.-J. Wagenmakers, Richard Berk, Kenneth A. Bollen, et al. 2018.\r\n“Redefine Statistical Significance.” Nature Human\r\nBehaviour 2 (1): 6–10. https://doi.org/10.1038/s41562-017-0189-z.\r\n\r\n\r\nBradley, Valerie C., Shiro Kuriwaki, Michael Isakov, Dino Sejdinovic,\r\nXiao-Li Meng, and Seth Flaxman. 2021. “Unrepresentative Big\r\nSurveys Significantly Overestimated US Vaccine\r\nUptake.” Nature, December, 1–6. https://doi.org/10.1038/s41586-021-04198-4.\r\n\r\n\r\nButton, Katherine S., John P. A. Ioannidis, Claire Mokrysz, Brian A.\r\nNosek, Jonathan Flint, Emma S. J. Robinson, and Marcus R. Munafò. 2013.\r\n“Power Failure: Why Small Sample Size Undermines the Reliability\r\nof Neuroscience.” Nature Reviews Neuroscience 14 (5):\r\n365–76. https://doi.org/10.1038/nrn3475.\r\n\r\n\r\nFisher, Ronald A. 1925. Statistical Methods for Research\r\nWorkers. Oliver; Boyd.\r\n\r\n\r\nGelman, Andrew, and John Carlin. 2014. “Beyond Power Calculations:\r\nAssessing Type s (Sign) and Type m (Magnitude) Errors.”\r\nPerspectives on Psychological Science, 1–11. https://doi.org/10.1177/1745691614551642.\r\n\r\n\r\nHauer, Ezra. 2004. “The Harm Done by Tests of\r\nSignificance.” Accident Analysis & Prevention 36\r\n(3): 495–500. https://doi.org/10.1016/S0001-4575(03)00036-8.\r\n\r\n\r\nLoken, Eric, and Andrew Gelman. 2017. “Measurement Error and the\r\nReplication Crisis.” Science 355 (6325): 584–85. https://doi.org/10.1126/science.aal3618.\r\n\r\n\r\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\r\nwith Examples in r and Stan. 2nd ed. CRC Texts in Statistical\r\nScience. Boca Raton: Taylor; Francis, CRC Press.\r\n\r\n\r\nMeng, Xiao Li. 2018. “Statistical Paradises and Paradoxes in Big\r\nData (I): Law of Large Populations, Big Data\r\nParadox, and the 2016 Us Presidential Election.” Annals of\r\nApplied Statistics 12 (2): 685–726. https://doi.org/10.1214/18-AOAS1161SF.\r\n\r\n\r\nAgain, technically 1.96 standard errors, but 2 standard\r\nerrors is a useful shorthand.↩︎\r\nGosset published his results under the pseudonym\r\n“Student”, because his employer did not want other breweries to catch on\r\nand start using t-tests.↩︎\r\nIf all this intrigues you, may I suggest reading my\r\nfavorite statistics book of all time – Statistical Rethinking\r\n(McElreath\r\n2020)? I considered assigning it for this class, but honestly\r\nyou need to have first thought about statistics before you can benefit\r\nfrom rethinking about statistics.↩︎\r\nRemarkably, the entire edifice of p < 0.05 denoting\r\n“statistical significance” is drawn from an offhand comment Ronald\r\nFisher wrote on page\r\n45 of Statistical Methods for Research Workers (Fisher 1925), calling it “convenient”.↩︎\r\n",
      "last_modified": "2023-08-16T15:28:37-04:00"
    }
  ],
  "collections": []
}
