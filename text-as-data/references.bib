
@book{grimmerTextDataNew2021,
	title = {Text As Data: a new framework for machine learning and the social sciences.},
	author = {Grimmer, Justin and Stewart, Brandon M. and Roberts, Margaret E.},
	year = {2021},
	date = {2021},
	publisher = {Princeton University Press},
	note = {OCLC: 1246624235},
	address = {S.l.},
	langid = {English}
}

@article{torres2021,
	title = {Learning to See: Convolutional Neural Networks for the Analysis of Social Science Data},
	author = {Torres, Michelle and {Cant√∫}, Francisco},
	year = {2021},
	month = {04},
	date = {2021-04-16},
	journal = {Political Analysis},
	pages = {1--19},
	doi = {10.1017/pan.2021.9},
	url = {https://www.cambridge.org/core/product/identifier/S1047198721000097/type/journal_article},
	langid = {en}
}

@techreport{schoenfeldDiscursiveLandscapesUnsupervised2018,
	title = {Discursive Landscapes and Unsupervised Topic Modeling in IR: A Validation of Text-As-Data Approaches through a New Corpus of UN Security Council Speeches on Afghanistan},
	author = {Schoenfeld, Mirco and Eckhard, Steffen and Patz, Ronny and van Meegdenburg, Hilde},
	year = {2018},
	month = {10},
	date = {2018-10-12},
	doi = {10.48550/arXiv.1810.05572},
	url = {http://arxiv.org/abs/1810.05572},
	note = {DOI: 10.48550/arXiv.1810.05572
arXiv:1810.05572 [cs]
type: article}
}

@article{wilkersonLargeScaleComputerizedText2017,
	title = {Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges},
	author = {Wilkerson, John and Casas, Andreu},
	year = {2017},
	month = {05},
	date = {2017-05-11},
	journal = {Annual Review of Political Science},
	pages = {529--544},
	volume = {20},
	number = {1},
	doi = {10.1146/annurev-polisci-052615-025542},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-polisci-052615-025542},
	langid = {en}
}

@book{mostellerInferenceDisputedAuthorship1964,
	title = {Inference and Disputed Authorship: The Federalist},
	author = {Mosteller, Frederick and Wallace, David L.},
	year = {1964},
	date = {1964},
	publisher = {Addison-Wesley}
}

@book{imaiQuantitativeSocialScience2017,
	title = {Quantitative social science: an introduction},
	author = {Imai, Kosuke},
	year = {2017},
	date = {2017},
	publisher = {Princeton University Press},
	note = {OCLC: ocn958799734},
	address = {Princeton}
}

@article{rodriguezWordEmbeddingsWhat2021,
	title = {Word Embeddings: What Works, What Doesn{\textquoteright}t, and How to Tell the Difference for Applied Research},
	author = {Rodriguez, Pedro L. and Spirling, Arthur},
	year = {2021},
	month = {05},
	date = {2021-05-06},
	journal = {The Journal of Politics},
	pages = {000--000},
	doi = {10.1086/715162},
	url = {https://www.journals.uchicago.edu/doi/10.1086/715162},
	note = {Publisher: The University of Chicago Press}
}

@article{benoit2016,
	title = {Crowd-sourced Text Analysis: Reproducible and Agile Production of Political Data},
	author = {Benoit, Kenneth and Conway, Drew and Lauderdale, Benjamin E. and Laver, Michael and Mikhaylov, Slava},
	year = {2016},
	month = {05},
	date = {2016-05},
	journal = {American Political Science Review},
	pages = {278--295},
	volume = {110},
	number = {2},
	doi = {10.1017/S0003055416000058},
	url = {https://www.cambridge.org/core/journals/american-political-science-review/article/abs/crowdsourced-text-analysis-reproducible-and-agile-production-of-political-data/EC674A9384A19CFA357BC2B525461AC3},
	note = {Publisher: Cambridge University Press},
	langid = {en}
}

@article{carlson2017,
	title = {A Pairwise Comparison Framework for Fast, Flexible, and Reliable Human Coding of Political Texts},
	author = {Carlson, David and Montgomery, Jacob M.},
	year = {2017},
	month = {11},
	date = {2017-11},
	journal = {American Political Science Review},
	pages = {835--843},
	volume = {111},
	number = {4},
	doi = {10.1017/S0003055417000302},
	url = {https://www.cambridge.org/core/product/identifier/S0003055417000302/type/journal_article},
	langid = {en}
}

@book{grimmerRepresentationalStyleCongress2013,
	title = {Representational style in Congress: what legislators say and why it matters},
	author = {Grimmer, Justin},
	year = {2013},
	date = {2013},
	publisher = {Cambridge University Press},
	address = {New York}
}

@book{grimmerTextDataNew2022,
	title = {Text as Data: A New Framework for Machine Learning and the Social Sciences},
	author = {Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
	year = {2022},
	month = {03},
	date = {2022-03-29},
	publisher = {Princeton University Press},
	address = {Princeton, New Jersey Oxford},
	langid = {English}
}

@article{spirlingWhyOpensourceGenerative2023,
	title = {Why open-source generative AI models are an ethical way forward for science},
	author = {Spirling, Arthur},
	year = {2023},
	month = {04},
	date = {2023-04-18},
	journal = {Nature},
	pages = {413--413},
	volume = {616},
	number = {7957},
	doi = {10.1038/d41586-023-01295-4},
	url = {https://www.nature.com/articles/d41586-023-01295-4},
	note = {Bandiera{\_}abtest: a
Cg{\_}type: World View
Number: 7957
Publisher: Nature Publishing Group
Subject{\_}term: Ethics, Machine learning, Technology, Scientific community},
	langid = {en}
}

@article{ornsteinHowTrainYour2022,
  title = {How to {{Train Your Stochastic Parrot}}: {{Large Language Models}} for {{Political Texts}}},
  author = {Ornstein, Joseph T and Blasingame, Elise N and Truscott, Jake S},
  year = {2022},
  abstract = {Large language models pre-trained on massive corpora of text from the Internet have transformed the way that computer scientists approach natural language processing over the past five years. But these ``foundation models'' have yet to see widespread adoption in the social sciences, partly due to their novelty and upfront costs. In this paper, we demonstrate that such models can be effectively applied to a wide variety of text-as-data tasks in political science \textendash{} including sentiment analysis, ideological scaling, and topic modeling. In a series of pre-registered analyses, this approach outperforms conventional supervised learning methods without the need for extensive data pre-processing or large sets of labeled training data. And performance is comparable to expert and crowd-coding methods at a fraction of the cost. We explore the accuracy-cost tradeoff associated with adding more model parameters, and discuss how best to adapt and validate the models for particular applications.},
  langid = {english},
  file = {C\:\\Users\\jo22058\\Zotero\\storage\\W77H8Y9S\\Ornstein et al. - How to Train Your Stochastic Parrot Large Languag.pdf}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  primaryclass = {cs},
  urldate = {2020-12-14},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\jo22058\\Zotero\\storage\\3YP5EZYG\\Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}


@article{brownLanguageModelsAre2020,
	title = {Language Models are Few-Shot Learners},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	month = {07},
	date = {2020-07-22},
	journal = {arXiv:2005.14165 [cs]},
	url = {http://arxiv.org/abs/2005.14165},
	note = {arXiv: 2005.14165}
}
