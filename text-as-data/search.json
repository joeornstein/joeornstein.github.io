{
  "articles": [
    {
      "path": "bag-of-words.html",
      "title": "The Bag of Words",
      "description": "What if we ignored everything we know about language and just counted the words? Would that get us anywhere?\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nStep 1: Choose a Unit of\r\nAnalysis\r\nStep 2: Tokenize\r\nStep 3: Reduce\r\nComplexity\r\nStep 4: Create\r\nthe Document-Feature Matrix\r\nPractice Problems\r\n\r\nWhenever we analyze a text as data, the first step after digitizing\r\nis to decide how we’re gonig to represent the text quantitatively. The\r\nmost straightforward such representation is the so-called “bag of\r\nwords”. We ignore word order, syntax, punctuation, meaning, and context,\r\nfocusing only on the frequency with which words appear in the text.\r\nThough this representation throws out a lot of detail, it can\r\nnevertheless be useful, depending on what you’re trying to\r\naccomplish.\r\nIn chapter 5 Grimmer, Stewart, and Roberts (2021)\r\npresent their “standard recipe” for representing a text corpus as a bag\r\nof words:\r\nChoose a unit of analysis\r\nTokenize\r\nReduce complexity\r\nCreate a document-feature matrix\r\nLet’s demonstrate this workflow using the State of the Union speeches\r\napplication from Chapter 5. Our objective is to describe what words\r\npresidents when discussing the topic of manufacturing.\r\nStep 1: Choose a Unit of\r\nAnalysis\r\nThe unit of analysis (or “document”) for this application is the\r\nsentence. We want to just look at the sentences that include a mention\r\nof manufacturing, ignoring the rest of the speeches. To do so, let’s\r\nload in the sotu dataset and use the excellent\r\nunnest_tokens() function from the tidytext\r\npackage to split the corpus into sentences.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\n\r\n# load the data from the sotu package\r\ndf <- sotu::sotu_meta |> \r\n  mutate(text = sotu::sotu_text,\r\n         speech_id = 1:length(sotu::sotu_text)) |> \r\n  select(speech_id, president, year, text)\r\n\r\ndf\r\n\r\n\r\n# A tibble: 236 x 4\r\n   speech_id president          year text                             \r\n       <int> <chr>             <int> <chr>                            \r\n 1         1 George Washington  1790 \"Fellow-Citizens of the Senate a~\r\n 2         2 George Washington  1790 \"\\n\\n Fellow-Citizens of the Sen~\r\n 3         3 George Washington  1791 \"\\n\\n Fellow-Citizens of the Sen~\r\n 4         4 George Washington  1792 \"Fellow-Citizens of the Senate a~\r\n 5         5 George Washington  1793 \"\\n\\n Fellow-Citizens of the Sen~\r\n 6         6 George Washington  1794 \"\\n\\n Fellow-Citizens of the Sen~\r\n 7         7 George Washington  1795 \"\\n\\nFellow-Citizens of the Sena~\r\n 8         8 George Washington  1796 \"\\n\\n Fellow-Citizens of the Sen~\r\n 9         9 John Adams         1797 \"\\n\\n Gentlemen of the Senate an~\r\n10        10 John Adams         1798 \"\\n\\n Gentlemen of the Senate an~\r\n# ... with 226 more rows\r\n\r\n# split into sentences\r\ntidy_sotu <- df |> \r\n  unnest_tokens(input = 'text',\r\n                output = 'sentence',\r\n                token = 'sentences') |> \r\n  # keep only the sentences that contain the word stem \"manufactu\"\r\n  filter(str_detect(sentence, 'manufactu'))\r\n\r\ntidy_sotu\r\n\r\n\r\n# A tibble: 507 x 4\r\n   speech_id president          year sentence                         \r\n       <int> <chr>             <int> <chr>                            \r\n 1         1 George Washington  1790 the advancement of agriculture, ~\r\n 2         3 George Washington  1791 your own observations in your re~\r\n 3         7 George Washington  1795 our agriculture, commerce, and m~\r\n 4         8 George Washington  1796 congress have repeatedly, and no~\r\n 5         8 George Washington  1796 as a general rule, manufactures ~\r\n 6         9 John Adams         1797 our agriculture, fisheries, arts~\r\n 7        12 John Adams         1800 the manufacture of arms within t~\r\n 8        12 John Adams         1800 at a considerable expense to the~\r\n 9        13 Thomas Jefferson   1801 agriculture, manufactures, comme~\r\n10        14 Thomas Jefferson   1802 to cultivate peace and maintain ~\r\n# ... with 497 more rows\r\n\r\nNotice that the unnest_tokens() function converts all\r\nthe words to lower case. We now have a dataframe where the unit of\r\nanalysis is the sentence, containing every sentence in State of the\r\nUnion speeches from 1790 to 2016 containing a mention of\r\nmanufacturing.\r\nStep 2: Tokenize\r\nNow that we have the corpus of text we’re interested in studying, we\r\ncan tokenize to the word level (using the same\r\nunnest_tokens() function) to create our bag of words.\r\n\r\n\r\ntidy_sotu <- tidy_sotu |> \r\n  unnest_tokens(input = 'sentence',\r\n                output = 'word') |> \r\n  # we're just interested in words that occur near manufacturing, so remove\r\n  # the manufacturing words themselves\r\n  filter(str_detect(word, 'manufactu', negate = TRUE))\r\n\r\ntidy_sotu\r\n\r\n\r\n# A tibble: 22,380 x 4\r\n   speech_id president          year word       \r\n       <int> <chr>             <int> <chr>      \r\n 1         1 George Washington  1790 the        \r\n 2         1 George Washington  1790 advancement\r\n 3         1 George Washington  1790 of         \r\n 4         1 George Washington  1790 agriculture\r\n 5         1 George Washington  1790 commerce   \r\n 6         1 George Washington  1790 and        \r\n 7         1 George Washington  1790 by         \r\n 8         1 George Washington  1790 all        \r\n 9         1 George Washington  1790 proper     \r\n10         1 George Washington  1790 means      \r\n# ... with 22,370 more rows\r\n\r\nStep 3: Reduce Complexity\r\nBecause there are so many words in the English language, it\r\ncan be advantageous to reduce the sparseness of the bag of words a bit.\r\nWe can do so by removing “stop words” (words like articles and\r\nprepositions that are common but do not themselves contain meaning) and\r\nrepresenting words with their “stem” (so words like “duties” and “duty”\r\nare represented by the same word stem, “duti”).\r\n\r\n\r\n# remove stopwords\r\ntidy_sotu <- tidy_sotu |> \r\n  anti_join(get_stopwords())\r\n\r\ntidy_sotu\r\n\r\n\r\n# A tibble: 10,967 x 4\r\n   speech_id president          year word          \r\n       <int> <chr>             <int> <chr>         \r\n 1         1 George Washington  1790 advancement   \r\n 2         1 George Washington  1790 agriculture   \r\n 3         1 George Washington  1790 commerce      \r\n 4         1 George Washington  1790 proper        \r\n 5         1 George Washington  1790 means         \r\n 6         1 George Washington  1790 trust         \r\n 7         1 George Washington  1790 need          \r\n 8         1 George Washington  1790 recommendation\r\n 9         1 George Washington  1790 can           \r\n10         1 George Washington  1790 forbear       \r\n# ... with 10,957 more rows\r\n\r\nWord stemming is available courtesy of the SnoballC\r\npackage.\r\n\r\n\r\nlibrary(SnowballC)\r\n\r\nwordStem('duty')\r\n\r\n\r\n[1] \"duti\"\r\n\r\nwordStem('duties')\r\n\r\n\r\n[1] \"duti\"\r\n\r\ntidy_sotu <- tidy_sotu |> \r\n  mutate(word_stem = wordStem(word))\r\n\r\ntidy_sotu\r\n\r\n\r\n# A tibble: 10,967 x 5\r\n   speech_id president          year word           word_stem \r\n       <int> <chr>             <int> <chr>          <chr>     \r\n 1         1 George Washington  1790 advancement    advanc    \r\n 2         1 George Washington  1790 agriculture    agricultur\r\n 3         1 George Washington  1790 commerce       commerc   \r\n 4         1 George Washington  1790 proper         proper    \r\n 5         1 George Washington  1790 means          mean      \r\n 6         1 George Washington  1790 trust          trust     \r\n 7         1 George Washington  1790 need           need      \r\n 8         1 George Washington  1790 recommendation recommend \r\n 9         1 George Washington  1790 can            can       \r\n10         1 George Washington  1790 forbear        forbear   \r\n# ... with 10,957 more rows\r\n\r\nStep 4: Create the\r\nDocument-Feature Matrix\r\nThis step is only necessary if you need a document-feature matrix for\r\na subsequent statistical model. If not, it can be useful for\r\nvisualization and summary statistics to keep the word counts in a tidy\r\ndataframe.\r\nHere’s a visualization of the word stems that presidents most\r\ncommonly use when discussing manufacturing.\r\n\r\n\r\nlibrary(wordcloud2)\r\n\r\ntidy_sotu |> \r\n  count(word_stem) |> \r\n  wordcloud2()\r\n\r\n\r\n\r\n{\"x\":{\"word\":[\"01\",\"03\",\"08\",\"1\",\"1,214,023\",\"1,232,839,670\",\"1,810,256\",\"10\",\"10,825,115.21\",\"100\",\"102,473\",\"103,884,274\",\"105\",\"114,416,547\",\"11th\",\"12\",\"12,810,671\",\"13\",\"133,058,720.81\",\"133,756\",\"14\",\"14,023,682\",\"144\",\"14th\",\"15\",\"15,548,757\",\"15.17\",\"15.54\",\"150\",\"16,838,240\",\"162,544,715\",\"17,392,099\",\"172,726\",\"18\",\"1815\",\"1816\",\"1824\",\"1842\",\"1849\",\"1864\",\"1866\",\"1876\",\"1880\",\"1883\",\"1884\",\"1890\",\"1891\",\"1892\",\"1896\",\"1897\",\"1898\",\"1899\",\"19\",\"19,038,665.81\",\"1900\",\"1906\",\"1909\",\"1911\",\"1928\",\"1979\",\"1990\",\"19ii\",\"1st\",\"2\",\"2,623,089\",\"2,900,735,884\",\"2,934,876\",\"20\",\"200\",\"208,900,415\",\"20th\",\"21\",\"21,462,534.34\",\"21.09\",\"22,083\",\"22,350,906\",\"23\",\"24\",\"250\",\"27,285,624.78\",\"28\",\"28,617,898.62\",\"285,401\",\"3\",\"3,837,112\",\"30\",\"30,315,130.68\",\"30,454,476\",\"31,414,788.04\",\"31,889,711.74\",\"32\",\"32,548,983.07\",\"33\",\"33,463,398\",\"339,592,146\",\"35\",\"36,230,522\",\"37,426,262\",\"375,143\",\"38,464,965\",\"39,515,421\",\"3d\",\"3rd\",\"4\",\"4,074,238\",\"4,891\",\"40\",\"41,309\",\"41,464,599\",\"433,851,756\",\"45,846,197\",\"47,103,248\",\"47,977,137.63\",\"49,686,705\",\"49,949,128\",\"5\",\"50\",\"500,000\",\"53,842,292\",\"560,000,000\",\"59,000,000\",\"6\",\"6,000\",\"6,377,925.09\",\"6,469,643.04\",\"63\",\"65\",\"667,697,693\",\"67\",\"67,133,383\",\"7\",\"7,500\",\"7,525,000\",\"7,670,493\",\"700\",\"705,123,955\",\"724,964,852\",\"740,513,609\",\"76,241\",\"79,768,972\",\"80\",\"800,000\",\"807,646,992\",\"83\",\"84\",\"85,259,250.25\",\"90\",\"900,000\",\"907,500,000\",\"92,546,999\",\"93\",\"94\",\"94,720,260.55\",\"abandon\",\"abid\",\"abil\",\"abl\",\"abolit\",\"abound\",\"abridg\",\"abroad\",\"abrog\",\"absenc\",\"absolut\",\"abstract\",\"abund\",\"abus\",\"academi\",\"acced\",\"access\",\"accid\",\"accommod\",\"accompani\",\"accomplish\",\"accord\",\"accordingli\",\"account\",\"accumul\",\"accuraci\",\"achiev\",\"acknowledg\",\"acquir\",\"acr\",\"across\",\"act\",\"action\",\"activ\",\"actual\",\"acut\",\"ad\",\"adapt\",\"add\",\"addit\",\"address\",\"adequ\",\"adjac\",\"adjoin\",\"adjud\",\"adjudg\",\"adjust\",\"adjut\",\"administ\",\"administr\",\"admir\",\"admit\",\"admonish\",\"adopt\",\"advanc\",\"advantag\",\"adventur\",\"advers\",\"advis\",\"advocaci\",\"aeronaut\",\"affair\",\"affect\",\"afford\",\"africa\",\"agent\",\"aggrav\",\"aggreg\",\"aggressor\",\"ago\",\"agre\",\"agreement\",\"agricultur\",\"agriculturist\",\"aid\",\"air\",\"airwai\",\"alcohol\",\"alien\",\"alik\",\"alleg\",\"allevi\",\"allot\",\"allow\",\"almost\",\"alon\",\"along\",\"alreadi\",\"also\",\"alter\",\"altern\",\"although\",\"altogeth\",\"alwai\",\"amend\",\"america\",\"america'\",\"american\",\"amic\",\"amidst\",\"among\",\"amount\",\"ampl\",\"anatom\",\"andra\",\"animos\",\"annex\",\"announc\",\"annual\",\"annum\",\"anoth\",\"anti\",\"anticip\",\"antonio\",\"anxious\",\"anyth\",\"appal\",\"appar\",\"appear\",\"appli\",\"applianc\",\"applic\",\"appoint\",\"appreci\",\"apprehens\",\"appropri\",\"approv\",\"approxim\",\"april\",\"arbitrari\",\"argentin\",\"aris\",\"arisen\",\"arm\",\"armament\",\"armi\",\"armor\",\"armori\",\"around\",\"arrang\",\"arrest\",\"arriv\",\"arsen\",\"art\",\"articl\",\"artist\",\"ascertain\",\"ascrib\",\"asia\",\"asid\",\"ask\",\"aspect\",\"assembl\",\"assess\",\"assist\",\"associ\",\"assum\",\"assur\",\"atlant\",\"attach\",\"attain\",\"attempt\",\"attend\",\"attent\",\"attest\",\"attornei\",\"attract\",\"attribut\",\"auction\",\"augment\",\"auspic\",\"australia\",\"author\",\"auto\",\"automobil\",\"auxiliari\",\"avail\",\"averag\",\"avert\",\"avoid\",\"avow\",\"award\",\"ayer\",\"back\",\"bad\",\"baker\",\"balanc\",\"bank\",\"banker\",\"bankruptci\",\"base\",\"basi\",\"batteri\",\"battl\",\"bear\",\"beat\",\"becom\",\"beg\",\"began\",\"begin\",\"begun\",\"behalf\",\"behind\",\"behold\",\"belief\",\"believ\",\"belong\",\"benefici\",\"benefit\",\"berlin\",\"besid\",\"best\",\"bestow\",\"better\",\"beyond\",\"bia\",\"bid\",\"big\",\"bigger\",\"bill\",\"billion\",\"billionair\",\"bipartisan\",\"blacksmith\",\"blend\",\"bless\",\"blueprint\",\"board\",\"boat\",\"bodi\",\"bond\",\"border\",\"bought\",\"bounti\",\"brain\",\"branch\",\"brandon\",\"brazil\",\"breadstuff\",\"breakthrough\",\"breech\",\"bremen\",\"brief\",\"bring\",\"britain\",\"british\",\"broad\",\"broaden\",\"broadest\",\"broken\",\"brought\",\"budget\",\"bui\",\"build\",\"built\",\"bulk\",\"bulletin\",\"burden\",\"bureau\",\"burthen\",\"buse\",\"busi\",\"butcher\",\"cadet\",\"calcul\",\"california\",\"call\",\"calm\",\"came\",\"can\",\"candid\",\"cane\",\"cannon\",\"capabl\",\"capac\",\"capit\",\"capitalist\",\"captain\",\"care\",\"career\",\"carefulli\",\"cargo\",\"carolina\",\"carpent\",\"carri\",\"case\",\"catalogu\",\"caus\",\"caution\",\"cavalri\",\"ceas\",\"celebr\",\"cement\",\"cemeteri\",\"censu\",\"census\",\"cent\",\"center\",\"central\",\"certain\",\"certainli\",\"certif\",\"cessat\",\"chagr\",\"chanc\",\"chang\",\"charact\",\"character\",\"charg\",\"chargeabl\",\"charter\",\"cheap\",\"cheapen\",\"cheaper\",\"check\",\"cheek\",\"chemic\",\"cherish\",\"cheroke\",\"chew\",\"chief\",\"chiefli\",\"children\",\"chile\",\"china\",\"chines\",\"chosen\",\"christendom\",\"cigar\",\"cigarett\",\"circuit\",\"circul\",\"circumst\",\"citi\",\"citizen\",\"citizenship\",\"civil\",\"clad\",\"claim\",\"class\",\"classif\",\"clean\",\"clear\",\"cleverli\",\"climat\",\"clipper\",\"clock\",\"close\",\"closer\",\"cloth\",\"coal\",\"coast\",\"coastwis\",\"code\",\"coffe\",\"coinag\",\"cold\",\"collaps\",\"collater\",\"collect\",\"colleg\",\"coloni\",\"color\",\"columbia\",\"combat\",\"combin\",\"come\",\"comfort\",\"command\",\"commenc\",\"commend\",\"commensur\",\"commerc\",\"commerci\",\"commingl\",\"commiss\",\"commissari\",\"commission\",\"commit\",\"committe\",\"commod\",\"common\",\"commun\",\"compani\",\"compar\",\"comparison\",\"compassion\",\"compel\",\"compens\",\"compensatori\",\"compet\",\"competit\",\"competitor\",\"compil\",\"complain\",\"complaint\",\"complet\",\"complex\",\"complianc\",\"complic\",\"compos\",\"composit\",\"compound\",\"conced\",\"concentr\",\"concern\",\"conclus\",\"concours\",\"concur\",\"condit\",\"conduct\",\"confederaci\",\"confer\",\"confid\",\"confin\",\"confirm\",\"conflict\",\"conform\",\"congratul\",\"congress\",\"congression\",\"connect\",\"consequ\",\"conserv\",\"consid\",\"consider\",\"consign\",\"consist\",\"consolid\",\"conspiraci\",\"constant\",\"constantli\",\"constitu\",\"constitut\",\"constru\",\"construct\",\"constructor\",\"consular\",\"consult\",\"consum\",\"consumm\",\"consumpt\",\"contact\",\"contain\",\"contempl\",\"contend\",\"content\",\"contigu\",\"contin\",\"conting\",\"continu\",\"contract\",\"contractor\",\"contrari\",\"contribut\",\"control\",\"conveni\",\"convent\",\"convers\",\"convict\",\"convinc\",\"cooper\",\"coordin\",\"cordial\",\"corollari\",\"corp\",\"corpor\",\"correspond\",\"cost\",\"cotton\",\"council\",\"counter\",\"counteract\",\"countless\",\"countri\",\"countrymen\",\"courag\",\"cours\",\"court\",\"cover\",\"creat\",\"credit\",\"cri\",\"crime\",\"crimin\",\"crisi\",\"critic\",\"crop\",\"crowd\",\"crown\",\"cuban\",\"cultiv\",\"cultur\",\"currenc\",\"current\",\"custom\",\"cut\",\"dai\",\"dairi\",\"danger\",\"data\",\"date\",\"day'\",\"deal\",\"dealer\",\"dealt\",\"debt\",\"decad\",\"decai\",\"decemb\",\"decid\",\"decis\",\"decker\",\"declar\",\"declin\",\"decre\",\"decreas\",\"deduct\",\"deem\",\"deepli\",\"defeat\",\"defend\",\"defens\",\"defici\",\"deficit\",\"defin\",\"definit\",\"degre\",\"delai\",\"deliber\",\"demand\",\"demonstr\",\"denatur\",\"deni\",\"denounc\",\"dep't\",\"depart\",\"depend\",\"deposit\",\"depot\",\"depress\",\"depriv\",\"derang\",\"deriv\",\"describ\",\"descript\",\"deserv\",\"design\",\"desir\",\"destitut\",\"destroi\",\"destruct\",\"detail\",\"detect\",\"determin\",\"detroit\",\"develop\",\"devis\",\"dexter\",\"diem\",\"differ\",\"difficult\",\"difficulti\",\"diffus\",\"dimens\",\"diminish\",\"diminut\",\"diplomat\",\"direct\",\"directli\",\"disadvantag\",\"disast\",\"disastr\",\"discharg\",\"disclos\",\"discontinu\",\"discourag\",\"discov\",\"discrimin\",\"discriminatori\",\"diseas\",\"disguis\",\"dishonest\",\"disintegr\",\"dismantl\",\"dispatch\",\"displai\",\"dispos\",\"dissatisfact\",\"dissolut\",\"dissolv\",\"distant\",\"distil\",\"distinguish\",\"distress\",\"distribut\",\"district\",\"disturb\",\"diversifi\",\"divis\",\"dock\",\"doctor\",\"doctrin\",\"dollar\",\"domest\",\"dominion\",\"done\",\"doubl\",\"doubt\",\"draw\",\"drawn\",\"dressmak\",\"drift\",\"drill\",\"drive\",\"driven\",\"drop\",\"drug\",\"due\",\"duti\",\"dutiabl\",\"dye\",\"dyestuff\",\"e\",\"eagerli\",\"earli\",\"earn\",\"earner\",\"earnest\",\"earth\",\"easi\",\"east\",\"eastern\",\"econom\",\"economi\",\"edict\",\"educ\",\"effect\",\"effectu\",\"effici\",\"effort\",\"eight\",\"either\",\"electr\",\"eleg\",\"element\",\"elementari\",\"eleven\",\"eleventh\",\"elimin\",\"els\",\"elsewher\",\"embarrass\",\"embrac\",\"emerg\",\"emperor\",\"empir\",\"emplac\",\"emploi\",\"employ\",\"employe\",\"empti\",\"enabl\",\"enact\",\"encount\",\"encourag\",\"end\",\"endang\",\"endeavor\",\"enemi\",\"energet\",\"energetx\",\"energi\",\"enforc\",\"engag\",\"engin\",\"england\",\"english\",\"enhanc\",\"enjoi\",\"enlarg\",\"enlighten\",\"enough\",\"ensu\",\"ensur\",\"enter\",\"enterpris\",\"entertain\",\"entir\",\"entitl\",\"entranc\",\"entrepreneur\",\"entri\",\"enumer\",\"envis\",\"envoi\",\"equal\",\"equip\",\"equit\",\"equival\",\"erad\",\"erron\",\"escap\",\"especi\",\"essenti\",\"establish\",\"estim\",\"etc\",\"europ\",\"european\",\"even\",\"event\",\"ever\",\"everi\",\"everyth\",\"everywher\",\"evid\",\"evidenc\",\"evil\",\"exact\",\"exactli\",\"examin\",\"exampl\",\"exce\",\"exceed\",\"exceedingli\",\"excel\",\"except\",\"exception\",\"excess\",\"exchang\",\"excis\",\"excit\",\"exclud\",\"exclus\",\"excus\",\"execut\",\"exempt\",\"exercis\",\"exert\",\"exhaust\",\"exhibit\",\"exhibitor\",\"exig\",\"exist\",\"expand\",\"expans\",\"expect\",\"expedi\",\"expend\",\"expenditur\",\"expens\",\"experi\",\"experienc\",\"experiment\",\"explos\",\"export\",\"exposit\",\"express\",\"extend\",\"extens\",\"extent\",\"extern\",\"extra\",\"extraordinari\",\"extravag\",\"extrem\",\"exuber\",\"fabric\",\"face\",\"facil\",\"facilit\",\"fact\",\"factor\",\"factori\",\"fail\",\"fair\",\"fairli\",\"faith\",\"faithless\",\"fall\",\"famili\",\"familiar\",\"far\",\"farm\",\"farmer\",\"farmer'\",\"fastest\",\"fate\",\"favor\",\"fear\",\"featur\",\"februari\",\"feder\",\"feel\",\"fellow\",\"felt\",\"ferment\",\"fertil\",\"field\",\"fill\",\"final\",\"financ\",\"financi\",\"find\",\"fine\",\"finish\",\"fire\",\"firm\",\"firmer\",\"first\",\"fiscal\",\"fish\",\"fisheri\",\"fishermen\",\"fit\",\"fitli\",\"five\",\"fix\",\"flax\",\"fleet\",\"flock\",\"florida\",\"flour\",\"flourish\",\"flow\",\"fluctuat\",\"foil\",\"follow\",\"food\",\"foodstuff\",\"foot\",\"forbear\",\"forbid\",\"forc\",\"forego\",\"foreign\",\"foremost\",\"foresight\",\"forest\",\"forg\",\"form\",\"former\",\"formerli\",\"fortif\",\"fortun\",\"forward\",\"foster\",\"found\",\"foundat\",\"four\",\"frame\",\"framer\",\"franc\",\"francisco\",\"fraud\",\"free\",\"freedom\",\"freer\",\"freight\",\"frequent\",\"fresh\",\"friendship\",\"fright\",\"front\",\"frontier\",\"fuel\",\"full\",\"fulli\",\"function\",\"fund\",\"fundament\",\"fur\",\"furnish\",\"futur\",\"gain\",\"garb\",\"gase\",\"gener\",\"geniu\",\"gentleman\",\"german\",\"get\",\"give\",\"given\",\"glad\",\"global\",\"globe\",\"go\",\"goe\",\"gold\",\"gone\",\"good\",\"govern\",\"government\",\"government'\",\"grade\",\"gradual\",\"graduat\",\"grain\",\"granari\",\"grant\",\"grate\",\"gratifi\",\"gratul\",\"graviti\",\"greas\",\"great\",\"greater\",\"greatest\",\"greatli\",\"grocer\",\"ground\",\"grow\",\"grower\",\"grown\",\"growth\",\"guarante\",\"guard\",\"guid\",\"gun\",\"gunboat\",\"habit\",\"half\",\"halt\",\"hamburg\",\"hand\",\"handi\",\"happen\",\"happi\",\"harbor\",\"hard\",\"hardli\",\"hardship\",\"harmon\",\"harmoni\",\"harvest\",\"hasten\",\"hastili\",\"head\",\"health\",\"healthi\",\"heavi\",\"heaviest\",\"heavili\",\"held\",\"help\",\"helpless\",\"hemp\",\"herd\",\"hereaft\",\"heretofor\",\"hide\",\"high\",\"higher\",\"highest\",\"highli\",\"hinder\",\"hire\",\"histori\",\"hit\",\"hitherto\",\"hold\",\"home\",\"honest\",\"honestli\",\"honor\",\"hope\",\"hostil\",\"hour\",\"hous\",\"household\",\"howev\",\"hub\",\"huge\",\"hum\",\"human\",\"hundr\",\"husbandman\",\"husbandri\",\"idea\",\"identifi\",\"idl\",\"illog\",\"illustr\",\"immedi\",\"immens\",\"immigr\",\"impair\",\"impart\",\"imparti\",\"impel\",\"imper\",\"imperi\",\"imperil\",\"implement\",\"impli\",\"impolit\",\"import\",\"importun\",\"impos\",\"imposit\",\"imposs\",\"impost\",\"impot\",\"impress\",\"improv\",\"impuls\",\"inadequ\",\"inaptli\",\"incalcul\",\"incent\",\"incid\",\"incident\",\"includ\",\"incom\",\"inconsider\",\"inconsist\",\"increas\",\"increasingli\",\"incur\",\"inde\",\"indebted\",\"independ\",\"indian\",\"indic\",\"indiffer\",\"indirect\",\"indiscrimin\",\"indispens\",\"individu\",\"induc\",\"industri\",\"inequ\",\"inevit\",\"inexpedi\",\"infant\",\"infantri\",\"inferior\",\"inflat\",\"inflict\",\"influenc\",\"inform\",\"infrastructur\",\"ingenu\",\"inhabit\",\"initi\",\"injur\",\"injuri\",\"injustic\",\"innocu\",\"innov\",\"innumer\",\"inquiri\",\"insert\",\"insist\",\"insourc\",\"instal\",\"instanc\",\"instead\",\"instil\",\"institut\",\"instrument\",\"insul\",\"insur\",\"intellig\",\"intend\",\"interchang\",\"intercours\",\"interdict\",\"interest\",\"interior\",\"interlocutori\",\"intern\",\"interrupt\",\"interst\",\"intim\",\"intoler\",\"intrins\",\"introduc\",\"introduct\",\"inur\",\"invent\",\"inventor\",\"invest\",\"investig\",\"invigor\",\"invit\",\"invoic\",\"involv\",\"ira\",\"iron\",\"irregular\",\"irrig\",\"island\",\"issu\",\"issuanc\",\"isthmu\",\"januari\",\"jealous\",\"jersei\",\"job\",\"join\",\"joiner\",\"joint\",\"judgment\",\"judici\",\"juli\",\"june\",\"juri\",\"just\",\"justic\",\"justifi\",\"justli\",\"jute\",\"keep\",\"kei\",\"kept\",\"kind\",\"kindr\",\"kingdom\",\"know\",\"knowledg\",\"known\",\"la\",\"labor\",\"lack\",\"laden\",\"lai\",\"laid\",\"lamp\",\"land\",\"languish\",\"larg\",\"larger\",\"last\",\"late\",\"latter\",\"launch\",\"law\",\"lawyer\",\"lead\",\"leadership\",\"leap\",\"learn\",\"least\",\"leav\",\"led\",\"left\",\"leg\",\"legisl\",\"legislatur\",\"legitim\",\"length\",\"less\",\"lessen\",\"lesson\",\"lest\",\"let\",\"letter\",\"level\",\"levi\",\"li\",\"liabl\",\"liber\",\"liberti\",\"licoric\",\"lieuten\",\"life\",\"lifesav\",\"like\",\"likewis\",\"limit\",\"line\",\"link\",\"liquor\",\"list\",\"literatur\",\"littl\",\"live\",\"load\",\"loan\",\"local\",\"logic\",\"long\",\"longer\",\"look\",\"loom\",\"loss\",\"lost\",\"low\",\"lower\",\"lowest\",\"lubeck\",\"lumber\",\"luxuri\",\"machineri\",\"made\",\"magazin\",\"magnet\",\"magnitud\",\"mai\",\"main\",\"maintain\",\"mainten\",\"major\",\"make\",\"man\",\"manag\",\"mani\",\"manifest\",\"manifestli\",\"manner\",\"manufactori\",\"march\",\"margin\",\"marin\",\"maritim\",\"mark\",\"market\",\"martial\",\"maryland\",\"mason\",\"mass\",\"match\",\"materi\",\"matter\",\"matur\",\"maximum\",\"mean\",\"measur\",\"meat\",\"mechan\",\"medic\",\"medicin\",\"medium\",\"meet\",\"melado\",\"melbourn\",\"member\",\"men\",\"menac\",\"mention\",\"mercantil\",\"merchandis\",\"merchant\",\"mere\",\"merit\",\"messag\",\"metal\",\"method\",\"mexico\",\"michigan\",\"midst\",\"might\",\"mileag\",\"militari\",\"mill\",\"millin\",\"million\",\"mind\",\"mine\",\"miner\",\"minimum\",\"minist\",\"minut\",\"mischief\",\"misdemeanor\",\"misl\",\"mississippi\",\"mode\",\"moder\",\"modern\",\"modif\",\"modifi\",\"molass\",\"molest\",\"moment\",\"monei\",\"monopoli\",\"monopolist\",\"month\",\"moral\",\"moreov\",\"morrow\",\"mortar\",\"motion\",\"move\",\"much\",\"multipli\",\"municip\",\"munit\",\"museum\",\"must\",\"mutual\",\"name\",\"narrow\",\"nation\",\"nation'\",\"nativ\",\"natur\",\"naval\",\"navi\",\"navig\",\"near\",\"nearli\",\"necess\",\"necessari\",\"necessarili\",\"necessit\",\"need\",\"needless\",\"negoti\",\"neighbor\",\"neither\",\"net\",\"netherland\",\"network\",\"neutral\",\"never\",\"nevertheless\",\"new\",\"newport\",\"next\",\"nine\",\"nitrat\",\"nitrogen\",\"nomin\",\"non\",\"noncommiss\",\"none\",\"nonimport\",\"nontax\",\"normal\",\"north\",\"northern\",\"northwest\",\"notabl\",\"noth\",\"notwithstand\",\"novemb\",\"now\",\"nuclear\",\"number\",\"numer\",\"nurseri\",\"nurtur\",\"object\",\"oblig\",\"observ\",\"obsolet\",\"obstacl\",\"obstruct\",\"obtain\",\"obviou\",\"occas\",\"occasion\",\"occup\",\"occupi\",\"occur\",\"ocean\",\"offens\",\"offer\",\"offic\",\"offici\",\"offset\",\"often\",\"ohio\",\"oil\",\"old\",\"oldenburg\",\"on\",\"oner\",\"onlin\",\"open\",\"oper\",\"opinion\",\"opium\",\"opportun\",\"opposit\",\"order\",\"orderli\",\"ordinari\",\"ordnanc\",\"oregon\",\"organ\",\"orlean\",\"ornament\",\"ostensibli\",\"other\",\"otherwis\",\"outfit\",\"outlai\",\"overbalanc\",\"overgrown\",\"overproduct\",\"overrul\",\"oversea\",\"overwhelm\",\"ow\",\"own\",\"owner\",\"ownership\",\"pace\",\"pacif\",\"pai\",\"paid\",\"pale\",\"panic\",\"paper\",\"paralyz\",\"paramount\",\"pari\",\"part\",\"partaken\",\"parti\",\"partial\",\"particip\",\"particl\",\"particular\",\"particularli\",\"partli\",\"partner\",\"partnership\",\"pass\",\"passag\",\"past\",\"patent\",\"patriot\",\"pauper\",\"payabl\",\"payment\",\"peac\",\"peacetim\",\"peck\",\"peculiar\",\"peculiarli\",\"pecuniari\",\"penalti\",\"pend\",\"pennsylvania\",\"pension\",\"peopl\",\"people'\",\"per\",\"perceiv\",\"percent\",\"percentag\",\"percept\",\"perfect\",\"perfectli\",\"perform\",\"perhap\",\"period\",\"perish\",\"perman\",\"permit\",\"person\",\"personnel\",\"pertain\",\"petit\",\"philadelphia\",\"philippin\",\"phosphor\",\"physician\",\"pictur\",\"pig\",\"pillar\",\"place\",\"plan\",\"plane\",\"plant\",\"planter\",\"plaster\",\"plate\",\"pleasant\",\"pledg\",\"plenti\",\"point\",\"poison\",\"polici\",\"polit\",\"pollut\",\"popul\",\"popular\",\"port\",\"portion\",\"portug\",\"posit\",\"possess\",\"possibl\",\"possibli\",\"post\",\"postag\",\"postmast\",\"pound\",\"powder\",\"power\",\"powerpl\",\"practic\",\"prais\",\"precari\",\"precaut\",\"precis\",\"predomin\",\"preemin\",\"preempt\",\"prefer\",\"preliminari\",\"prematur\",\"prepar\",\"prescrib\",\"present\",\"preserv\",\"press\",\"pressur\",\"presum\",\"prevail\",\"preval\",\"prevent\",\"previou\",\"price\",\"pride\",\"primari\",\"primarili\",\"princip\",\"principl\",\"print\",\"prior\",\"prioriti\",\"privat\",\"probabl\",\"problem\",\"proce\",\"proceed\",\"process\",\"procur\",\"produc\",\"product\",\"profess\",\"profession\",\"professor\",\"professorship\",\"profit\",\"profound\",\"program\",\"progress\",\"prohibit\",\"prohibitori\",\"projectil\",\"promin\",\"promis\",\"promot\",\"prompt\",\"promptitud\",\"promptli\",\"proof\",\"proper\",\"properli\",\"properti\",\"proport\",\"proportion\",\"propos\",\"prosecut\",\"prospect\",\"prosper\",\"prostrat\",\"protect\",\"prove\",\"provid\",\"provinc\",\"provis\",\"prudent\",\"prussia\",\"psychotrop\",\"public\",\"publish\",\"purchas\",\"pure\",\"purpos\",\"pursuit\",\"push\",\"put\",\"qualiti\",\"quantiti\",\"quantum\",\"quarter\",\"quartermast\",\"quest\",\"question\",\"quicken\",\"quietli\",\"quit\",\"quot\",\"race\",\"radic\",\"radioisotop\",\"railroad\",\"railwai\",\"rais\",\"raleigh\",\"rancher\",\"rang\",\"rank\",\"rant\",\"rapid\",\"rapidli\",\"rate\",\"ratifi\",\"ratio\",\"raw\",\"reach\",\"readi\",\"readjust\",\"real\",\"realiz\",\"realm\",\"reap\",\"reappropri\",\"reason\",\"reassur\",\"rebellion\",\"rebound\",\"rebuild\",\"receipt\",\"receiv\",\"recent\",\"recess\",\"reciproc\",\"reckless\",\"recogn\",\"recognit\",\"recoil\",\"recollect\",\"recommend\",\"record\",\"recoveri\",\"recur\",\"reduc\",\"reduct\",\"redund\",\"reestablish\",\"refer\",\"refineri\",\"reflect\",\"reform\",\"refrain\",\"regard\",\"region\",\"regret\",\"regul\",\"regular\",\"rejoic\",\"rel\",\"relat\",\"releas\",\"reli\",\"relief\",\"reliev\",\"religi\",\"remain\",\"remark\",\"remedi\",\"remind\",\"remit\",\"remnant\",\"remonstr\",\"remov\",\"remun\",\"remuner\",\"render\",\"renew\",\"reorgan\",\"repair\",\"repeal\",\"repeatedli\",\"repin\",\"replac\",\"replenish\",\"report\",\"repres\",\"represent\",\"republ\",\"request\",\"requir\",\"requisit\",\"research\",\"reserv\",\"resist\",\"resolut\",\"resourc\",\"respect\",\"respectfulli\",\"respons\",\"rest\",\"restor\",\"restrict\",\"result\",\"retail\",\"retain\",\"retali\",\"retard\",\"return\",\"revenu\",\"review\",\"revis\",\"reviv\",\"revoc\",\"revolut\",\"revuls\",\"reward\",\"reynold\",\"rhetor\",\"rhode\",\"rich\",\"rifl\",\"right\",\"rigor\",\"ripe\",\"rise\",\"risen\",\"rival\",\"rivalri\",\"road\",\"roar\",\"rock\",\"roll\",\"round\",\"rude\",\"ruin\",\"ruinou\",\"ruinous\",\"rule\",\"run\",\"rural\",\"rush\",\"russia\",\"sacr\",\"sacrific\",\"safe\",\"safeguard\",\"safeti\",\"sai\",\"said\",\"sail\",\"sale\",\"salutari\",\"sampl\",\"san\",\"satisfact\",\"satisfactori\",\"satisfactorili\",\"satisfi\",\"save\",\"scale\",\"scarc\",\"scene\",\"schedul\",\"scheme\",\"scienc\",\"scientif\",\"scour\",\"scrap\",\"scrupul\",\"scrutin\",\"sea\",\"seamstress\",\"season\",\"second\",\"secretari\",\"section\",\"sector\",\"secur\",\"seem\",\"seen\",\"select\",\"self\",\"selfish\",\"sell\",\"semimonthli\",\"send\",\"sensibl\",\"sent\",\"sentenc\",\"separ\",\"septemb\",\"sergeant\",\"seri\",\"seriou\",\"serious\",\"serv\",\"servant\",\"servic\",\"session\",\"set\",\"settl\",\"settlement\",\"settler\",\"seven\",\"seventh\",\"seventi\",\"sever\",\"shall\",\"shanghai\",\"shape\",\"share\",\"sharp\",\"shear\",\"shed\",\"sheep\",\"shell\",\"shelter\",\"shepherd\",\"shield\",\"shift\",\"shingl\",\"ship\",\"shipbuild\",\"shipper\",\"shipyard\",\"shock\",\"shop\",\"shore\",\"shortcom\",\"shorten\",\"show\",\"shrinkag\",\"shut\",\"side\",\"signific\",\"silent\",\"silk\",\"silver\",\"similar\",\"simpl\",\"simpli\",\"simplif\",\"sinc\",\"singl\",\"singular\",\"sink\",\"site\",\"situat\",\"sixti\",\"skeptic\",\"skill\",\"skin\",\"slide\",\"slight\",\"slightest\",\"slope\",\"slow\",\"small\",\"smaller\",\"smile\",\"smoke\",\"smuggl\",\"snuff\",\"social\",\"societi\",\"sociolog\",\"soil\",\"solar\",\"sold\",\"soldier\",\"sole\",\"solicit\",\"solid\",\"solitari\",\"solut\",\"someth\",\"sometim\",\"somewhat\",\"soon\",\"sorghum\",\"sort\",\"sought\",\"sound\",\"sourc\",\"south\",\"southern\",\"space\",\"spare\",\"speak\",\"speaker\",\"speci\",\"special\",\"specif\",\"specul\",\"speedi\",\"spend\",\"spirit\",\"spontan\",\"spread\",\"spring\",\"springfield\",\"spuriou\",\"stabil\",\"stabl\",\"staff\",\"stage\",\"stagnat\",\"stake\",\"stand\",\"standard\",\"stapl\",\"start\",\"starv\",\"state\",\"statement\",\"station\",\"statist\",\"statut\",\"statutori\",\"steadi\",\"steadili\",\"steam\",\"steamship\",\"steel\",\"step\",\"stifl\",\"still\",\"stimul\",\"stimulu\",\"stipul\",\"stock\",\"storag\",\"store\",\"storehous\",\"strain\",\"strength\",\"strict\",\"strictli\",\"strike\",\"strong\",\"strongli\",\"struggl\",\"student\",\"studi\",\"sturdi\",\"style\",\"subject\",\"submit\",\"subsequ\",\"subsid\",\"subsist\",\"substanc\",\"substanti\",\"substitut\",\"subtli\",\"succeed\",\"success\",\"successfulli\",\"sudden\",\"suffer\",\"suffic\",\"suffici\",\"sugar\",\"suggest\",\"suit\",\"suitabl\",\"sum\",\"summer\",\"superintend\",\"superior\",\"supersed\",\"supplement\",\"suppli\",\"support\",\"suppress\",\"sure\",\"surg\",\"surpass\",\"surplu\",\"surplus\",\"surrend\",\"surround\",\"survei\",\"surviv\",\"suspend\",\"suspens\",\"sustain\",\"sweden\",\"swell\",\"synthet\",\"system\",\"systemat\",\"tabl\",\"tailor\",\"tailoress\",\"take\",\"taken\",\"talent\",\"talk\",\"tariff\",\"task\",\"tax\",\"taxat\",\"tea\",\"teach\",\"tech\",\"technic\",\"technologi\",\"temporari\",\"tend\",\"tendenc\",\"tender\",\"tenth\",\"term\",\"termin\",\"tern\",\"territori\",\"test\",\"testimoni\",\"texa\",\"thenc\",\"theori\",\"thereaft\",\"therebi\",\"therefor\",\"therein\",\"thereof\",\"thereon\",\"thereto\",\"thereupon\",\"thing\",\"think\",\"third\",\"thirst\",\"thoroli\",\"thorough\",\"thoroughli\",\"though\",\"thought\",\"thousand\",\"three\",\"thrive\",\"throughout\",\"thrown\",\"thu\",\"ti\",\"tile\",\"till\",\"time\",\"timeli\",\"tin\",\"tip\",\"titl\",\"tobacco\",\"todai\",\"togeth\",\"toil\",\"told\",\"tonight\",\"tonnag\",\"took\",\"tool\",\"topic\",\"torpedo\",\"total\",\"touch\",\"toward\",\"town\",\"trace\",\"trade\",\"trade'\",\"tradesman\",\"tradesmen\",\"traffic\",\"train\",\"tranquil\",\"transact\",\"transmiss\",\"transmit\",\"transport\",\"travel\",\"treacheri\",\"treasuri\",\"treasury'\",\"treat\",\"treati\",\"treatment\",\"trial\",\"tribe\",\"tribut\",\"triumph\",\"troop\",\"truck\",\"true\",\"truli\",\"trunk\",\"trust\",\"truth\",\"turbin\",\"turkei\",\"turn\",\"turnov\",\"twelv\",\"twenti\",\"two\",\"type\",\"u\",\"u.\",\"ultim\",\"unabl\",\"unanticip\",\"unavoid\",\"uncertain\",\"uncertainti\",\"unchang\",\"underproduct\",\"undersel\",\"understand\",\"understood\",\"undertak\",\"undoubt\",\"undoubtedli\",\"undu\",\"unemploy\",\"unequ\",\"unexampl\",\"unexcel\",\"unexpect\",\"unexpend\",\"unfamiliar\",\"unfortun\",\"unfriendli\",\"unfruit\",\"uniform\",\"uninterrupt\",\"union\",\"unit\",\"uniti\",\"univers\",\"unjust\",\"unknown\",\"unless\",\"unnatur\",\"unnecessari\",\"unnecessarili\",\"unobjection\",\"unpreced\",\"unprofit\",\"unrestrict\",\"unscour\",\"unstabl\",\"unsurpass\",\"unusu\",\"unvari\",\"unworthi\",\"upbuild\",\"upgrad\",\"upon\",\"urg\",\"urgent\",\"us\",\"user\",\"usual\",\"util\",\"utmost\",\"vain\",\"valorem\",\"valu\",\"valuabl\",\"vari\",\"varieti\",\"variou\",\"various\",\"vast\",\"vessel\",\"vest\",\"veto\",\"via\",\"vicissitud\",\"vienna\",\"view\",\"vigil\",\"vigor\",\"violat\",\"virginia\",\"visit\",\"visitor\",\"vital\",\"volum\",\"voluntarili\",\"wage\",\"wai\",\"wait\",\"walk\",\"want\",\"war\",\"ware\",\"warehous\",\"warfar\",\"warrant\",\"washington\",\"wast\",\"water\",\"wave\",\"wealth\",\"wealthi\",\"wear\",\"weav\",\"week\",\"weekli\",\"weight\",\"welcom\",\"welfar\",\"well\",\"west\",\"western\",\"wharv\",\"whatev\",\"wheat\",\"wheaton\",\"wheel\",\"whenev\",\"whether\",\"whilst\",\"whole\",\"wholesal\",\"wholli\",\"whose\",\"wide\",\"wider\",\"widespread\",\"will\",\"wind\",\"window\",\"wine\",\"winter\",\"wisdom\",\"wise\",\"wish\",\"wit\",\"withdraw\",\"withdrawn\",\"within\",\"without\",\"women\",\"wonder\",\"wood\",\"wooden\",\"wool\",\"woolen\",\"work\",\"worker\",\"workingman\",\"workingmen\",\"workmen\",\"workweek\",\"world\",\"world'\",\"worldwid\",\"worst\",\"worth\",\"worthi\",\"wrong\",\"wrought\",\"wrt\",\"wrung\",\"yard\",\"yarn\",\"year\",\"yearli\",\"yet\",\"yield\",\"york\",\"youngstown\",\"youth\",\"z898\",\"zero\",\"zollverein\"],\"freq\":[1,4,1,4,1,1,1,2,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,4,2,1,1,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,2,1,1,2,2,1,1,2,3,6,1,1,3,1,2,1,8,1,2,1,5,4,1,1,1,3,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,5,1,5,1,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,3,1,1,2,1,1,1,1,1,1,1,1,2,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,5,4,2,2,2,21,1,2,1,2,9,2,3,1,1,1,1,3,3,4,1,7,2,2,1,1,4,1,3,24,8,7,3,1,8,7,5,11,2,9,1,1,1,1,6,1,1,6,1,8,1,8,15,25,1,3,1,1,1,1,6,17,1,5,1,4,1,3,1,1,70,3,9,2,1,2,1,2,1,1,1,5,10,1,2,3,23,1,1,4,1,3,2,16,1,52,1,1,15,19,3,1,1,2,1,3,5,1,5,1,3,1,1,1,1,2,5,8,1,6,4,3,1,11,3,3,1,1,4,2,1,7,2,5,7,3,2,4,1,3,5,10,74,1,1,1,2,2,4,2,1,1,4,2,1,3,1,1,8,4,3,14,1,1,4,1,2,7,2,1,12,1,2,1,6,2,1,3,1,1,1,5,1,1,5,3,3,1,2,3,2,3,2,1,8,1,1,6,1,1,1,1,1,13,6,4,17,1,4,9,2,9,6,1,3,2,1,2,4,1,1,1,1,2,2,8,1,1,1,1,1,2,1,15,1,1,3,1,1,1,1,9,5,11,1,2,1,1,7,1,5,6,6,1,1,6,4,1,1,29,1,2,2,1,11,1,1,52,1,1,3,3,6,16,6,1,8,1,2,3,1,1,10,9,1,13,1,1,5,1,1,2,5,1,13,4,2,10,1,1,1,1,2,10,1,1,9,1,1,2,3,3,1,1,6,3,1,1,3,2,1,1,9,4,1,1,2,1,1,1,7,8,16,1,1,1,7,13,1,2,3,1,1,1,1,5,2,7,2,3,1,4,2,1,1,1,2,13,1,1,1,2,1,9,15,4,3,1,3,1,62,35,1,12,1,2,4,2,6,2,12,7,4,1,1,3,4,1,12,25,3,1,1,1,10,2,1,2,4,1,2,2,1,1,2,1,5,29,2,1,3,8,3,1,2,2,1,42,1,5,10,4,2,31,2,5,3,1,4,1,1,8,1,14,1,6,1,14,1,5,1,2,2,1,1,1,1,1,22,12,2,1,2,7,1,2,2,1,1,6,1,1,1,2,3,4,24,11,1,1,2,2,99,2,1,8,3,4,15,5,1,1,1,2,2,1,1,1,1,4,1,7,3,5,2,8,1,3,1,2,1,4,2,1,4,2,2,6,1,2,1,2,2,3,4,2,2,1,1,1,9,2,1,1,1,5,1,2,24,3,1,1,1,2,23,10,1,1,6,3,1,11,1,1,3,2,3,1,1,2,4,1,2,2,12,2,1,1,15,1,2,2,1,6,3,1,10,3,3,1,1,1,1,2,5,4,10,1,1,1,3,1,2,2,1,8,2,1,1,3,5,1,3,4,2,1,1,2,1,1,2,4,40,3,9,3,7,2,2,1,2,2,1,2,1,2,13,83,1,1,4,1,1,3,1,3,1,4,2,3,4,5,8,2,4,22,4,5,13,2,4,2,1,3,1,1,1,1,1,2,3,2,3,1,4,1,14,14,4,2,12,3,2,23,4,1,3,1,1,1,9,2,12,4,2,1,6,2,16,4,1,1,2,16,11,2,6,4,1,3,2,3,1,1,13,7,1,2,1,1,1,13,7,27,2,2,12,5,12,1,7,22,3,1,6,1,3,4,1,1,3,3,1,1,2,13,1,3,10,3,1,6,5,1,6,2,3,3,2,14,1,2,17,5,4,8,1,2,5,7,5,1,1,2,34,6,5,11,12,15,2,1,2,3,1,1,8,1,5,1,14,1,8,10,14,4,1,1,2,1,1,14,4,14,1,1,1,25,5,2,1,12,4,4,2,5,1,6,3,1,4,2,14,4,1,3,2,1,16,4,2,2,1,3,1,1,3,1,1,1,1,1,2,2,2,1,10,8,1,4,2,1,9,1,81,2,1,2,1,11,5,2,3,4,1,3,13,1,3,1,1,4,1,3,14,1,3,1,2,1,4,1,1,1,2,9,5,1,6,1,1,8,3,2,1,1,27,2,1,1,4,23,16,1,3,2,1,1,4,1,38,53,2,1,3,2,2,1,1,6,1,6,1,1,2,45,10,5,4,1,4,5,1,1,12,3,1,1,14,1,2,6,1,2,6,1,1,3,1,3,1,2,2,2,4,1,1,1,5,1,10,1,1,7,9,1,1,1,2,2,2,25,14,5,7,1,2,2,1,1,3,30,3,1,2,6,1,5,7,6,12,4,1,1,2,2,1,3,2,1,1,1,1,10,1,1,1,2,1,1,1,1,1,3,1,1,68,1,16,2,1,6,1,3,13,3,1,1,1,3,1,12,11,2,1,1,72,1,1,2,1,5,3,6,1,2,1,4,6,3,68,1,3,1,1,1,1,2,1,5,11,1,1,1,4,1,8,2,1,1,2,2,1,1,1,2,2,3,1,6,2,1,1,4,1,2,5,1,72,1,1,15,2,2,1,1,1,3,2,1,3,3,10,4,2,8,1,4,1,12,1,1,2,1,1,1,3,1,1,22,2,1,1,2,3,4,4,1,11,5,9,1,1,7,1,4,6,2,2,1,5,3,1,44,1,2,5,8,1,9,1,37,6,22,3,2,3,36,1,16,1,1,1,4,5,1,5,1,18,1,2,1,11,4,1,1,2,3,2,6,1,1,3,1,1,1,6,2,12,2,14,5,1,5,4,2,4,1,1,3,2,1,9,2,10,3,4,2,4,5,4,1,2,1,2,43,1,1,2,53,3,12,2,8,34,10,4,29,1,2,6,2,3,2,3,1,5,60,1,1,1,2,2,38,3,2,1,12,12,1,13,1,4,2,6,1,1,2,6,1,4,1,11,24,1,5,9,2,8,5,1,1,7,1,4,2,1,6,1,10,4,1,2,2,1,1,1,1,1,1,5,3,1,3,1,3,6,3,1,3,1,1,1,1,1,2,13,1,1,2,2,31,3,2,1,60,1,3,6,5,9,27,1,9,7,30,5,1,28,1,5,1,2,2,2,1,2,5,1,42,1,10,1,1,3,1,1,1,4,1,1,4,2,1,1,2,2,1,1,27,1,12,4,1,1,9,4,10,1,3,1,8,2,3,2,3,2,3,1,2,1,13,5,1,2,2,4,3,1,23,1,1,12,17,4,8,12,2,13,1,1,10,1,3,1,1,1,6,5,2,2,1,1,1,1,3,1,1,2,2,1,1,1,19,10,2,1,1,1,1,2,22,1,7,1,3,1,9,3,3,2,1,3,3,7,2,4,1,1,7,7,1,1,2,3,3,1,3,1,1,24,1,14,1,2,1,2,4,1,1,3,8,1,10,8,4,1,1,1,5,2,1,1,1,1,1,19,4,2,9,4,1,4,1,1,1,6,2,16,4,1,7,1,12,9,2,3,4,9,2,5,1,1,2,2,29,2,14,1,1,1,1,2,1,1,1,1,1,6,2,35,5,2,1,2,4,1,7,3,46,3,2,2,5,4,2,2,1,11,4,1,2,3,1,2,51,104,2,3,1,1,20,1,1,18,2,3,1,3,3,11,1,1,1,1,9,1,2,8,1,12,5,3,22,2,45,5,22,1,16,2,2,1,35,1,8,1,17,11,1,2,4,2,1,1,1,1,14,1,1,5,1,1,1,1,2,2,12,1,1,2,2,1,6,4,22,1,1,18,5,4,2,2,4,1,2,2,10,1,2,1,1,2,11,7,2,4,1,3,3,1,2,23,2,2,1,17,14,1,1,9,1,2,3,1,11,4,1,10,2,1,2,21,2,1,2,4,1,5,3,2,2,1,1,1,5,1,2,7,2,1,1,8,1,1,2,1,23,10,1,4,1,22,2,2,4,1,2,9,9,1,4,4,4,2,25,1,2,1,1,10,45,1,3,5,2,4,1,7,1,1,1,3,2,7,3,1,1,1,4,1,2,1,1,2,1,2,2,1,1,4,2,1,1,3,1,3,4,2,5,2,3,1,14,2,1,2,3,4,1,8,4,3,1,1,4,2,4,1,2,1,1,1,4,1,3,3,18,8,2,23,7,1,5,4,1,6,1,1,1,1,1,1,2,1,1,4,2,2,1,16,6,2,2,2,1,1,2,1,10,24,1,1,3,1,1,1,3,1,2,1,2,1,1,17,1,1,2,2,1,3,1,1,13,2,1,1,3,1,3,5,7,1,1,1,5,4,1,1,5,6,1,1,11,2,1,1,1,1,1,7,2,1,1,2,1,1,3,1,6,1,7,3,1,1,1,1,1,1,1,1,6,1,3,1,2,9,3,2,5,1,2,3,4,18,10,2,1,1,10,1,3,1,2,1,2,1,1,1,1,1,3,6,2,1,1,80,3,1,4,3,1,2,2,2,1,11,4,2,15,4,1,3,2,3,3,1,1,2,1,1,2,2,1,2,2,4,1,1,15,3,3,1,4,2,2,1,1,1,12,2,1,5,1,6,7,7,1,2,4,1,1,1,1,2,32,6,4,3,1,1,10,1,2,1,1,1,3,1,4,1,1,3,22,2,2,1,1,13,7,1,1,47,3,24,4,1,3,7,2,1,2,8,1,1,1,7,1,1,4,2,1,1,1,2,1,4,7,1,5,1,1,1,7,4,4,1,1,1,1,1,2,4,4,4,5,2,14,1,2,2,28,1,2,1,1,12,4,2,2,1,3,5,1,1,1,4,10,2,7,2,1,41,1,2,1,1,3,1,2,1,2,11,3,1,11,1,2,6,1,1,3,3,1,1,1,1,1,1,9,1,1,4,6,2,1,1,12,1,19,1,2,1,1,3,1,2,1,1,1,5,1,4,1,2,2,3,2,1,1,2,1,1,1,2,1,3,1,13,47,1,2,3,1,2,1,5,1,1,1,3,1,1,1,1,1,1,1,2,1,69,4,2,45,3,5,2,3,1,2,37,3,1,1,9,1,2,23,1,1,1,1,1,11,1,1,5,1,4,1,1,6,1,20,10,2,1,9,18,2,1,2,1,2,2,5,1,8,2,4,1,1,1,2,1,5,29,1,2,1,4,1,1,1,1,16,5,6,2,5,5,1,3,1,3,1,1,2,1,2,2,2,1,2,1,15,23,2,1,1,1,19,7,41,6,1,1,5,1,17,5,1,2,2,1,2,1,1,1,3,3,41,1,8,6,2,2,1,1,1,1],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":\"random-dark\",\"minSize\":0,\"weightFactor\":1.73076923076923,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.785398163397448,\"maxRotation\":0.785398163397448,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":{\"render\":[{\"code\":\"function(el,x){\\n                        console.log(123);\\n                        if(!iii){\\n                          window.location.reload();\\n                          iii = False;\\n\\n                        }\\n  }\",\"data\":null}]}}\r\nTo convert the tidy dataframe to a matrix, we can use the\r\ncast_dtm() function from tidytext.\r\n\r\n\r\ndtm_sotu <- tidy_sotu |> \r\n  count(speech_id, word_stem) |> \r\n  cast_dtm(document = 'speech_id',\r\n           term = 'word_stem',\r\n           value = 'n')\r\n\r\ndtm_sotu\r\n\r\n\r\n<<DocumentTermMatrix (documents: 141, terms: 2426)>>\r\nNon-/sparse entries: 8870/333196\r\nSparsity           : 97%\r\nMaximal term length: 14\r\nWeighting          : term frequency (tf)\r\n\r\nPractice Problems\r\nCreate a bag of words that co-occur in State of the Union\r\nsentences with the word “Mexico”, or some other word of your\r\nchoice.\r\nCreate a bag of words representation of the Federalist Papers\r\ncorpus, available in the corpus package\r\n(corpus::federalist).\r\n\r\n\r\n\r\nGrimmer, Justin, Brandon M. Stewart, and Margaret E. Roberts. 2021.\r\nText as Data: A New Framework for Machine Learning and the Social\r\nSciences. S.l.: Princeton University Press.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-06-15T07:41:44-04:00"
    },
    {
      "path": "clustering.html",
      "title": "Clustering",
      "description": "For when we don't really know what we're looking for in our data and just want the computer to tell us what it sees.\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nK-means Clustering\r\nValidation, Validation,\r\nValidation\r\nClustering with Word\r\nEmbeddings\r\nPractice Problems\r\nFurther Reading\r\n\r\nBroadly speaking, we can divide the approaches for modeling text data\r\ninto two camps: supervised learning and unsupervised learning. Supervised learning approaches tend\r\nto be the most familiar to social scientists – there is some outcome\r\nwe’d like to predict, so we fit a function of observable covariates to\r\ntry and predict it. In the context of text as data, this means we have a\r\nset of labeled documents, and we fit a model to see how well we can\r\npredict the labels (e.g. predicting the authorship of the Federalist Papers).\r\nUnsupervised learning, by comparison, is less about prediction and\r\nmore about discovery. You start with a set of\r\nunlabeled documents, and ask the computer to see if it can find\r\na sensible way to organize them. Are there patterns of language that\r\ndistinguish one set of documents from others? What words can help\r\nidentify a cluster of documents, by appearing within them more\r\nfrequently than one would expect by chance? These sorts of approaches,\r\nwhich include both clustering and topic models,\r\nrequire a healthy dose of human judgment to derive meaningful insights,\r\nand they often serve as the first stage of a research agenda that moves\r\nfrom discovery to explanation, prediction, and inference.\r\nK-means Clustering\r\nChapter 12 of Grimmer, Stewart, and Roberts (2021)\r\nintroduces the dataset of Congressional press releases that\r\n@grimmerRepresentationalStyleCongress2013 explores in his study of\r\nrepresentational style. Using a k-means clustering model, he developed a\r\nset of categories to describe these ways that members of Congress\r\ncommunicate with their constituents, discovering new categories that\r\nwere previously understudied by political scientists. The full dataset\r\nis available here,\r\nand I’ve included the press releases from Senator Lautenberg on the\r\ncourse repository. Let’s load and tidy the data, representing each press\r\nrelease as a bag of word stems.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(SnowballC)\r\n\r\nload('data/press-releases/lautenberg-press-releases.RData')\r\n\r\ntidy_press_releases <- df |>\r\n  # remove the preamble common to each press release\r\n  mutate(text = str_replace_all(text,\r\n                                pattern = '     Senator Frank R  Lautenberg                                                                                                                      Press Release        of        Senator Lautenberg                                                                                ',\r\n                                replacement = '')) |>\r\n  # tokenize to the word level\r\n  unnest_tokens(input = 'text',\r\n                output = 'word') |>\r\n  # remove stop words\r\n  anti_join(get_stopwords()) |>\r\n  # remove numerals\r\n  filter(str_detect(word, '[0-9]', negate = TRUE)) |>\r\n  # create word stems\r\n  mutate(word_stem = wordStem(word)) |>\r\n  filter(word_stem != '') |> \r\n  # count up bag of word stems\r\n  count(id, word_stem) |> \r\n  # compute term frequency\r\n  bind_tf_idf(term = 'word_stem',\r\n              document = 'id',\r\n              n = 'n') |>\r\n  filter(!is.na(tf_idf))\r\n\r\ntidy_press_releases\r\n\r\n\r\n# A tibble: 81,504 x 6\r\n      id word_stem     n      tf   idf  tf_idf\r\n   <int> <chr>     <int>   <dbl> <dbl>   <dbl>\r\n 1     1 account       2 0.00654 1.86  0.0121 \r\n 2     1 also          2 0.00654 0.891 0.00582\r\n 3     1 america       2 0.00654 1.80  0.0118 \r\n 4     1 american      1 0.00327 1.05  0.00344\r\n 5     1 answer        1 0.00327 3.69  0.0120 \r\n 6     1 apologi       1 0.00327 5.63  0.0184 \r\n 7     1 appropri      1 0.00327 1.60  0.00522\r\n 8     1 april         2 0.00654 2.18  0.0143 \r\n 9     1 ask           1 0.00327 2.13  0.00698\r\n10     1 assault       2 0.00654 3.93  0.0257 \r\n# ... with 81,494 more rows\r\n\r\nNext, we’ll convert that tidy dataframe into a document-term\r\nmatrix.\r\n\r\n\r\n# create document-term matrix\r\nlautenberg_dtm <- cast_dtm(data = tidy_press_releases,\r\n                           document = 'id',\r\n                           term = 'word_stem',\r\n                           value = 'tf')\r\nlautenberg_dtm\r\n\r\n\r\n<<DocumentTermMatrix (documents: 558, terms: 7073)>>\r\nNon-/sparse entries: 81504/3865230\r\nSparsity           : 98%\r\nMaximal term length: 33\r\nWeighting          : term frequency (tf)\r\n\r\nThe k-means clustering algorithm searches for a set of \\(k\\) centroids that yield the smallest sum\r\nof squared distances between the observations and their nearest\r\ncentroid. If each document is represented by a vector of term\r\nfrequencies, then k-means produces \\(k\\) sets of documents that have the most\r\nsimilar usages of words.1\r\n\r\n\r\nset.seed(42)\r\n\r\nkm <- kmeans(x = lautenberg_dtm,\r\n             centers = 4,\r\n             nstart = 100)\r\n\r\ntable(km$cluster)\r\n\r\n\r\n\r\n  1   2   3   4 \r\n158  53 264  83 \r\n\r\nMaking sense of this algorithm’s output is tricky. Sure, we\r\nsimplified the problem a bit. We started with 558 documents, each\r\nrepresented by a 7,073-dimensional vector. Now we have 4 document\r\nclusters, each represented by a 7,073-dimensional vector.\r\n\r\n\r\n\r\nSo…what do we do with those?\r\nOne of the most common ways to interpret the k-means clusters is to\r\ngenerate a list of the most distinctive words from each cluster. Then we\r\ncan look at which words show up more frequently in one cluster than any\r\nother, and use that information to assign labels to the clusters.\r\n\r\n\r\n# function to find the words that are most overrepresented in the cluster mean for a given cluster\r\nget_top_words <- function(centers, cluster_of_interest, n = 10){\r\n  (centers[cluster_of_interest,] - colMeans(centers[-cluster_of_interest,])) |>\r\n    sort(decreasing = TRUE) |>\r\n    head(n)\r\n}\r\n\r\n\r\n\r\n\r\n\r\nget_top_words(km$centers, 1)\r\n\r\n\r\n        new      jersei    menendez        fund     project \r\n0.020521488 0.017404097 0.007306990 0.006406194 0.004548713 \r\n    million       feder          nj     program         sen \r\n0.003967707 0.003579091 0.003422422 0.003390473 0.003174855 \r\n\r\nIt looks like that first cluster contains words related to New\r\nJersey-specific projects. Maybe we’ll call this category “credit\r\nclaiming”.\r\n\r\n\r\nget_top_words(km$centers, 2)\r\n\r\n\r\n      secur      chemic    homeland        port     protect \r\n0.047101748 0.017238562 0.012628968 0.009966087 0.005874105 \r\n       risk         law          dh        bill  lautenberg \r\n0.005328492 0.005129255 0.004088366 0.003876055 0.003604233 \r\n\r\nThe second cluster contains words related to security.\r\n\r\n\r\nget_top_words(km$centers, 3)\r\n\r\n\r\n     legisl         sen    american        bill         epa \r\n0.003556391 0.002868324 0.002606283 0.002589137 0.002394477 \r\n        act      famili      victim        year      amtrak \r\n0.002078147 0.001866299 0.001717110 0.001707263 0.001613547 \r\n\r\nThe third cluster has words related to various pieces of legislation\r\nand Senate business.\r\n\r\n\r\n# cluster 4 (credit claiming for New Jersey projects)\r\nget_top_words(km$centers, 4)\r\n\r\n\r\n     presid        bush   statement       senat     comment \r\n0.019325460 0.015638002 0.007088761 0.004435791 0.004353666 \r\n       unit       elect      follow        issu         tax \r\n0.004192093 0.004036246 0.003940064 0.003897682 0.003500055 \r\n\r\nAnd the final cluster looks like the “partisan taunting” category\r\ndicussed in the book.\r\nValidation, Validation,\r\nValidation\r\nTo validate our manually-assigned cluster labels, we want to go back\r\nto the text and check to see if they do a good job summarizing the\r\ndocuments. If not, we should modify our cluster labels or try a\r\ndifferent value for \\(k\\).\r\n\r\n\r\ncluster_assignments <- tibble(id = km$cluster |> \r\n                                names() |> \r\n                                as.numeric(),\r\n                              cluster = km$cluster)\r\n\r\ndf <- df |>\r\n  left_join(cluster_assignments,\r\n            by = 'id')\r\n\r\n\r\n\r\nIf we pull a random document from Cluster 1, it should be related to\r\nNew Jersey in some way.\r\n\r\n\r\nprint_text <- function(text){\r\n  cat(str_wrap(text), sep = '\\n')\r\n}\r\n\r\ndf |>\r\n  filter(cluster == 1) |>\r\n  slice_sample(n = 1) |>\r\n  pull(text) |> \r\n  print_text()\r\n\r\n\r\nSenator Frank R Lautenberg Press Release of Senator Lautenberg Lautenberg\r\nMenendez Announce 21 Million for Improvements in Screening Areas at Newark\r\nLiberty Airport Contact Alex Formuzis 202 224 7340 Thursday August 3 2006\r\nWASHINGTON D C Air travelers who depart from Newark Liberty International\r\nAirport will benefit from improvements in the security screening area thanks\r\nto almost 21 million in federal grants announced today by U S Senators Frank\r\nLautenberg D NJ and Robert Menendez D NJ The grants will be used to widen\r\nterminal connecting areas creating more space for passengers waiting to\r\npass through security checkpoints These improvements will make it even more\r\nconvenient to fly from Newark Liberty Airport said Senator Lautenberg By\r\nimproving the airport we protect our economy and our quality of life Newark\r\nLiberty International Airport is a huge hub of activity for New Jersey and\r\nthe nation It provides jobs for New Jerseyans acts as a means for American and\r\ninternational businessmen and women to work throughout the region and allows\r\nvisitors to come enjoy our great state said Senator Menendez Senator Lautenberg\r\nand I fought tirelessly for these funds to improve the safety and efficiency of\r\nthis thriving center of travel The funds were awarded by the Federal Aviation\r\nAdministration to the Port Authority of New York and New Jersey which operates\r\nthe airport Questions or Comments\r\n\r\nIf we pull a random document from Cluster 2, it should be about\r\nsecurity.\r\n\r\n\r\ndf |>\r\n  filter(cluster == 2) |>\r\n  slice_sample(n = 1) |>\r\n  pull(text) |> \r\n  print_text()\r\n\r\n\r\nSenator Frank R Lautenberg Press Release of Senator Lautenberg Lautenberg\r\nPallone Menendez Blast Proposal to Block New Jersey s Chemical Security\r\nRegulations Contact Alex Formuzis 202 224 7340 Thursday February 8 2007\r\nWASHINGTON D C U S Sens Frank R Lautenberg D NJ and Robert Menendez D NJ and U S\r\nRep Frank Pallone Jr D NJ today blasted the U S Department of Homeland Security\r\nDHS for proposing a federal rule that would preempt New Jersey s existing\r\nchemical security regulations The three lawmakers submitted extensive comments\r\nas part of the public comment period for a DHS proposed regulation to develop\r\ntemporary federal regulations to help secure chemical facilities A copy of that\r\nletter is attached The regulation was developed in response to a legislative\r\nprovision in the Fiscal Year 2007 Homeland Security Appropriations Act passed\r\nin 2006 Although that provision did not give the Department the right to preempt\r\nstate or local laws on the subject the Department s recent proposal assumed\r\nsuch authority As representatives of the citizens of New Jersey we simply cannot\r\naccept a proposed regulatory scheme that requires our constituents to rely\r\nupon the best efforts of private companies and this Administration to ensure\r\ntheir safety from terrorist attacks on chemical facilities in their communities\r\nthe three New Jersey lawmakers wrote In 2005 New Jersey implemented Chemical\r\nSecurity Sector Best Practices requiring all chemical facilities in the state to\r\ncomply with security standards conduct an assessment of their vulnerability to\r\nterrorist attacks develop prevention preparedness and response plans to minimize\r\nsuch attacks and review whether it would be practical to use safer materials\r\nor processes New Jersey took steps to improve its security after 9 11 and it\r\nmay need to take additional steps in the future Lautenberg Pallone and Menendez\r\ncontinued in their public comment letter We strongly oppose any efforts by\r\nDHS and the rest of this Administration to prevent it from doing so Lautenberg\r\nand Pallone have introduced comprehensive chemical security bills in previous\r\nCongresses and announced their intention to do so during this Congress Questions\r\nor Comments\r\n\r\nIf we pull a random document from Cluster 3, it be about legislation\r\nand/or Senate business.\r\n\r\n\r\ndf |>\r\n  filter(cluster == 3) |>\r\n  slice_sample(n = 1) |>\r\n  pull(text) |> \r\n  print_text()\r\n\r\n\r\nSenator Frank R Lautenberg Press Release of Senator Lautenberg Lautenberg\r\nSpecter Introduce Bill To Give Justice To Victims of State Sponsored Terrorism\r\nMeasure Would Empower Victims To Pursue Assets of Countries Like Iran That\r\nSponsor Terror Contact Press Office 202 224 3224 Thursday August 2 2007\r\nWASHINGTON D C Sen Frank R Lautenberg D NJ and Sen Arlen Specter R PA today\r\nled a strong bipartisan coalition of Senators introducing legislation to give\r\nvictims of state sponsored terrorism their day in court Far too many Americans\r\nhave suffered at the hands of terrorism My bill would allow victims of state\r\nsponsored terror to have their day in court It would let victims sue countries\r\nand hold those countries accountable said Sen Lautenberg I am pleased to\r\ncosponsor this legislation which gives the victims of terrorism and their\r\nfamilies the ability to seek legal redress said Sen Specter This bill reaffirms\r\nthat the United States will not tolerate state sponsored terrorism The bill\r\nwould allow victims of state sponsored terror to sue countries that promote\r\nterrorism The measure would allow victims to seize hidden commercial assets for\r\ncompensation This legislation is important to the families of the victims of the\r\n1983 Marine Barracks bombing in Beirut Lebanon It will hold the government of\r\nIran accountable for the murder of 241 men in this bombing one of whom was my\r\nbrother Captain Vincent L Smith United States Marine Corps The injustice of this\r\nover the long years has been a heavy burden the Iranian government has literally\r\nbeen getting away with murder for almost 24 years The passage of this bill will\r\nbring justice by holding the criminals accountable for their crime And I believe\r\nit will mitigate future terrorism This bill is a huge statement of support for\r\nvictims of terrorism and a powerful way to fight terrorism without the use of\r\nmilitary force said Lynn Derbyshire who serves as the national spokesperson for\r\nThe Beirut Families The legislation the Justice for Victims of State Sponsored\r\nTerrorism Act is based on a 1996 amendment to the Foreign Sovereign Immunities\r\nAct known as the Flatow Amendment which enabled American victims of terrorism to\r\ngo after state sponsors of terrorism in court The billwould reaffirm the rights\r\nof plaintiffs to sue state sponsors of terrorism allow the seizure of hidden\r\ncommercial assets belonging to terrorist states so victims of terrorism can be\r\njustly compensated limit the number of appeals that a terrorist state can pursue\r\nin U S courts and provide foreign nationals working for the U S government these\r\nsame benefits if they are victimized in a terrorist attack during their official\r\nduties The measure has an impressive bipartisan list of original cosponsors\r\nincluding Senators Robert Menendez D NJ Trent Lott R MS Joseph Biden D DE John\r\nCornyn R TX Hillary Clinton D NY Lindsey Graham R SC Diane Feinstein D CA Joseph\r\nLieberman I CT Charles Schumer D NY Norm Coleman R MN Robert Casey D PA Susan\r\nCollins R ME and Ted Stevens R AK Questions or Comments\r\n\r\nAnd a random document from Cluster 4 should contain some form of\r\n“partisan taunting”.\r\n\r\n\r\ndf |>\r\n  filter(cluster == 4) |>\r\n  slice_sample(n = 1) |>\r\n  pull(text) |> \r\n  print_text()\r\n\r\n\r\nSenator Frank R Lautenberg Press Release of Senator Lautenberg Lautenberg\r\nOutraged Over Bush Admin Decision to Let Libya Off the Hook for Pan Am 103\r\nTerrorist Bombing Contact Alex Formuzis 202 224 7340 Wednesday June 28 2006\r\nWASHINGTON D C United States Senator Frank R Lautenberg D NJ who has led the\r\nfight on behalf of the families of victims of Pan Am 103 today issued the\r\nfollowing statement in response to the Bush Administration s decision to let\r\nthe Libyan government off the hook Today the Bush Administration put other\r\ninterests ahead of American victims of terrorism I am very disappointed that\r\nthe Administration chose to renew its relationship with Qadhafi before making\r\nsure he fulfilled his promises to American victims of his terror said Senator\r\nLautenberg Under the original agreement between the Libyan government and the\r\nfamilies of the victims of Pan Am 103 each family was to receive 10 million from\r\nthe Libyan government to be paid out in three installments 4 million when the U\r\nN lifted its sanctions 4 million when the U S lifted its trade sanctions and the\r\nfinal 2 million when Libya was taken off the U S terrorist list which officially\r\nhappens today On May 15 Secretary of State Condoleezza Rice announced that the\r\nadministration would renew diplomatic relations with Libya at which point a\r\n45 day review began That review period ends today and Libya will be formally\r\nremoved from the U S State Department s list of state sponsors of terrorism\r\nThe Libyan Government and the Bush administration appear to have agreed that\r\nthe final payment does not need to be paid to the families Earlier this month\r\nthe Senate approved a Resolution by Senators Lautenberg and Lindsey Graham R SC\r\nurging the Bush administration not to establish diplomatic relations with Libya\r\nuntil it fulfills its responsibilities to the families of the Pan Am 103 victims\r\nIn August 2003 the Libyan government took responsibility for the bombing of\r\nPan Am flight 103 over Lockerbie Scotland on December 21st 1988 that killed 270\r\npeople Of the 189 Americans who died as a result of the bombing 38 were from New\r\nJersey Questions or Comments\r\n\r\nClustering with Word\r\nEmbeddings\r\nIn that last clustering exercise, we represented each document as a\r\nvector of word counts. This can lead us astray when the documents are\r\nvery short but our vocabulary is very large. For example, Senator\r\nLautenberg has a number of press releases that I would classify as being\r\nabout the environment, but the Bag of\r\nWords has no idea that words like “preservation”, “mercury”, and\r\n“rivers” might belong in the same category.\r\nWhen documents are as brief as these press releases, one could\r\npotentially represent them with the average word embedding of the words in each\r\ndocument. The k-means clustering algorithm then works in the same way,\r\nexcept that it is assigning clusters within a vector space corresponding\r\nto the meaning of the documents (in a sense) rather than a\r\nvector space that’s just counting up the frequency of words.\r\nLet’s give it a try, shall we? First, get the word embeddings from\r\nthe textdata package.\r\n\r\n\r\nlibrary(textdata)\r\n\r\n# get the glove embedding vectors\r\nglove <- embedding_glove6b(dimensions = 100)\r\n\r\n# convert to a matrix (it will make the computation easier)\r\nglove_tokens <- glove$token\r\n\r\nglove <- glove |>\r\n  select(-token) |>\r\n  as.matrix()\r\n\r\nrownames(glove) <- glove_tokens\r\n\r\nglove[100:120,1:3]\r\n\r\n\r\n               d1        d2        d3\r\nu.s.     0.323960  0.598100  1.237800\r\nso      -0.395510  0.546600  0.503150\r\nthem    -0.101310  0.109410  0.240650\r\nwhat    -0.151800  0.384090  0.893400\r\nhim      0.042409 -0.521950  0.403890\r\nunited   0.217330  0.561160  0.630620\r\nduring  -0.278910 -0.229740 -0.474540\r\nbefore   0.362810 -0.185590  0.461190\r\nmay      0.082528 -0.075290  0.014696\r\nsince    0.422610  0.309450  0.218540\r\nmany    -0.329140  0.828870 -0.141820\r\nwhile    0.094157  0.464570  0.453500\r\nwhere    0.051044  0.598240  0.311950\r\nstates   0.138150  0.451660  0.938580\r\nbecause  0.067634  0.415950  0.584510\r\nnow     -0.014495  0.591070  0.704690\r\ncity     0.265720  0.034857  0.490550\r\nmade    -0.198200 -0.284050  0.145840\r\nlike    -0.268700  0.817080  0.698960\r\nbetween  0.082441 -0.040760  0.525170\r\ndid      0.304490 -0.196280  0.202250\r\n\r\nNext, we’ll tokenize the press releases, removing stop words and\r\nkeeping only the words available in the GloVe lexicon.\r\n\r\n\r\n# get all the press releases and count up the words for each\r\ntidy_press_releases <- df |>\r\n  # get rid of the earlier cluster assignments\r\n  select(-cluster, -date) |>\r\n  mutate(text = str_replace_all(text,\r\n                                pattern = '     Senator Frank R  Lautenberg                                                                                                                      Press Release        of        Senator Lautenberg                                                                                ',\r\n                                replacement = '')) |>\r\n  # tokenize to the word level\r\n  unnest_tokens(input = 'text',\r\n                output = 'word') |>\r\n  # remove stop words\r\n  anti_join(get_stopwords()) |>\r\n  # remove numerals\r\n  filter(str_detect(word, '[0-9]', negate = TRUE)) |>\r\n  # remove the words that aren't in the glove lexicon\r\n  filter(word %in% glove_tokens)\r\n\r\n\r\n\r\nTODO\r\nPractice Problems\r\nTry changing the value of \\(k\\) and\r\nsee if the cluster assignments seem to improve. Look for cluster labels\r\nthat are exclusive (topics aren’t overlapping; words that are\r\nsupposed to distinguish one cluster don’t frequently appear in other\r\nclusters) and cohesive (it’s easy to identify a unique topic\r\nfor each cluster).\r\nFurther Reading\r\nTODO\r\n\r\n\r\n\r\nGrimmer, Justin, Brandon M. Stewart, and Margaret E. Roberts. 2021.\r\nText as Data: A New Framework for Machine Learning and the Social\r\nSciences. S.l.: Princeton University Press.\r\n\r\n\r\nAllison Horst has a delightful\r\nillustrated explanation of how the algorithm works on this\r\npage.↩︎\r\n",
      "last_modified": "2022-06-15T07:42:50-04:00"
    },
    {
      "path": "federalist-papers.html",
      "title": "Federalist Paper Authorship",
      "description": "Modeling the bag of words.\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nMultinomial Model\r\nVector Space Model\r\nValidation, Validation,\r\nValidation\r\nPractice Problems\r\n\r\nA great entry point for modeling text as data is the now classic\r\nproblem of guessing who wrote 15 disputed Federalist Papers (Mosteller and Wallace 1964). To do so,\r\nwe’ll look at the distribution of words that Hamilton, Madison, and Jay\r\ntend to write and try to detect their stylistic patterns in the disputed\r\ntexts. We’ll use two approaches, outlined in Grimmer, Stewart, and Roberts (2021)\r\nChapters 6 and 7: the multinomial model and the vector space model.\r\nMultinomial Model\r\nTo motivate this model, suppose that each author owns a literal bag\r\nof words. When they are writing, they randomly draw each word from the\r\nbag, replacing it when they’re done writing it down. Evidently,\r\nAlexander Hamilton’s word bag has a lot of the word “upon”.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(wordcloud2)\r\n\r\ntidy_federalist <- corpus::federalist |> \r\n  # tokenize to the word level\r\n  unnest_tokens(input = 'text',\r\n                output = 'word')\r\n\r\n# keep only a selection of stop words\r\ninteresting_words <- c('although', 'always', \r\n                     'commonly', 'consequently',\r\n                     'considerable', 'heretofore', \r\n                     'upon', 'whilst')\r\n\r\ntidy_federalist <-filter(tidy_federalist,\r\n                         word %in% interesting_words)\r\n\r\ntidy_federalist |> \r\n  filter(author == 'Hamilton') |> \r\n  count(word) |> \r\n  wordcloud2()\r\n\r\n\r\n\r\n{\"x\":{\"word\":[\"although\",\"always\",\"commonly\",\"consequently\",\"considerable\",\"heretofore\",\"upon\",\"whilst\"],\"freq\":[1,62,23,4,46,13,374,1],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":\"random-dark\",\"minSize\":0,\"weightFactor\":0.481283422459893,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.785398163397448,\"maxRotation\":0.785398163397448,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":{\"render\":[{\"code\":\"function(el,x){\\n                        console.log(123);\\n                        if(!iii){\\n                          window.location.reload();\\n                          iii = False;\\n\\n                        }\\n  }\",\"data\":null}]}}\r\nBy comparison, James Madison’s bag of words has relatively more\r\n“whilst”.\r\n\r\n\r\ntidy_federalist |> \r\n  filter(author == 'Madison') |> \r\n  count(word)\r\n\r\n\r\n  word          n\r\n1 although      7\r\n2 always        7\r\n3 consequently 12\r\n4 considerable  5\r\n5 heretofore    1\r\n6 upon          7\r\n7 whilst       12\r\n\r\nObviously this model leaves out a lot of detail about how authors\r\nactually write. It ignores word order, syntax, meaning, context, and\r\nintent. But for certain questions, such a simplified model may\r\nnevertheless prove useful. Let’s consider Federalist Paper No. 18,\r\ncounting the frequency of “upon”, “whilst”, and the other interesting\r\nstop words listed above.1\r\n\r\n\r\ntidy_federalist |> \r\n  filter(name == 'Federalist No. 18') |> \r\n  count(word)\r\n\r\n\r\n  word         n\r\n1 always       1\r\n2 considerable 1\r\n3 upon         1\r\n4 whilst       1\r\n\r\nWhat’s the likelihood that this set of word counts would have been\r\ngenerated by random draws from Hamilton’s bag of words, compared to\r\nMadison’s or Jay’s? (What’s the chance that Hamilton would have written\r\na paper with so few “upon”s?) To estimate, we’ll first compute the word\r\nvectors for each author and for the disputed paper.\r\n\r\n\r\n# get the frequencies of the interesting words in each author's corpus\r\nbags_of_words <- tidy_federalist |>\r\n  filter(author %in% c('Hamilton', 'Madison', 'Jay')) |>\r\n  # convert these to factors so count() doesn't drop the zero counts\r\n  mutate(author = factor(author),\r\n         word = factor(word)) |> \r\n  count(author, word, .drop = FALSE) |>\r\n  # sort the words in alphabetical order\r\n  arrange(author, word)\r\n\r\n# pull the vectors for Hamilton, Madison, and Jay\r\nhamilton_vector <- bags_of_words |>\r\n  filter(author == 'Hamilton') |>\r\n  pull(n) |>\r\n  # add names to make the vector more readable\r\n  set_names(interesting_words)\r\n\r\nhamilton_vector\r\n\r\n\r\n    although       always     commonly consequently considerable \r\n           1           62           23            4           46 \r\n  heretofore         upon       whilst \r\n          13          374            1 \r\n\r\nmadison_vector <- bags_of_words |>\r\n  filter(author == 'Madison') |>\r\n  pull(n) |>\r\n  # add names to make the vector more readable\r\n  set_names(interesting_words)\r\n\r\nmadison_vector\r\n\r\n\r\n    although       always     commonly consequently considerable \r\n           7            7            0           12            5 \r\n  heretofore         upon       whilst \r\n           1            7           12 \r\n\r\njay_vector <- bags_of_words |>\r\n  filter(author == 'Jay') |>\r\n  pull(n) |>\r\n  # add names to make the vector more readable\r\n  set_names(interesting_words)\r\n\r\njay_vector\r\n\r\n\r\n    although       always     commonly consequently considerable \r\n           5            8            1            4            1 \r\n  heretofore         upon       whilst \r\n           1            1            0 \r\n\r\n# now get the vector for Federalist No. 18\r\nfed18_vector <- tidy_federalist |>\r\n  mutate(word = factor(word)) |> \r\n  filter(name == 'Federalist No. 18') |>\r\n  count(word, .drop = FALSE) |>\r\n  pull(n) |>\r\n  # add names to make the vector more readable\r\n  set_names(interesting_words)\r\n\r\nfed18_vector\r\n\r\n\r\n    although       always     commonly consequently considerable \r\n           0            1            0            0            1 \r\n  heretofore         upon       whilst \r\n           0            1            1 \r\n\r\nNext we’ll use the dmultinom() function to estimate the\r\nlikelihood that fed18_vector would have been drawn from\r\neach of the authors’ bags of words.\r\n\r\n\r\ndmultinom(x = fed18_vector,\r\n          prob = hamilton_vector)\r\n\r\n\r\n[1] 0.0003395527\r\n\r\ndmultinom(x = fed18_vector,\r\n          prob = madison_vector)\r\n\r\n\r\n[1] 0.01042985\r\n\r\ndmultinom(x = fed18_vector,\r\n          prob = jay_vector)\r\n\r\n\r\n[1] 0\r\n\r\nThe computation above makes a very strong assumption about Jay:\r\nbecause he never uses the word “whilst” in any of his Federalist papers,\r\nwe assume that he would never ever use the word whilst\r\nin another paper. We can do better by regularizing our estimates, adding\r\na small positive number to each vector (Laplace smoothing) to encode the\r\npossibility that Jay might someday use the word “whilst”, even if we’ve\r\nnever seen him do it.\r\n\r\n\r\nhamilton_likelihood <- dmultinom(x = fed18_vector,\r\n                                 prob = hamilton_vector + 0.1)\r\n\r\nmadison_likelihood <- dmultinom(x = fed18_vector,\r\n                                prob = madison_vector + 0.1)\r\n\r\njay_likelihood <- dmultinom(x = fed18_vector,\r\n                            prob = jay_vector + 0.1)\r\n\r\n# likelihood ratios\r\nmadison_likelihood / hamilton_likelihood\r\n\r\n\r\n[1] 27.8199\r\n\r\nmadison_likelihood / jay_likelihood\r\n\r\n\r\n[1] 99.56548\r\n\r\nSince this paper is roughly 28 times more likely to have been\r\ngenerated from Madison’s bag of words over Hamilton’s (and roughly 100\r\ntimes more likely than Jay’s), we can conclude with some degree of\r\nconfidence that he was the author.\r\nVector Space Model\r\nAnother way to model the bag of words is to think of each set of word\r\ncounts as a multidimensional vector. The angle between two such vectors\r\ngives us a sense of the two documents’ similarity to one another. If the\r\nangle is zero, then both documents have the exact same mix of words\r\n(though maybe one document is longer than the other). If the angle is 90\r\ndegrees, then the two documents are orthogonal – as different a\r\nmix of words as they possibly could be.\r\nCosine similarity captures this idea, because the cosine of 90\r\ndegrees is zero, and the cosine of 0 degrees is 1. Let’s compute the\r\ncosine similarity between Madison, Hamilton, and Jay’s known writings\r\nwith the term vector from the disputed Federalist No. 18.\r\n\r\n\r\n# define cosine similarity\r\ncosine_similarity <- function(x1, x2){\r\n  sum(x1*x2) / sqrt(sum(x1^2)) / sqrt(sum(x2^2))\r\n}\r\n\r\n# get the cosine similarity for Federalist 18 with all the authors\r\ncosine_similarity(fed18_vector, hamilton_vector)\r\n\r\n\r\n[1] 0.630843\r\n\r\ncosine_similarity(fed18_vector, madison_vector)\r\n\r\n\r\n[1] 0.721907\r\n\r\ncosine_similarity(fed18_vector, jay_vector)\r\n\r\n\r\n[1] 0.4789131\r\n\r\nThis yields a similar result to what we found with the multinomial\r\nmodel. The cosine similarity between the disputed paper and Madison’s\r\nother papers is largest, suggesting he is the author.\r\nValidation, Validation,\r\nValidation\r\nWhenever you develop a method to measure or predict some quantity of\r\ninterest, it is imperative that you first assess its performance against\r\na known benchmark before applying it to new data. This process is called\r\nvalidation, and Grimmer, Stewart, and Roberts (2021)\r\nrepeatedly emphasize how central it is to the text-as-data workflow. So\r\nthe procedure we just created predicts that Madison wrote Federalist\r\nNo. 18. How much should we trust that prediction?\r\nTo gain confidence in our approach, let’s see what it predicts for\r\npapers where authorship is not in dispute. If it correctly\r\npredicts the authors of the known texts, then we can be more certain\r\nit’s doing a good job with the unknown texts.\r\nFor the validation test, we’ll repeat the same steps as before,\r\nexcept we hold out the validation set when training the model. If we’re\r\ntrying to predict whether Madison wrote Federalist 10, then it would be\r\ncheating to include Federalist 10 in the vector of things we know about\r\nMadison’s writing style.\r\n\r\n\r\n# get the Federalist 10 word count vector\r\nfed10_vector <- tidy_federalist |>\r\n  mutate(word = factor(word)) |> \r\n  filter(name == 'Federalist No. 10') |>\r\n  count(word, .drop = FALSE) |>\r\n  pull(n) |>\r\n  # add names to make the vector more readable\r\n  set_names(interesting_words)\r\n\r\nfed10_vector\r\n\r\n\r\n    although       always     commonly consequently considerable \r\n           0            2            0            1            0 \r\n  heretofore         upon       whilst \r\n           0            0            0 \r\n\r\n# Recompute Madison's word count vector, omitting Federalist 10\r\nmadison_vector <- tidy_federalist |> \r\n  mutate(word = factor(word)) |> \r\n  filter(name != 'Federalist No. 10',\r\n         author == 'Madison') |> \r\n  count(word, .drop = FALSE) |> \r\n  pull(n) |> \r\n  set_names(interesting_words)\r\n\r\nmadison_vector\r\n\r\n\r\n    although       always     commonly consequently considerable \r\n           7            5            0           11            5 \r\n  heretofore         upon       whilst \r\n           1            7           12 \r\n\r\ndmultinom(x = fed10_vector,\r\n          prob = hamilton_vector)\r\n\r\n\r\n[1] 0.0003206053\r\n\r\ndmultinom(x = fed10_vector,\r\n          prob = madison_vector)\r\n\r\n\r\n[1] 0.007459852\r\n\r\ndmultinom(x = fed10_vector,\r\n          prob = jay_vector)\r\n\r\n\r\n[1] 0.08292841\r\n\r\nAlready we can see a problem. The words we chose before may do a good\r\njob distinguishing between Hamilton and Madison (it’s\r\ninconceivable that Hamilton would have written an entire paper\r\nwithout a single use of the word “upon”, so the model correctly rules\r\nhim out), but it struggles with distinguishing Jay’s style from\r\nMadison’s style, especially for a paper that only contains 3 of the\r\nchosen stop words. Because the word “always” appears the most\r\nfrequently, the model incorrectly predicts that Jay wrote Federalist\r\nNo. 10, not Madison.\r\nLet’s see if we can find a set of discriminating words that\r\ndo a better job distinguishing between all three authors.2 To\r\ndo so, I’ll start with the list of stop words from Mosteller and\r\nWallace (1964) and add in the list of stop words\r\nfrom the tidytext package.\r\n\r\n\r\nmw1964_words <- c(\"a\", \"all\", \"also\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"been\", \"but\", \"by\", \"can\", \"do\", \"down\",\r\n                  \"even\", \"every\", \"for\", \"from\", \"had\", \"has\", \"have\", \"her\", \"his\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\",\r\n                  \"may\", \"more\", \"must\", \"my\", \"no\", \"not\", \"now\", \"of\", \"on\", \"one\", \"only\", \"or\", \"our\", \"shall\", \"should\",\r\n                  \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"then\", \"there\", \"things\", \"this\", \"to\", \"up\", \"upon\",\r\n                  \"was\", \"were\", \"what\", \"when\", \"which\", \"who\", \"will\", \"with\", \"would\", \"your\")\r\n\r\nall_stopwords <- union(mw1964_words, get_stopwords()$word)\r\n\r\n\r\n\r\nThis yields a list of 186 potential words. Next, we’ll count up how\r\nfrequently each author used each word, keeping only the words that one\r\nauthor used three times more frequently than another auhor (the number\r\nthree is arbitrary, but as we’ll see it yields a manageable list of\r\ndistinctive words).\r\n\r\n\r\ntidy_federalist <- corpus::federalist |>\r\n  # tokenize to the word level\r\n  unnest_tokens(input = 'text',\r\n                output = 'word') |>\r\n  filter(word %in% all_stopwords) |>\r\n  mutate(author = factor(author),\r\n         word = factor(word))\r\n\r\nfrequency_table <- tidy_federalist |>\r\n  filter(author %in% c('Hamilton', 'Jay', 'Madison')) |>\r\n  count(author, word, .drop = FALSE) |>\r\n  pivot_wider(names_from = 'author',\r\n              values_from = 'n') |>\r\n  # normalize by each author's number of words\r\n  mutate(Hamilton = Hamilton / sum(Hamilton),\r\n         Jay = Jay / sum(Jay),\r\n         Madison = Madison / sum(Madison)) |>\r\n  mutate(Hamilton_Jay = Hamilton / Jay,\r\n         Hamilton_Madison = Hamilton / Madison,\r\n         Jay_Madison = Jay / Madison) |>\r\n  # just keep the words that one author uses 3 times more often than another\r\n  filter(Hamilton_Jay > 3 |\r\n           Hamilton_Jay < 0.33333 |\r\n           Hamilton_Madison > 3 |\r\n           Hamilton_Madison < 0.33333 |\r\n           Jay_Madison > 3 |\r\n           Jay_Madison < 0.33333)\r\n\r\n\r\n\r\nThis leaves us with 38 words to include in our multinomial model /\r\ndocument vectors.\r\n\r\n\r\ninteresting_words <- factor(frequency_table$word)\r\n\r\ninteresting_words\r\n\r\n\r\n [1] about      above      again      also       before     below     \r\n [7] did        down       during     every      further    here      \r\n[13] hers       herself    himself    his        how        itself    \r\n[19] me         my         myself     off        ought      our       \r\n[25] ours       ourselves  over       she        theirs     there     \r\n[31] through    up         upon       when       where      while     \r\n[37] whom       yourselves\r\n38 Levels: about above again also before below did down ... yourselves\r\n\r\nLet’s see how the modified list performs on the validation test.\r\n\r\n\r\n# keep only the new list of words\r\ntidy_federalist <-\r\n  tidy_federalist |>\r\n  filter(word %in% interesting_words) |> \r\n  mutate(word = factor(word))\r\n\r\n# compute the new Federalist 10 vector\r\nfed10_vector <- tidy_federalist |>\r\n  filter(name == 'Federalist No. 10') |>\r\n  count(word, .drop = FALSE) |>\r\n  pull(n) |>\r\n  # add names to make the vector more readable\r\n  set_names(interesting_words)\r\n\r\nfed10_vector\r\n\r\n\r\n     about      above      again       also     before      below \r\n         0          0          2          0          0          0 \r\n       did       down     during      every    further       here \r\n         0          0          0          3          0          1 \r\n      hers    herself    himself        his        how     itself \r\n         0          0          1          8          0          2 \r\n        me         my     myself        off      ought        our \r\n         1          0          0          0          2          8 \r\n      ours  ourselves       over        she     theirs      there \r\n         0          0          6          0          0          6 \r\n   through         up       upon       when      where      while \r\n         2          1          0          2          2          0 \r\n      whom yourselves \r\n         0          0 \r\n\r\n# get word count vectors for each author\r\nmadison_vector <- tidy_federalist |> \r\n  filter(name != 'Federalist No. 10',\r\n         author == 'Madison') |> \r\n  count(word, .drop = FALSE) |> \r\n  pull(n) |> \r\n  set_names(interesting_words)\r\n\r\nmadison_vector\r\n\r\n\r\n     about      above      again       also     before      below \r\n         1          2         11         30         13          2 \r\n       did       down     during      every    further       here \r\n         3          3         11         83         16         22 \r\n      hers    herself    himself        his        how     itself \r\n         1          2          6         48         13         26 \r\n        me         my     myself        off      ought        our \r\n         3          3          4          1         57         41 \r\n      ours  ourselves       over        she     theirs      there \r\n         0          3         31          3          2         27 \r\n   through         up       upon       when      where      while \r\n         6          2          7         18         32          0 \r\n      whom yourselves \r\n         9          0 \r\n\r\nhamilton_vector <- tidy_federalist |> \r\n  filter(author == 'Hamilton') |> \r\n  count(word, .drop = FALSE) |> \r\n  pull(n) |> \r\n  set_names(interesting_words)\r\n\r\nhamilton_vector\r\n\r\n\r\n     about      above      again       also     before      below \r\n        20          5          5         35         31          2 \r\n       did       down     during      every    further       here \r\n        12         14         17        179         29         33 \r\n      hers    herself    himself        his        how     itself \r\n         0          5         34        239         53         81 \r\n        me         my     myself        off      ought        our \r\n        15         39         15          3        159        191 \r\n      ours  ourselves       over        she     theirs      there \r\n         7         21         65         16          1        379 \r\n   through         up       upon       when      where      while \r\n        27         34        374        110         86         36 \r\n      whom yourselves \r\n        34          0 \r\n\r\njay_vector <- tidy_federalist |> \r\n  filter(author == 'Jay') |> \r\n  count(word, .drop = FALSE) |> \r\n  pull(n) |> \r\n  set_names(interesting_words)\r\n\r\njay_vector\r\n\r\n\r\n     about      above      again       also     before      below \r\n         0          2          1         11          0          0 \r\n       did       down     during      every    further       here \r\n         6          0          0          5          0          1 \r\n      hers    herself    himself        his        how     itself \r\n         0          0          0          5          9          1 \r\n        me         my     myself        off      ought        our \r\n         3          3          0          1          2         38 \r\n      ours  ourselves       over        she     theirs      there \r\n         0          3          2          4          1         10 \r\n   through         up       upon       when      where      while \r\n         0          0          1         15          2          2 \r\n      whom yourselves \r\n        10          1 \r\n\r\ndmultinom(x = fed10_vector,\r\n          prob = madison_vector + 0.1) /\r\ndmultinom(x = fed10_vector,\r\n          prob = hamilton_vector + 0.1) \r\n\r\n\r\n[1] 72.57105\r\n\r\ndmultinom(x = fed10_vector,\r\n          prob = madison_vector + 0.1) /\r\ndmultinom(x = fed10_vector,\r\n          prob = jay_vector + 0.1)\r\n\r\n\r\n[1] 1.140731e+12\r\n\r\ncosine_similarity(madison_vector, fed10_vector)\r\n\r\n\r\n[1] 0.747645\r\n\r\ncosine_similarity(hamilton_vector, fed10_vector)\r\n\r\n\r\n[1] 0.7205429\r\n\r\ncosine_similarity(jay_vector, fed10_vector)\r\n\r\n\r\n[1] 0.6565746\r\n\r\nThis is much nicer. Both methods now correctly identify Madison as\r\nthe author of Federalist 10. A more thorough model-development loop\r\nmight iterate through validation and refinement a few more times,\r\nholding out a different validation set each time. But this is the basic\r\nworkflow.\r\nPractice Problems\r\nConduct more validation tests on undisputed Federalist Papers to see\r\nif the list of words we came up with in the last section does a good\r\njob.\r\nSee if you can create a model that accurately classifies whether a\r\nState of the Union speech was delivered\r\nin the 19th or 20th century.\r\n\r\n\r\n\r\nGrimmer, Justin, Brandon M. Stewart, and Margaret E. Roberts. 2021.\r\nText as Data: A New Framework for Machine Learning and the Social\r\nSciences. S.l.: Princeton University Press.\r\n\r\n\r\nImai, Kosuke. 2017. Quantitative Social Science: An\r\nIntroduction. Princeton: Princeton University Press.\r\n\r\n\r\nMosteller, Frederick, and David L. Wallace. 1964. Inference and\r\nDisputed Authorship: The Federalist. Addison-Wesley.\r\n\r\n\r\nThis list was inspired by Chapter 5\r\nof Imai (2017). More on how to identify\r\ndiscriminating words in a later section.↩︎\r\nSee Grimmer, Stewart, and Roberts (2021)\r\nChapter 11 for more on this process.↩︎\r\n",
      "last_modified": "2022-06-15T07:42:56-04:00"
    },
    {
      "path": "index.html",
      "title": "Text As Data",
      "description": "A course on analyzing political texts using the `R` programming language\n",
      "author": [
        {
          "name": "Joe Ornstein",
          "url": "https://joeornstein.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nOverview\r\n\r\nOverview\r\nThis site is intended to serve as a companion to Grimmer, Stewart,\r\nand Roberts (2021), an excellent book on how to think\r\nabout text as data, which makes a deliberate choice to omit code when\r\ndescribing their examples. (Wisely, in my view, as books with code can\r\nquickly become dated.) Thus the need for this R code\r\nsupplement, which was developed during my Summer 2022 graduate-level\r\nText As Data course at the University of Georgia. All the code and data\r\nnecessary to replicate the results on this site are available at the\r\nGitHub link on the upper right.\r\nThe site is divided into three sections, corresponding to the three\r\nstages of any text-as-data workflow:\r\nHarvest the Text: How to carefully choose what\r\ntexts to include in your corpus, and how to get them from some messy\r\nformat like HTML or PDF into a plaintext dataframe.\r\nTidy the Text: How to represent large amounts of\r\ntext quantitatively, and what choices you need to make during the\r\npreprocessing stage.\r\nModel the Text: How to build a model to meet your\r\nobjective, be it prediction, classification, causal inference, or\r\nexploration.\r\nFor each stage in the workflow, there are a number of useful\r\nR packages that can help accomplish these tasks, including\r\nwebscraping (rvest), optical character recognition\r\n(tesseract), tidying (tidytext), topic\r\nmodeling (topicmodels), sentiment analysis\r\n(sentimentR), and many others. On this site, we will walk\r\nthrough several tutorials of these packages – motivated by political\r\nscience applications – with links to more detailed documentation for\r\nthose interested in exploring further.\r\n\r\n\r\n\r\nGrimmer, Justin, Brandon M. Stewart, and Margaret E. Roberts. 2021.\r\nText as Data: A New Framework for Machine Learning and the Social\r\nSciences. S.l.: Princeton University Press.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-06-15T07:42:57-04:00"
    },
    {
      "path": "LDA.html",
      "title": "Topic Models",
      "description": "A bag filled with bags of words.\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nIntuition\r\nStep 1: Load the\r\nDocuments and Tidy Up\r\nStep 2: Convert\r\nto a Document-Term Matrix\r\nStep 3: Fit the Model\r\nStep\r\n4: Interpret the Topic-Level Probability Vectors\r\nStep 5:\r\nInterpret the Document-Level Probability Vectors\r\nPractice Problems\r\nFurther Reading\r\n\r\nIntuition\r\nThe workhorse model for assigning topics to texts is the Latent\r\nDirichlet Allocation (LDA), which is a sort of mix between the bag of words model and clustering. I like to think of it as a “bag\r\nof bags of words”. Imagine that, rather than drawing from a single bag\r\nof words, authors first draw a topic, which has its own special\r\nbag of words. This approach is particularly useful when we think that a\r\ndocument may be about more than one topic, and we don’t want to impose\r\njust one classification for each text like we do with k-means.\r\nTo demonstrate the workflow in R, let’s take the set of\r\nSenator Lautenberg’s press releases from the clustering tutorial and fit an LDA using the\r\ntopicmodels package.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(topicmodels)\r\nlibrary(SnowballC)\r\n\r\n\r\n\r\nStep 1: Load the\r\nDocuments and Tidy Up\r\n\r\n\r\nload('data/press-releases/lautenberg-press-releases.RData')\r\n\r\n\r\ntidy_press_releases <- df |>\r\n  # remove a common preamble to each press release\r\n  mutate(text = str_replace_all(text,\r\n                                pattern = '     Senator Frank R  Lautenberg                                                                                                                      Press Release        of        Senator Lautenberg                                                                                ',\r\n                                replacement = '')) |>\r\n  # tokenize to the word level\r\n  unnest_tokens(input = 'text',\r\n                output = 'word') |>\r\n  # remove stop words\r\n  anti_join(get_stopwords()) |>\r\n  # remove numerals\r\n  filter(str_detect(word, '[0-9]', negate = TRUE)) |>\r\n  # generate word stems\r\n  mutate(word_stem = wordStem(word)) |>\r\n  # count up the word stems in each document\r\n  count(id, word_stem) |> \r\n  # remove empty strings\r\n  filter(word_stem != '')\r\n\r\nhead(tidy_press_releases)\r\n\r\n\r\n# A tibble: 6 x 3\r\n     id word_stem     n\r\n  <int> <chr>     <int>\r\n1     1 account       2\r\n2     1 also          2\r\n3     1 america       2\r\n4     1 american      1\r\n5     1 answer        1\r\n6     1 apologi       1\r\n\r\nStep 2: Convert to a\r\nDocument-Term Matrix\r\nNote LDA requires a matrix of counts, just like the multinomial bag of words model.\r\n\r\n\r\nlautenberg_dtm <- cast_dtm(data = tidy_press_releases,\r\n                           document = 'id',\r\n                           term = 'word_stem',\r\n                           value = 'n')\r\nlautenberg_dtm\r\n\r\n\r\n<<DocumentTermMatrix (documents: 558, terms: 7073)>>\r\nNon-/sparse entries: 81504/3865230\r\nSparsity           : 98%\r\nMaximal term length: 33\r\nWeighting          : term frequency (tf)\r\n\r\nStep 3: Fit the Model\r\nFitting an LDA is just one line of code. It’s the interpretation,\r\nevaluation, and refinement that’s the tricky part.\r\n\r\n\r\nlautenberg_lda <- LDA(lautenberg_dtm, \r\n                      k = 30, \r\n                      control = list(seed = 42))\r\n\r\n\r\n\r\nStep 4:\r\nInterpret the Topic-Level Probability Vectors\r\nLet’s look at the most common terms by topic.\r\n\r\n\r\n# use the tidy() function from tidytext to extract the beta vector\r\nlautenberg_topics <- tidy(lautenberg_lda, matrix = 'beta')\r\n\r\nlautenberg_topics |>\r\n  group_by(topic) |>\r\n  slice_max(beta, n=10) |>\r\n  arrange(topic, -beta)\r\n\r\n\r\n# A tibble: 300 x 3\r\n# Groups:   topic [30]\r\n   topic term         beta\r\n   <int> <chr>       <dbl>\r\n 1     1 program    0.0203\r\n 2     1 grant      0.0194\r\n 3     1 lautenberg 0.0144\r\n 4     1 safeti     0.0143\r\n 5     1 d          0.0136\r\n 6     1 educ       0.0127\r\n 7     1 school     0.0117\r\n 8     1 nj         0.0115\r\n 9     1 fund       0.0112\r\n10     1 provid     0.0110\r\n# ... with 290 more rows\r\n\r\nSurprise, surprise. The most common term in each topic is often\r\n“Lautenberg”. Instead of looking at the terms with the highest\r\nprobability in each bag, let’s look at the terms that are the most\r\nover-represented, compared to their probability in the average\r\ntopic.\r\n\r\n\r\nlautenberg_topics |>\r\n  # get each word's average beta across topics\r\n  group_by(term) |>\r\n  mutate(average_beta = mean(beta)) |>\r\n  ungroup() |>\r\n  # compare beta in that topic with the average beta\r\n  mutate(delta = beta - average_beta) |>\r\n  # get the words with the largest difference in each topic\r\n  group_by(topic) |>\r\n  slice_max(delta, n = 10) |>\r\n  # plot it\r\n  ggplot(mapping = aes(x=delta, y=reorder(term, delta))) +\r\n  geom_col() +\r\n  theme_minimal() +\r\n  facet_wrap(~topic, scales = 'free') +\r\n  labs(x = 'Term Probability Compared to Average',\r\n       y = 'Term')\r\n\r\n\r\n\r\n\r\nTopics 6 and 9 appear to involve words related to transportation\r\ninfrastructure, while topics 2, 5, 8, 17, 21, 23, 27, and 30 appear to\r\nbe about security, the miltary, and foreign affairs. Topics 3, 7, 10,\r\n13, 15, 19, 24, and 26 are all related to the environment. This all\r\nseems consistent with Senator Lautenberg’s work as chairman of the\r\nSenate subcommittees on Homeland Security, Surface Transportation\r\nSecurity, and Superfund, Toxics, and Environmental Health – which should\r\ngive us some confidence in the results. Topics 28 and 29 look like the\r\n“partisan taunting” category identified in the book.\r\nStep 5:\r\nInterpret the Document-Level Probability Vectors\r\nIf these are roughly how we would categorize each topic…\r\n\r\n\r\ntopic_labels <- tribble(~topic, ~label,\r\n                        1, 'Programs',\r\n                        2, 'Military',\r\n                        3, 'Environment',\r\n                        4, 'Health',\r\n                        5, 'Security',\r\n                        6, 'Transportation',\r\n                        7, 'Environment',\r\n                        8, 'Security',\r\n                        9, 'Transportation',\r\n                        10, 'Environment',\r\n                        11, 'Crime and Courts',\r\n                        12, 'Health',\r\n                        13, 'Environment',\r\n                        14, 'Programs',\r\n                        15, 'Environment',\r\n                        16, 'Health',\r\n                        17, 'Military',\r\n                        18, 'Health',\r\n                        19, 'Environment',\r\n                        20, 'Health',\r\n                        21, 'Military',\r\n                        22, 'Crime and Courts',\r\n                        23, 'Oil',\r\n                        24, 'Environment',\r\n                        25, 'New Jersey',\r\n                        26, 'Environment',\r\n                        27, 'Security',\r\n                        28, 'Partisan Taunting',\r\n                        29, 'Partisan Taunting',\r\n                        30, 'Security')\r\n\r\n\r\n\r\n…then here’s what the breakdown in topics across the 558 press\r\nreleases looks like.\r\n\r\n\r\nlautenberg_documents <- tidy(lautenberg_lda, matrix = 'gamma')\r\n\r\nlautenberg_documents |> \r\n  # join with topic labels\r\n  mutate(document = as.numeric(document)) |>\r\n  left_join(topic_labels, by = 'topic') |> \r\n  # get the most probable document labels\r\n  filter(gamma > 0.3) |> \r\n  arrange(document, -gamma) |> \r\n  head(20)\r\n\r\n\r\n# A tibble: 20 x 4\r\n   document topic gamma label            \r\n      <dbl> <dbl> <dbl> <chr>            \r\n 1        1    22 0.999 Crime and Courts \r\n 2        2    18 0.999 Health           \r\n 3        3    23 0.999 Oil              \r\n 4        4     7 0.976 Environment      \r\n 5        5    11 0.693 Crime and Courts \r\n 6        5    29 0.302 Partisan Taunting\r\n 7        6    26 0.912 Environment      \r\n 8        7     6 0.936 Transportation   \r\n 9        8    11 0.897 Crime and Courts \r\n10        9    16 0.991 Health           \r\n11       10    18 0.996 Health           \r\n12       11    13 0.520 Environment      \r\n13       11    17 0.382 Military         \r\n14       12    10 0.477 Environment      \r\n15       12     7 0.363 Environment      \r\n16       13     9 0.403 Transportation   \r\n17       13     6 0.318 Transportation   \r\n18       14    24 0.705 Environment      \r\n19       15    27 0.999 Security         \r\n20       16    21 0.998 Military         \r\n\r\nDocument 1 should be about Crime/Courts.\r\n\r\n\r\nprint_text <- function(text){\r\n  cat(str_wrap(text), sep = '\\n')\r\n}\r\n\r\nprint_text(df$text[1])\r\n\r\n\r\nSenator Frank R Lautenberg Press Release of Senator Lautenberg Lautenberg Cites\r\nCriminal Laws DeLay May Have Broken in Threat Against Federal Judges Friday\r\nApril 1 2005 WASHINGTON DC Responding to possible violations of criminal law\r\nby House Majority Leader Tom DeLay when he directed threatening remarks toward\r\nfederal judges involved in the Terri Schiavo case Untied Stated Senator Frank R\r\nLautenberg today called on Mr DeLay to renounce his comments In a letter to Mr\r\nDeLay Senator Lautenberg said the remarks could incite violence against judges\r\nand noted that federal statutes provide for prison terms up to six years for\r\nthreatening members of the court Threats against specific Federal judges are\r\nnot only a serious crime but also beneath a Member of Congress In my view the\r\ntrue measure of democracy is how it dispenses justice Your attempt to intimidate\r\njudges in America not only threatens our courts but our fundamental democracy\r\nas well wrote Lautenberg in his letter to Mr DeLay Majority Leader DeLay s\r\ncomments yesterday may violate a Federal criminal statute 18 U S C 115 a 1 B\r\nThat law states Whoever threatens to assault or murder a United States judge\r\nwith intent to retaliate against such judge on account of the performance of\r\nofficial duties shall be punished by up to six years in prison A copy of the\r\nentire letter is attached to this release April 1 2005 Tom DeLay Majority Leader\r\nHouse of Representatives Washington DC 20515 Dear Majority Leader DeLay I was\r\nstunned to read the threatening comments you made yesterday against Federal\r\njudges and our nation s courts of law in general In reference to certain Federal\r\njudges you stated The time will come for the men responsible for this to answer\r\nfor their behavior As you are surely aware the family of Federal Judge Joan H\r\nLefkow of Illinois was recently murdered in their home And at the state level\r\nJudge Rowland W Barnes and others in his courtroom were gunned down in Georgia\r\nOur nation s judges must be concerned for their safety and security when they\r\nare asked to make difficult decisions every day That s why comments like those\r\nyou made are not only irresponsible but downright dangerous To make matters\r\nworse is it appropriate to make threats directed at specific Federal and state\r\njudges You should be aware that your comments yesterday may violate a Federal\r\ncriminal statute 18 U S C 115 a 1 B That law states Whoever threatens to\r\nassault or murder a United States judge with intent to retaliate against such\r\njudge on account of the performance of official duties shall be punished by up\r\nto six years in prison Threats against specific Federal judges are not only a\r\nserious crime but also beneath a Member of Congress In my view the true measure\r\nof democracy is how it dispenses justice Your attempt to intimidate judges in\r\nAmerica not only threatens our courts but our fundamental democracy as well\r\nFederal judges as well as state and local judges in our nation are honorable\r\npublic servants who make difficult decisions every day You owe them and all\r\nAmericans an apology for your reckless statements Sincerely Frank R Lautenberg\r\nQuestions or Comments\r\n\r\nDocument 2 should be about Health.\r\n\r\n\r\nprint_text(df$text[2])\r\n\r\n\r\nSenator Frank R Lautenberg Press Release of Senator Lautenberg Sens Lautenberg\r\nand Menendez Lead Defeat of Amendment Specifically Targeting NJ s Children\r\nHealth Coverage Bunning Amendment Would Cast Thousands of NJ Children Into The\r\nRanks of The Uninsured Contact Press Office 202 224 3224 Wednesday August 1 2007\r\nWASHINGTON New Jersey Senators Frank R Lautenberg D NJ and Robert Menendez D NJ\r\nled the effort that tonight resulted in the defeat of an amendment specifically\r\ntargeting childrens health insurance in New Jersey The Bunning amendment to\r\nthe Childrens Health Insurance Program CHIP reauthorization bill was tabled by\r\na 53 43 vote The amendment was a direct shot at New Jerseys FamilyCare program\r\nwhich covers children from families that make up to 350 of the federal poverty\r\nlevel working and low income families that dont qualify for Medicaid but cannot\r\nafford health insurance in a high cost of living state like New Jersey Had\r\nthe amendment passed only children in New Jersey would have been immediately\r\naffected Lautenberg and Menendez helped persuade colleagues to oppose the\r\namendment and spoke vehemently in opposition Today the U S Senate defeated an\r\nattack on the health of New Jerseys children said Lautenberg We stood strong\r\nagainst right wing efforts to take away the health insurance of 3 000 children\r\nin New Jersey This vote means we can continue to provide quality affordable\r\nhealth care to children in New Jersey and nationwide This was a rifle shot at\r\nNew Jersey and we worked hard to make our colleagues understand what it meant\r\nfor our states children and why it had to be deflected said Menendez Without\r\nthis level of coverage thousands of New Jersey children would have been dropped\r\ninto the ocean of the uninsured Throughout the debate on this bill we have\r\nbeen repeatedly confronted with amendments attacking coverage for children and\r\nfamilies in New Jersey and we have repeatedly held our ground and rejected them\r\nTodays action brings us one step closer to a major victory for working and low\r\nincome children and families in our state Among the amendments offered to this\r\npoint during Senate debate of CHIP three in particular have taken aim at New\r\nJerseys strong health coverage program and all three have been defeated Bunning\r\nAmendment Motion to Table Passed 53 43 This amendment would have reduced the\r\nreimbursement rate for CHIP covered children above 300 of poverty in all states\r\nto the Medicaid matching rate including states that already cover these kids\r\nunder CHIP Only New Jersey has an eligibility level above 300 now in effect New\r\nYork has enacted legislation increasing its eligibility to 400 but has not yet\r\ngotten approval from the Secretary of HHS for its state plan amendment to make\r\nthe change This amendment unfairly targeted a small percentage of CHIP covered\r\nchildren in New Jersey This amendment would have pushed 3 000 New Jersey kids\r\noff the CHIP program Gregg Amendment Failed 42 53 This amendment would have\r\npushed all parents covered under CHIP into Medicaid Over 80 000 parents would\r\nhave lost coverage in NJ Allard Amendment Failed 37 59 The amendment would\r\nhave disallowed states from using any type of income disregards to determine\r\neligibility in CHIP Under current law states are permitted to disregard types\r\nof income or blocks of income and most states use this flexibility to disregard\r\nwages child support payments and child care expenses to enable working families\r\nto earn a living wage and still be eligible for CHIP This amendment would have\r\ndisrupted NJs ability to enroll children at a higher income level in CHIP It\r\nwould have prevented us from covering kids above 200 of the federal poverty\r\nlevel and would have jeopardized coverage for over 30 000 New Jersey children\r\nQuestions or Comments\r\n\r\nDocument 5 seems to be some combination of Crime/Courts and Partisan\r\nTaunting.\r\n\r\n\r\nprint_text(df$text[5])\r\n\r\n\r\nSenator Frank R Lautenberg Press Release of Senator Lautenberg Statement by\r\nSenator Lautenberg on the Retirement of Justice Sandra Day O Connor Friday\r\nJuly 1 2005 WASHINGTON D C United States Senator Frank R Lautenberg issued\r\nthe following statement today regarding the retirement of Justice Sandra\r\nDay O Connor Justice O Connor has earned a place in history not only as the\r\nfirst woman to serve on the Supreme Court but also as an independent thinker\r\nwho avoided extreme positions The American people want fair moderate judges\r\nprotecting our rights and I strongly urge President Bush to send us a nominee\r\nwho reflects mainstream legal views not partisan extremes Questions or Comments\r\n\r\nNot bad! It’s a press release that is mostly honoring Sandra Day\r\nO’Connor, but the last sentence is a dig at President Bush.\r\nPractice Problems\r\nFit an LDA to the Federalist Paper corpus (instead of focusing on\r\nstop words as in the authorship prediction task, I’d advice removing\r\nstop words and focusing on the substantive terms). What sorts of topics\r\ndoes the model produce? What value of \\(k\\) yields the most sensible set of\r\ntopics?\r\nFit an LDA to the UN\r\nSecurity Council speeches about Afghanistan (Schoenfeld et al. 2018), available at\r\ndata/un-security-council/UNSC_Afghan_Spchs_Meta.RData on\r\nthe repository.\r\nFurther Reading\r\nGrimmer,\r\nStewart, and Roberts (2021) Chapter 13\r\nText\r\nMing With R Chapter 6\r\nFor a principled procedure for varying \\(k\\) to identify the best set of topics, see\r\nWilkerson and Casas\r\n(2017) .\r\n\r\n\r\n\r\nGrimmer, Justin, Brandon M. Stewart, and Margaret E. Roberts. 2021.\r\nText as Data: A New Framework for Machine Learning and the Social\r\nSciences. S.l.: Princeton University Press.\r\n\r\n\r\nSchoenfeld, Mirco, Steffen Eckhard, Ronny Patz, and Hilde van\r\nMeegdenburg. 2018. “Discursive Landscapes and Unsupervised Topic\r\nModeling in IR: A Validation of Text-as-Data Approaches Through a New\r\nCorpus of UN Security Council Speeches on Afghanistan.” https://doi.org/10.48550/arXiv.1810.05572.\r\n\r\n\r\nWilkerson, John, and Andreu Casas. 2017. “Large-Scale Computerized\r\nText Analysis in Political Science: Opportunities and\r\nChallenges.” Annual Review of Political Science 20 (1):\r\n529–44. https://doi.org/10.1146/annurev-polisci-052615-025542.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-06-15T07:43:06-04:00"
    },
    {
      "path": "OCR.html",
      "title": "Optical Character Recognition (OCR)",
      "description": "What to do when someone asks you to type up 100 pages of text from clippings of old newspapers.\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nOptical Character\r\nRecognition\r\nImage Pre-Processing\r\nPractice Problems\r\nFurther Reading\r\n\r\nIn the webscraping and Twitter API tutorials, we worked with texts\r\nthat are already stored in digital form, so that getting them into\r\nR is just a matter of removing all the HTML code. But other\r\ntexts (lots of texts) are not so digitally accessible. Maybe\r\nyou’re interested in historical archives living in a dark basement. Or\r\nold press releases living in a PDF. In either case, we need a method\r\nthat can recognize text in images, and convert it into\r\nplaintext. This is a job for Optical Character Recognition (OCR).\r\nOptical Character\r\nRecognition\r\nOCR is a notoriously difficult task for computers, which is why the\r\n“are you a human” tests on some websites might ask you to type a bunch\r\nof numbers and letters within a blurry or distorted image.\r\n\r\nBut if the text is in straight lines on a white background,\r\noff-the-shelf OCR packages do a pretty good job. The workhorse OCR\r\nengine is called Tesseract,\r\nand it’s available in R through a package called\r\ntesseract.1\r\nLet’s see if it can recognize the text in that xkcd comic above,\r\nusing the ocr() function.\r\n\r\n\r\nlibrary(tesseract)\r\n\r\nxkcd <- ocr(image = 'img/suspicion.png')\r\n\r\nxkcd\r\n\r\n\r\n[1] \"Fie, HED UR 1S JUST... NOW AND THEN | BEFORE THIS GOES ANY FURTHER,\\nONLINE CHATS You MENTION PRODUCTS YOU | T THINK WE SHOULD GO GET TESTED,\\nTHESE. PAST FEW LIKE, AND... I UORRY. | YOUKNOU, TOGETHER. VK Coupes Tene\\nONTHS, I = PROVES Cape.\\nUSA. \\\\ ROM KE. WHAT? HONEY... \\\\ You Dont Taist mee\\nYOU, ROB. I. JUST WANT To BE\\n4 SURE. O S\\nio) #) OKAY, PINE SANS Uy\\nd z PUBRARY: YOURS? \\\"5\\n\\\\ iS TH MORE THAN A\\n4) WE OCOD. SeAMBOT! OUR LOVE\\nVe = Goooere, usa, 4 WAS REAL!\\nana VC\\n\"\r\n\r\nClearly it cannot make out “Library” and “Kittens”, but by my count\r\nit reads about 65% of these handwritten words correctly. They’re not in\r\nthe right order though. Tesseract reads from left to right, top to\r\nbottom, and does not understand things like comic panels or speech\r\nbubbles. If we’re just using a bag of words representation and we don’t\r\ncare about word order, then we can use the ocr_data()\r\nfunction, which splits the input into a dataframe with one row for each\r\nword. It even includes a handy confidence column, which\r\ntells you how confident the model is in its prediction.\r\n\r\n\r\nxkcd <- ocr_data(image = 'img/suspicion.png')\r\n\r\nhead(xkcd)\r\n\r\n\r\n# A tibble: 6 x 3\r\n  word    confidence bbox        \r\n  <chr>        <dbl> <chr>       \r\n1 Fie,           0   4,6,37,41   \r\n2 HED           41.0 50,6,74,41  \r\n3 UR            48.8 85,6,107,41 \r\n4 1S            55.7 170,9,185,21\r\n5 JUST...       82.5 191,9,318,23\r\n6 NOW           65.9 238,5,260,33\r\n\r\nBut if we do care about word order, then we’ll need to be\r\nmore careful about pre-processing the image before conducting OCR. For\r\nexample, let’s try to read in the text on page 3 of this document about the California Supreme Court.\r\nThe pdftools package can convert the page to an image, when\r\nwe can then OCR.\r\n\r\n\r\nlibrary(pdftools)\r\n\r\npdf_convert('img/SOJ.pdf', pages = 3, dpi = 600, filenames = 'img/SOJ.png')\r\n\r\n\r\nConverting page 3 to img/SOJ.png... done!\r\n[1] \"img/SOJ.png\"\r\n\r\ntext <- ocr('img/SOJ.png')\r\n\r\ntext\r\n\r\n\r\n[1] \"Remarks  . ~~\\n_ By Chief Justice Rose Elizabeth Bird*  - a,\\nIt is a pleasure to be here this afternoon. I own making. Our court system is an indepen-\\nwould like to thank the Conference of dent branch of government, and there is\\nDelegates for affording me this opportunity to strength to be derived from thai unity. But un-\\nshare with you some thoughts about issues til we judges begin to see ourselves as part of :\\nwhich judges, lawyers, and citizens of Califor- an organic whole, that strength will be |\\nnia willbe facing during thecoming year. _ . dissipated and wasted. .\\nOur democratic form of government is Sadly, our justice system is marred by\\nblessed with a court system that is particular- fractionization and segmentation at all levels.\\nly well designed to resolve the disputes that Judging is by its nature often a solitary and\\narise in a heterogeneous society. There is time-consuming task. Its demands tend to\\nstrength in the diversity of views found in our isolate judges from one another, even though\\nsociety, but that strength cannot be drawn they may serve on the same court. This isola-\\nupon unless conflicting views are moderated tion is intensified when judges are sitting at\\nand balanced and excesses checked. different levels and is sometimes characteriz-\\nThe courts hold a unique position among —_ ed by feelings of elitism and of superiority\\nour democratic institutions. In a sense, they over judges of “lower courts,” __ |\\nrepresent one of the last bastions of par- _ dn turn, these antagonistic feelings have 5\\nticipatory democracy in our society. They — often led judges to take a competitive, rather i\\nstand as a symbol of the great strength of our than a cooperative, view of one another ~- an .\\n- representative form of government. The in- attitude which only further deepens the sense\\n. dividual disputants go directly before a judge of isolation and fragmentation.\\nora jury to raise and resolve a specific issue. It is time for this cycle to stop. Our justice §\\nIn no other context within our governmental system is an interrelated whole, and the more\\n, system does an individual have the opportuni- that judges in one part of the system unders-\\nty to take a problem directly to the decision ._ tand how the other levels function, the more i\\nmaker who represents the full force and power effective we will be in meeting the complex i\\nof that particular branch of government. realities of the society we serve. The judges at i\\nThis direct interchange between the in- different levels of the system have different\\ndividual and the state is at the heart of the but equally important roles to play. :\\ndemocratic process. AS more barriers are. There are many things that can be done to\\nraised between the litigant and the decision- | promote the spirit of cooperation of which I\\nmaker, the participatory nature of the ex- speak. And this spirit can be achieved without i\\nperience is diminished. We must protect this imposing any constraints on the power of\\nunique heritage and strive to preserve the judges torun their own courtrooms or to make :\\nvalues it represents. local administrative decisions, which they are\\nThe barriers to which I refer are in part uniquely well situated to do. :\\nthe resuit of the increasingly complex society An important step toward this sense of\\nin which we live. However, I fear that to some common judicial venture could be taken by\\nextent these barriers are of the judiciary’s providing state funding for our trail courts.\\nnen This issue, though not a new one, was recently\\n* California Supreme Court. This is a speech given brought into sharp focus by the passage of pro-\\nbefore the Canference of Delegates at the State position 13 and the availability of local funding\\nBar Convention in San Francisco, Cal. on Septem- for the trial courts. As you may know, Califor-\\n, ber 10, 1978. nia ranks last among all states in the percen- |\\n4\\n\"\r\n\r\nNotice two things. First, ocr() performs much better\r\nwith the typed text than it did with the handwritten comic. Second, all\r\nthe words are out of order, because it reads left to right across the\r\ntwo columns on that page. So we need to first crop the image into two\r\ncolumns and read each column separately.\r\nFor that, we’ll turn to the magick package.\r\nImage Pre-Processing\r\n\r\n\r\nlibrary(magick)\r\n\r\n# First, read in the image with magick\r\npage3 <- image_read('img/SOJ.png')\r\npage3\r\n\r\n\r\n\r\n# Next, crop it into two images with image_crop()\r\n# syntax is width x height + left offset + top offset\r\npage3_left <- image_crop(page3, '2550 x 4300 + 0 + 1000')\r\npage3_left\r\n\r\n\r\n\r\npage3_right <- image_crop(page3, '2550 x 5000 + 2550 + 1000')\r\npage3_right\r\n\r\n\r\n\r\n# Finally, ocr() each column, then paste the results together\r\ntext_left <- ocr(page3_left)\r\ntext_right <- ocr(page3_right)\r\n\r\ntext <- paste(text_left, text_right)\r\n\r\ntext\r\n\r\n\r\n[1] \"It is a pleasure to be here this afternoon. I\\nwould like te thank the Conference of\\nDelegates for affording me this opportunity to\\nshare with you some thoughts about issues\\nwhich judges, lawyers, and citizens of Califor-\\nnia willbe facing during thecoming year. _\\n\\nOur democratic form of government is\\nblessed with a court system that is particular-\\nly well designed to resolve the disputes that\\narise in a heterogeneous society. There is\\nstrength in the diversity of views found in our\\nsociety, but that strength cannot be drawn\\nupon unless conflicting views are moderated\\nand balanced and excesses checked.\\n\\nThe courts hold a unique position among\\nour democratic institutions. In a sense, they\\nrepresent one of the last bastions of par-\\nticipatory democracy in our society. They\\nstand as a symbol of the great strength of our\\n\\n. representative form of government. The in-\\n\\n. dividual disputants go directly before a judge\\n\\nora jury to raise and resolve a specific issue.\\n\\nIn no other context within our governmental\\n\\n, system does an individual have the opportuni-\\nty to take a problem directly to the decision |\\n\\nmaker who represents the full force and power\\n\\nof that particular branch of government.\\n\\nThis direct interchange between the in-\\ndividual and the state is at the heart of the\\ndemocratic precess. AS more barriers are\\nraised between the litigant and the decision- _\\nmaker, the participatory nature of the ex-\\nperience is diminished. We must protect this\\nunique heritage and strive to preserve the\\nvalues it represents.\\n\\nThe barriers to which I refer are in part\\nthe result of the increasingly complex society\\nin which we live. However, I fear that to some\\nextent these barriers are of the judiciary’s\\n own making. Our court system is an indepen-\\ndent branch of government, and there is\\nstrength to be derived from thai unity. But un-\\ntil we judges begin to see ourselves as part of :\\nan organic whole, that strength will be |\\n. dissipated and wasted. ,\\nSadly, our justice system is marred by\\nfractionization and segmentation at all levels.\\nJudging is by its nature often a solitary and\\ntime-consuming task. Its demands tend to\\nisolate judges from one another, even though\\nthey may serve on the same court. This isola-\\ntion is intensified when judges are sitting at\\ndifferent levels and is sometimes characteriz-\\ned by feelings of elitism and of superiority\\nover judges of “lower courts,” __ }\\n_ dn turn, these antagonistic feelings have 5\\n_ often led judges to take a competitive, rather i\\nthan a cooperative, view of one another ~- an\\nattitude which only further deepens the sense\\nof isolation and fragmentation.\\n\\nIt is time for this cycle to stop. Our justice f\\nsystem is an interrelated whole, and the more\\nthat judges in one part of the system unders-\\ntand how the other levels function, the more |\\neffective we will be in meeting the complex i\\nrealities of the society we serve. The judges at\\ndifferent levels of the system have different P\\nbut equally important roles to play. :\\n\\n; There are many things that can be done to :\\npromote the spirit of cooperation of which I\\nspeak. And this spirit can be achieved without i\\nimposing any constraints on the power of\\njudges torun their own courtrooms or to make :\\nlocal administrative decisions, which they are\\nuniquely well situated to do. :\\n\\nAn important step toward this sense of\\ncommon judicial venture could be taken by\\nproviding state funding for our trail courts.\\n\\nThis issue, though not a new one, was recently\\nbrought into sharp focus by the passage of pro-\\nposition 13 and the availability of local funding\\nfor the trial courts. As you may know, Califor-\\nnia ranks last among all states in the percen- |\\n\"\r\n\r\nPractice Problems\r\nOCR the text on page 4 of SOJ.pdf.\r\nOCR the text in this newspaper\r\nclipping about the sinking of the Titanic.\r\nFurther Reading\r\nFor more tesseract features, see this vignette\r\nState of the art optical character recognition uses advances in\r\nconvolutional neural networks to handle handwritten and other kinds of\r\ndifficult-to-read texts. See Torres and Cantú (2021) for a primer.\r\n\r\n\r\n\r\nTorres, Michelle, and Francisco Cantú. 2021. “Learning to See:\r\nConvolutional Neural Networks for the Analysis of Social Science\r\nData.” Political Analysis, April, 1–19. https://doi.org/10.1017/pan.2021.9.\r\n\r\n\r\nAs of May 2022,\r\ntesseract doesn’t play nicely with version 4.2.0 of\r\nR, so you’ll want one of the 4.1 versions.↩︎\r\n",
      "last_modified": "2022-06-15T07:43:23-04:00"
    },
    {
      "path": "sentiment-analysis.html",
      "title": "Sentiment Analysis",
      "description": "Teaching the computer to understand feelings.\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nHand Coding\r\nDictionary\r\nClassification\r\nUp Next: Supervised\r\nLearning\r\nPractice Problems\r\nFurther Reading\r\n\r\nFor unsupervised learning models like k-means and LDA, the\r\nobjective is often discovery. We have a set of unlabeled data, and we\r\nwant to get a sense of how we might organize the documents, how we might\r\nsort them into buckets. But frequently social scientists turn to text\r\ndata because we’re interested in measuring some concept that is\r\ntough to quantify in other ways. For this, we’ll want a different set of\r\ntools – particularly supervised learning methods – because we\r\nhave an objective and we want to build a model that satisfies that\r\nobjective.\r\nTo illustrate the process of measurement using text data, let’s\r\nconsider the field of sentiment analysis. We have a set\r\nof documents, and we’re interested in classifying the sentiment/emotion\r\nthe author is trying to convey – positive, negative, or neutral. Here is\r\na dataset of 945 tweets about the Supreme Court that I compiled for a\r\nproject with Jake Truscott and Elise Blasingame.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\n\r\n# load the tweets\r\ntweets <- read_csv('data/supreme-court-tweets.csv')\r\n\r\ntweets |> select(-tweet_id)\r\n\r\n\r\n# A tibble: 945 x 4\r\n   text                                        expert1 expert2 expert3\r\n   <chr>                                         <dbl>   <dbl>   <dbl>\r\n 1 \"Just in time for #PrideMonth #Pride2018 #~       1      -1      -1\r\n 2 \"The silliness of the day-long kerfuffle o~       1      -1      -1\r\n 3 \"The @Scotus ruling was a \\U0001f967 pie-i~       1      -1       1\r\n 4 \"Let’s be real, lame anti-gay cake probabl~       1       1      -1\r\n 5 \"#ReligiousFreedom #SCOTUS \\r\\nWould a Mus~       1      -1      -1\r\n 6 \"I’m happy SCOTUS ruled in favor of the ba~       1      -1      -1\r\n 7 \"Breaking: Supreme Court rules New York pr~       1      -1       0\r\n 8 \"SUPREME COURT: \\r\\n\\r\\nTRUMP IS NOT ABOVE~       1       1      -1\r\n 9 \"Not gonna lie....shocked Kavanaugh rules ~      -1       1       1\r\n10 \"Gonna have a drink at “the low bar” to ce~      -1      -1       1\r\n# ... with 935 more rows\r\n\r\nHand Coding\r\nThe dataset contains the text of the tweet, plus three “expert”\r\nratings on a scale from -1 (negative), 0 (neutral), to 1 (positive).\r\nEach author independently read and coded every tweet1\r\nthen discussed the cases where we disagreed, and went back for a second\r\nround of coding on those tweets where everyone produced a different\r\nmeasure. One way to assess how well we did at capturing sentiment is\r\ninter-coder reliability (aka Fleiss’ kappa),\r\nwhich measures how frequently the coders agreed relative to chance, on a\r\nscale from -1 (completely disagreement on everything) to 1 (perfect\r\nagreement).\r\n\r\n\r\nlibrary(irr)\r\ntweets |>\r\n  select(expert1, expert2, expert3) |>\r\n  kappam.fleiss()\r\n\r\n\r\n Fleiss' Kappa for m Raters\r\n\r\n Subjects = 920 \r\n   Raters = 3 \r\n    Kappa = 0.688 \r\n\r\n        z = 48.9 \r\n  p-value = 0 \r\n\r\nInterestingly, inter-coder reliability is much worse on the first 100\r\ntweets than on the last 845, suggesting that it took a while for us to\r\nget used to the task, and that we should go back and try those first 100\r\ntweets again.\r\n\r\n\r\ntweets |> \r\n  group_by(row_number() <= 100) |> \r\n  summarize(kappa = kappam.fleiss(cbind(expert1, expert2, expert3))$value)\r\n\r\n\r\n# A tibble: 2 x 2\r\n  `row_number() <= 100` kappa\r\n  <lgl>                 <dbl>\r\n1 FALSE                 0.719\r\n2 TRUE                  0.447\r\n\r\nDictionary Classification\r\nUNDER CONSTRUCTION (SEE NOTES)\r\nUp Next: Supervised Learning\r\nBoth human-coding and dictionary classification are examples of\r\nrule-based measurement. You decide in advance exactly what\r\nsteps you will take to measure each document, and then you (or your\r\ncomputer) follows the rules you set out. The problem with such\r\nrules-based measures is that they are either:\r\nNot scalable (e.g. human-coding). For a dataset of 945 tweets, we\r\nwere able to tackle it in relatively short order. But if we were\r\ninterested in 100,000 tweets? Or a million tweets? There’s no way to\r\nscale that procedure, except with crowd coding on something like\r\nAmazon’s MTurk (Benoit et al. 2016; Carlson and Montgomery 2017), and that\r\ngets expensive quickly.\r\nScalable, but terrible (e.g. dictionary methods). With dictionary\r\nmethods, it’s trivial to classify a million tweets. But the results, as\r\nwe have seen, are fairly crummy.\r\nThe best alternative to rules-based classification is\r\nstatistical classification, and that is the topic of the next page.\r\nPractice Problems\r\nHow accurate does the dictionary classifier need to be until it’s\r\n“good enough”? A useful benchmark is to compare your model against a\r\nnull model. For example, in the Twitter corpus, how accurate is\r\nthe null model “predict every tweet will be negative”?\r\nHow accurate can you get the dictionary classifier to be, by varying\r\nthe lexicon and modifying the word list to match our specific context\r\n(i.e. filtering out words whose dictionary meaning and context-specific\r\nmeaning are different, like “Trump” and “Supreme”)?\r\nFurther Reading\r\n\r\n\r\n\r\nBenoit, Kenneth, Drew Conway, Benjamin E. Lauderdale, Michael Laver, and\r\nSlava Mikhaylov. 2016. “Crowd-Sourced Text Analysis: Reproducible\r\nand Agile Production of Political Data.” American Political\r\nScience Review 110 (2): 278–95. https://doi.org/10.1017/S0003055416000058.\r\n\r\n\r\nCarlson, David, and Jacob M. Montgomery. 2017. “A Pairwise\r\nComparison Framework for Fast, Flexible, and Reliable Human Coding of\r\nPolitical Texts.” American Political Science Review 111\r\n(4): 835–43. https://doi.org/10.1017/S0003055417000302.\r\n\r\n\r\nYes, it was dreadful and I don’t\r\nrecommend trying this at home.↩︎\r\n",
      "last_modified": "2022-06-15T07:43:27-04:00"
    },
    {
      "path": "twitter-api.html",
      "title": "Twitter API",
      "description": "How to get the tweets",
      "author": [],
      "contents": "\r\n\r\nContents\r\nExample Workflow\r\nIntroducing Yourself To\r\nTwitter\r\nPractice Problems\r\nFurther Reading\r\n\r\nIn the webscraping tutorial, we\r\nharvested text directly from the HTML code of a webpage. This can be\r\npretty laborious, so fortunately, there are some websites that provide\r\nan easier path to collecting their data, called an Application\r\nProgramming Interface (API). To introduce ourselves to APIs, we’ll pull\r\ntext data from Twitter using the fantastic rtweet\r\npackage.\r\nIf you don’t have a Twitter account, take a moment to sign up.\r\nExample Workflow\r\nThe following code chunk downloads the 100 most recent tweets that\r\nmention the president’s account (@potus). Then it performs\r\na rudimentary sentiment analysis,\r\nmerging it with a sentiment lexicon and counting the number of positive\r\nvs. negative words in each tweet.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(rtweet)\r\n\r\ntweets <- search_tweets('@potus', n = 100)\r\n\r\ntweets |> \r\n  # create a unique ID for each tweet\r\n  mutate(ID = 1:nrow(tweets)) |> \r\n  # tokenize to words\r\n  unnest_tokens(input = 'text',\r\n                output = 'word') |> \r\n  # merge with sentiment lexicon\r\n  inner_join(get_sentiments('bing')) |> \r\n  group_by(ID) |>\r\n  summarize(positive_words = sum(sentiment == 'positive'),\r\n            negative_words = sum(sentiment == 'negative'),\r\n            tweet_sentiment = (positive_words - negative_words) / (positive_words + negative_words)) |> \r\n  ggplot(mapping = aes(x = tweet_sentiment)) + \r\n  geom_histogram(color = 'black') +\r\n  theme_minimal() +\r\n  labs(x = 'Tweet Sentiment', y = 'Number of Tweets',\r\n       title = 'Sentiment of Tweets with @potus')\r\n\r\n\r\n\r\n\r\n\r\n\r\nAnd here is a word cloud of the most common words in those\r\ntweets.\r\n\r\n\r\nlibrary(wordcloud2)\r\n\r\ntweets |> \r\n  unnest_tokens(input = 'text', output = 'word') |> \r\n  count(word) |> \r\n  anti_join(get_stopwords()) |> \r\n  # remove filler words and the potus handle itself\r\n  filter(!(word %in% c('potus', 'https', 't.co')), \r\n         n > 4) |> \r\n  # wordcloud2() needs a column called freq\r\n  rename(freq = n) |> \r\n  wordcloud2()\r\n\r\n\r\n\r\n{\"x\":{\"word\":[\"1\",\"1969\",\"achievements\",\"act\",\"america\",\"american\",\"americans\",\"asian\",\"baby\",\"back\",\"breadth\",\"celebrate\",\"communities\",\"congress\",\"crisis\",\"culture\",\"democracy\",\"diversity\",\"even\",\"fell\",\"fellow\",\"first\",\"formula\",\"get\",\"half\",\"hawaiian\",\"insurance\",\"islander\",\"just\",\"know\",\"make\",\"media\",\"month\",\"nation\",\"native\",\"pacific\",\"package\",\"passed\",\"people\",\"percent\",\"percentage\",\"por\",\"receiving\",\"recognize\",\"secblinken\",\"security\",\"shortage\",\"since\",\"stop\",\"stronger\",\"tar5xvusks\",\"tigray\",\"tigraygenocide\",\"time\",\"today\",\"trump\",\"ukraine\",\"un\",\"unemployment\",\"us\",\"vp\",\"war\",\"weapons\",\"women\",\"work\",\"world\"],\"freq\":[10,6,5,9,8,5,9,5,8,10,5,5,6,6,5,5,8,5,6,6,5,9,11,6,5,5,6,5,6,8,10,7,5,7,5,5,9,5,16,6,6,5,6,5,6,7,5,6,6,6,5,11,5,9,6,5,15,11,7,7,5,5,6,5,6,8],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":\"random-dark\",\"minSize\":0,\"weightFactor\":11.25,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.785398163397448,\"maxRotation\":0.785398163397448,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":{\"render\":[{\"code\":\"function(el,x){\\n                        console.log(123);\\n                        if(!iii){\\n                          window.location.reload();\\n                          iii = False;\\n\\n                        }\\n  }\",\"data\":null}]}}\r\nIntroducing Yourself To\r\nTwitter\r\nIf you end up wanting to use this extensively, you should apply for a\r\ndeveloper\r\naccount, which will permit you to download more tweets per month.\r\nOnce you follow the instructions at that link, save the four enormous\r\npasswords it gives you in four separate text documents (you’ll need the\r\nAPI key, API secret, access token, and access token). Then, at the\r\nbeginning of your script, include the create_token()\r\nfunction to authenticate your credentials with Twitter.\r\n\r\n\r\ncreate_token(\r\n  app = \"Maymester\",\r\n  consumer_key = read_file('data/twitter-keys/api-key.txt'),\r\n  consumer_secret = read_file('data/twitter-keys/api-key-secret.txt'),\r\n  access_token = read_file('data/twitter-keys/access-token.txt'),\r\n  access_secret = read_file('data/twitter-keys/access-token-secret.txt')\r\n)\r\n\r\n\r\n\r\nIt is good practice not to include your passwords directly in the\r\nR script, especially if you intend to share you replication\r\ncode with others, which is why we’re reading them from an outside\r\nfile.\r\nPractice Problems\r\nPull a sample of 1,000 tweets referencing “inflation”, remove the\r\nstop words, and create a word cloud of the most common words in those\r\ntweets.\r\nPull a sample of 500 tweets referencing Ukraine and create a word\r\ncloud of the most common words (filtering out the stop words).\r\nFurther Reading\r\nMichael\r\nKearney’s rtweet tutorial\r\n\r\n\r\n\r\n",
      "last_modified": "2022-06-15T07:43:30-04:00"
    },
    {
      "path": "under-construction.html",
      "title": "Under Construction",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2022-06-15T07:43:31-04:00"
    },
    {
      "path": "webscraping.html",
      "title": "Webscraping Tutorial",
      "description": "What to do when you just want text, but the website where the text lives is trying to sell you prescription medications or something.",
      "author": [],
      "contents": "\r\n\r\nContents\r\nThe rvest package\r\nReading HTML\r\nGetting the Right\r\nElements\r\nSelectorGadget\r\n\r\nBeing Polite\r\nPractice Problems\r\n\r\nThe central difficulty we face scraping text data from the web is\r\nthat web pages are never just text. They come with a whole\r\nbunch of other junk to make the text look pretty. That junk is written\r\nin HTML (Hypertext Markup Language) code, and our first task as\r\nresearchers is to separate the plain text we want from all the HTML code\r\nthat’s making it look pretty.\r\nFor example, suppose for some reason I wanted to know what Tucker\r\nCarlson said on his television program on April 20, 2022. The transcript\r\nis here,\r\nbut it’s cluttered. There are graphics, ads, pictures, links to other\r\npages, fonts, and a bunch of other things we don’t need for our\r\nresearch. Fortunately, the plain text of the transcript is hiding in the\r\npage’s HTML code, if we know where to look.\r\nThe rvest package\r\nAs of writing (May 2022), the most user-friendly R\r\npackage for getting text data from web pages is rvest\r\n(read that name like “harvest”, as in harvesting data).\r\nLet’s begin by loading that package.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(rvest)\r\n\r\n\r\n\r\nReading HTML\r\nTo read the HTML from a web page, we can use the\r\nread_html() function, just like we would read a data file\r\nfrom our computer. Just supply it with the web page’s URL.\r\n\r\n\r\npage <- read_html('https://www.foxnews.com/transcript/tucker-the-us-is-looking-at-a-grim-economic-picture')\r\n\r\n\r\n\r\nGetting the Right Elements\r\nEvery HTML page is divided into sections by tags. If you\r\nwant to get deep into webscraping, it will be useful to be able to\r\nidentify some of those tags, because that’s how we’re going to select\r\nwhich elements from a HTML page we want to keep.\r\nFor instance, there is an HTML tag called <p>,\r\nwhich denotes paragraphs of text. To get a list of all elements with the\r\n<p> tag on this webpage we loaded, we run the\r\nfollowing line of code:\r\n\r\n\r\nparagraphs <- page |>\r\n  html_elements('p')\r\n\r\nparagraphs\r\n\r\n\r\n{xml_nodeset (5)}\r\n[1] <p class=\"copyright\">\\n      This material may not be published ...\r\n[2] <p data-v-a7f268cc>Fox News host gives his take on how the U.S. ...\r\n[3] <p class=\"speakable\"><i>This is a rush transcript from \"Tucker  ...\r\n[4] <p class=\"speakable\">TUCKER CARLSON, FOX NEWS CHANNEL HOST, TUC ...\r\n[5] <p class=\"copyright\">\\n        This material may not be publish ...\r\n\r\nThis gives us a list with all the raw HTML code. Notice that the 4th\r\nentry looks like it has the transcript text we want. To get the plain\r\ntext from that line of HTML code, we’ll use the html_text()\r\nfunction.\r\n\r\n\r\ntext <- html_text(paragraphs[[4]])\r\n\r\ntext\r\n\r\n\r\n[1] \"TUCKER CARLSON, FOX NEWS CHANNEL HOST, TUCKER CARLSON TONIGHT: Good evening. Welcome to TUCKER CARLSON TONIGHT. Happy Monday.Knowing what we know about our current leadership class, the one thing we can be absolutely certain of is they always go too far. They always get over their skis. They just can't help themselves. That's who they are.So, in February, when the Russian military invaded Ukraine, there was always out there the chance that the Biden administration would find a way to turn what was a regional tragedy into something bigger, like a historical global catastrophe. That was always possible.So even before Russian forces entered into Ukraine, the White House promised us that would not happen, quote: \\\"There is no intention or interest or desire by the President to send troops to Ukraine.\\\" That was the word from Biden's publicist from the White House. She said the very same thing almost identically the next month.The month after that, she said it again, quote: \\\"Joe Biden does not have the intention of sending U.S. troops to Ukraine,\\\" Jen Psaki solemnly pledged to the nation. Why did Jen Psaki keep saying the same thing over and over? Well, because it wasn't true. She had to repeat it because it was a lie. That's how lying works. It's not believable so you have to say it again and again and again.If you want if someone is lying, count the times they assert something. The more often they assert it, the less likely it is to be true, but ultimately the truth does come out and this weekend it began to.Joe Biden's top surrogate in the Congress -- that would be Senator Chris Coons of Delaware, a former Biden intern -- appeared on CBS News and said the opposite of what the White House has been telling us for months. Coons demanded that the Pentagon deploy American troops to Ukraine to fight Russian soldiers. Watch.(BEGIN VIDEO CLIP)MARGARET BRENNAN, CBS ANCHOR: In some public remarks this week, you said the country needs to talk about when it might be willing to send troops to Ukraine.SEN. CHRIS COONS (D-DE): If Vladimir Putin, who has shown us how brutal he can be, is allowed to just continue to massacre civilians, to commit war crimes throughout Ukraine, without NATO, without the West coming more forcefully to his aid, I deeply worry that what is going to happen next is that we will see Ukraine turn into Syria.The American people cannot turn away from this tragedy in Ukraine. I think the history of the 21st Century turns on how fiercely we defend freedom in Ukraine and that Putin will only stop when we stop him.(END VIDEO CLIP)CARLSON: \\\"Without the West coming more forcefully to the aid of Ukraine.\\\" So, the Ukrainian military has been trained by NATO. It uses American arms. In some cases, it's being led by Americans. The Ukrainian government is advised directly moment-to-moment by Americans. There are many Americans in Ukraine right now doing that.So, what Chris Coons is calling for is land war with Russia. Now, that's not a small thing, given that once again, Chris Coons is Joe Biden's closest ally in the Senate. Chris Coons is an unfailing and faithful spokesman for the administration's policies, whatever those policies happen to be.So, when Chris Coons calls for war with Russia, he does not do it accidentally and in this case, the White House has not distanced itself from what Chris Coons said. Oh, why? Well, because war with Russia is the administration's actual policy.Leaders of the Democratic Party want to topple the Russian government by force and effect regime change in Russia. They have wanted this since the day that Hillary Clinton lost the 2016 presidential race. We know this because they've said so many, many times and the only reason the rest of us missed it is because we didn't take them seriously enough, but we should have.A Hot War with Vladimir Putin, using American troops, is the logical, maybe inevitable end stage of Russiagate. So, the whole thing began with Hillary Clinton complaining, then the pee tape and now it's moving toward nuclear war.So, what would that look like? A Hot War with Russia? How many Americans would die during that war? How likely is it to escalate to nuclear conflict? And if we succeed, if we remove Vladimir Putin from office, who will replace him as the head of Russia in charge of 6,000 nuclear weapons?Those are some of the first questions that jump to mind when a war with Russia is discussed. But the news reader on CBS, who was interviewing Chris Coons didn't bother to ask any of those questions, nor did she ask the one thing you would need to know if you were running a functioning democracy, which is how many Americans, how many voters, actually want war with Russia? What percentage of the American population believes that Ukraine's borders are worth dying for?That's a central question in a democracy, and as it happens we know the answer, because CBS itself ran a poll on that topic just a week ago, and they asked this question: Should the United States send troops to Ukraine?Answer? Fully 75 percent of Americans said no, the United States should not send troops to Ukraine. And yet, somehow the CBS News reader forgot to mention any of this to Chris Coons. Why? Well, you know why, because in Washington, what you think is irrelevant.Our foreign policy matters of life and death, decisions that destroy nations, are made entirely by people with no skin in the game, people who face no conceivable risk of injury, people like John Bolton, and Max Boot, and Toria Nuland. Your opinion doesn't factor into the equation at any point.So, if you bothered to ask American citizens what they think -- and if you cared about democracy, you would -- they'd likely tell you that their borders are more important to them than Ukraine's borders are. That makes sense because they're Americans, not Ukrainians.And if you ask deeper, you would find out that at the very top of their concerns is not Ukraine. They feel sympathy for the people of Ukraine, but they're not taking up too much disk space brooding about Ukraine moment to moment, because they have other things to worry about, starting with their own economy, especially the cost of food, energy, and housing. They're worried about these things and they're right to be worried.At this point, the United States is looking at a grim economic picture.(BEGIN VIDEO CLIP)NBC REPORTER: Before you pull out that credit card today, beware. If you don't pay it off in full, your interest rate is probably about to jump.The Federal Reserve, the nation's Central Bank, is expected to raise rates by a quarter point today as it tries to throw cold water on runaway inflation.UNIDENTIFIED FEMALE: Every time you go and shop, every, like a price of something has gone up by so much. It's insane.NBC REPORTER: Every American shopper has seen it firsthand. Clothing up 6.5 percent, food up 8.5 percent, electricity up nine percent, used cars up 41 percent, and gasoline up nearly $1.50 from a year ago.UNIDENTIFIED FEMALE: Milk, flour, sugar, cartons of oil, like frying oil - - that's been insane. That's like tripled in price. So, that's been crazy.NBC REPORTER: The highest inflation in 40 years.(END VIDEO CLIP)CARLSON: Highest inflation in 40 years. Those are the official numbers, which of course, bear no resemblance to the day-to-day reality. Everything is much more expensive and that's especially true of the big things. The big things are the most expensive of all. Why is that? Simple, the declining power of U.S. currency has created an unprecedented asset bubble. That means investors around the world are rushing to convert increasingly worthless U.S. dollars into objects that might hold value over time.So, anything tangible costs a lot more, a lot more than it did a year ago. There's no mystery in this. This is exactly what happens when you pump too much money into an economy. The money becomes worth less.So, where is this going? How is it going to unfold? Nobody believes the interest rate rises we're seeing will get inflation under control quickly. So, what happens? Well, at some point consumers will begin to run out of cash to spend. Assets will become too expensive to buy and the average person will have less money to buy them.At the same time that prices are rising, so are taxes. Property taxes are rising in many places in tandem with the real estate bubble. So even if you didn't buy a new house, you will suffer because of that. State income taxes have risen dramatically in places like New York.So, they're getting it from both ends and that means that some people, maybe a lot of people, will start to go broke and as they do go broke, they'll be forced to curtail what they buy.In an economy driven largely by consumer spending, this is a very scary trend. When people stop buying things, the crash comes. So, you can see very clearly where this is going. Everyone in Washington understands exactly where it's going, but instead of taking real steps to fix it, like stop writing these massive spending bills, they're taking everything they can. All the money is still on the table while there still is money.Janet Yellen, the secretary of the Treasury, declared last week, for example, that quote: \\\"We must redouble our efforts to decarbonize our economy.\\\" So, what does that mean exactly? How do you decarbonize an economy? Well, by spending trillions in new stimulus spending on renewable energy schemes that, by the way, are owned by the Chinese government and Democratic donors.See how that works? You pass the cash around while it still exists. Another name for this is looting. It cannot go on forever, by definition, because if the economy tanks, everything resets, not just the economic questions. There could be genuine social and political volatility.Our current conversation can only happen in a country that still believes itself to be rich, but once the country doesn't think it is rich, everything changes. This tells you exactly why our leaders seem so jumpy.It's why they're more determined than ever to move the conversation away from economics, \\\"No talking about economics.\\\" And toward questions of race and obscure sexual politics.Every new moral panic they create -- and they create them by the dozens -- diverts attention away from themselves. They've been doing this for quite a while, since at least the financial crisis.Since that time -- 2008-2009 -- our leaders have been telling us over and over and over again, many books have been written about it, that the central divide in America, the seeping wound, the original sin, is race.Consider the timing. At exactly the moment the U.S. government bailed out Wall Street, not a popular move, use of the terms \\\"race\\\" and \\\"racism\\\" in \\\"The Washington Post,\\\" \\\"The New York Times,\\\" and \\\"U.S.A. Today\\\" jumped by more than 700 percent.So, the official message was really clear: You've got problems and White men caused those problems. The White guys are taking all the money and the perks for themselves and they're holding everyone else down.Now to this day, you hear that constantly, including from Joe Biden. It's the most divisive possible message. It's also, factually speaking, a lie. According to Federal statistics, White men aren't even close to the richest group in the United States. Indian-Americans, Chinese-Americans, Filipinos, Koreans, Indonesians, among others, all have much higher median household incomes than Whites.So, the story is not only destructive of the social fabric, it's not even true. The actual fault line in American life is not color, it is money.The real problem isn't racism, it is wealth distribution. A small number of people, smaller every year, have become richer than anyone else in history. Meanwhile, the rest of the country has stagnated and if you don't believe that, drive out 20 miles from the city center and see how people are doing.So, if the population understood this -- that effectively it's an economics game, it's got nothing to do with racism or transgenders -- the population would be pretty mad about that. Joe Biden and his donors fear that. They don't want you to think about economics. They prefer to keep you paralyzed by guilt and shame and if that doesn't work, they'd rather you worried about Ukraine. \\\"Ukraine is the real scandal where economic problems are Putin's fault.\\\" Watch Joe Biden.(BEGIN VIDEO CLIP)JOE BIDEN (D), PRESIDENT OF THE UNITED STATES: I'm doing everything within my power by executive orders to bring down the price and address the Putin price hike. In fact, we've already made progress since March inflation data was collected.Your family budget, your ability to fill up your tank, none of it should hinge on whether a dictator declares war and commits genocide and a half a world away.(END VIDEO CLIP)CARLSON: So this is their last desperate talking point and it's highly familiar to anyone who's watched American politics for the last six years, \\\"Putin did it.\\\" But it's ridiculous and we feel it's always a moral obligation to rebut it with facts. This chart clearly shows it.Here are several core measures of inflation, they go back to 2011. You notice that all of those measures began spiking in what year -- 2021, right after Joe Biden took office. It was the spending that did it. He's not the only one who spent too much, but he spent the most.Now, Biden could claim the spike started this February and the media would probably protect him, but things have gotten so obvious, economic decline being the one thing you really can't hide because people feel it every single day, that the media are beginning to stop defending him. Watch this.(BEGIN VIDEO CLIP)ED O'KEEFE, CBS NEWS CORRESPONDENT: The White House says those price jumps are happening because of the war in what they call, quote, \\\"Putin's price hike,\\\" But remember, prices started spiking well before the war in Ukraine began.JONATHAN LEMIRE, WHITE HOUSE BUREAU CHIEF, POLITICO: And every time we talk about gas prices, Democrats do, President Biden does as always \\\"Putin's price hike.\\\" They're trying to blame, of course, the Russian President and the invasion of Ukraine for the jump in prices, but of course, as polling suggests, this President is going to take a lot of the blame here.ABC NEW REPORTER: Biden has called it a \\\"Putin price hike,\\\" but most Americans aren't buying it.DON LEMON, CNN ANCHOR: Despite what President Biden says, inflation was a major concern way before Putin's invasion.(END VIDEO CLIP)CARLSON: So, no one's buying it because everyone understands the most simple principle in all economics, which is supply and demand. If you create a lot more money, the money loses its value, obviously, and yet this administration proposes spending even more money at a scale that this country has never seen.The founder of FedEx, Fred Smith, summed it up this way, quote: \\\"Had we passed the Build Back Better bill that Biden wanted, my guess is that we would be Weimar Germany right now. We'd have 25 percent inflation rather than nine percent or 10 percent.\\\"That's all very obvious, and again, you don't need to be running the Fed to understand it. Joe Biden may be the only person who doesn't understand. In this case, he gets a pass because in Joe Biden's head, he's far away, he's somewhere else.Yesterday, for example, Biden was fumbling through questions about foreign policy when a staffer in an Easter Bunny costume appeared out of nowhere and led him away. It's that bad now? How bad is it? Here is Biden in one of the saddest moments of his or any other presidency.This is from last week. Watch.(BEGIN VIDEO CLIP)BIDEN: There's not a single thing America can't do when we do it together as the United States of America. God bless you all.(END VIDEO CLIP)CARLSON: Shaking imaginary hands, wandering off into the great distance. So the one thing we can say with certainty about the White House in the spring of 2022 is that Joe Biden is not running it. Joe Biden is gone. In his place are unelected ideologues, people who do not care what the price of gas is. They don't care if you can afford dinner at Applebee's with your kids. They don't care even if war with Russia could cause the destruction of entire populations. Those things are not interesting to them. They care about theories, not reality.So, improving the life of Americans, elevating the American population, doesn't even rate on their scale of concern. It's a pure afterthought.Now we've known that for a while. They've been in charge for a while and that's been sustainable as long as everyone's getting a check, as long as everyone feels like \\\"I've got enough money,\\\" but the second the economy turns south, the calculation changes completely. Everything is different in a country that believes it is becoming poor. The short-term effect is a peaceful revolt by voting.So, you can be certain of this: If there are free and fair elections this November, neo liberalism will be swept away entirely, revealed as the joke it is. So, the thing to worry about right now is not public opinion. Public opinion is settled. Nobody likes this.The thing to worry about right now is voting, the mechanics of voting. If the public is allowed to express its preference in the midterms, we're done, but will they be allowed? That should be the thing people are paying attention to.So as the Biden administration tells us, once again that the territorial integrity of Ukraine is worth dying for, our country's territorial integrity is not only an afterthought, but a historical footnote. It doesn't exist.Drug cartels, human traffickers, even terrorists are walking into the United States of America totally unimpeded. FOX's Bill Melugin broke this story, as he has so many, he is live on the border for us tonight. Hey, Bill.BILL MELUGIN, FOX NEWS CHANNEL NATIONAL CORRESPONDENT: Tucker, good evening to you.I was able to obtain a C.B.P. record through a Freedom of Information Act request that reveals at least 23 possible terror linked individuals were stopped here at the U.S. Southern border last year. Take a look at this graphic. We'll get right into the numbers.What you're looking at are hits on the T.S.D.B. that is the Terrorist Screening Database of known and suspected terrorists maintained by the F.B.I. and what you see are the different Border Patrol sectors where these hits happened. Four in San Diego sector, four in El Centro, two in Yuma, two in Tucson, three in El Paso sector, four in Del Rio sector and four in the Rio Grande Valley sector, again totaling up to 23.But keep in mind, those are only the ones they caught, only the ones they know about, Tucker.Back out here live, that is a major concern because C.B.P. officials tell us in the last six months alone here at the border, there have been more than 300,000 known gotaways.I'll send it back to you.CARLSON: Amazing. Bill, as you've chronicled for months now, millions of foreign nationals come over, they don't stay in the Rio Grande Valley. There is not the economy or the services to support them. An awful lot of them wind up in Southern California, and when they get there, it is very easy to break the law and not get punished for it.You were instrumental in the documentary we just finished on Los Angeles, it started streaming today, and you have news to report relevant to that story for us tonight. What is it?MELUGIN: Yes, Tucker. That's right. So look, ever since LA DA George Gascon was elected, it's no secret that criminals have been celebrating his soft on crime policies, but never have we seen such a blatant example of this as what your viewers are about to see and hear right now.FOX News has obtained exclusive jailhouse audio of a convicted gang murderer named Luis Angel Hernandez boasting that he is going to get George Gascon's name tattooed on his face and he calls George Gascon a champ for dropping his gang and gun enhancements. Take a listen.(BEGIN AUDIO CLIP)LUIS ANGEL HERNANDEZ, CONVICTED GANG MURDERED: That [bleep] looking real good. Now, we've got a new DA in LA, so they're going to -- I got caught on the 14th, fool.Right there in Compton on Thursday, so, they're going to drop the gang or like gun enhancement, my gang enhancement. My gang enhancement is 10 years, fool, for being a gang member. And then, the gun in the commission of crime.UNIDENTIFIED MALE: Whatever (INAUDIBLE) Gascon or whatever the [bleep].HERNANDEZ: I'm going to get [bleep] name on my face. That's a champ right there. [Bleep] Gascon.UNIDENTIFIED MALE: (INAUDIBLE) for misdemeanor and stuff.HERNANDEZ: That's the [bleep] right there, bro. He is making historic changes for all of us, fool. You know, so, I am just grateful, fool, like I've got good news off that [bleep].So, at least now, I know like, they're' like, \\\"You're coming home, blood.\\\" Like, they are already told me, my lawyer told me, \\\"You're coming home.\\\"(END AUDIO CLIP)MELUGIN: And Tucker, what you heard right there certainly is not an isolated incident. Your documentary also has another murderer from behind his cell in prison toasting one of his cellmates with prison moonshine smiling, saying, \\\"Hey, we're going home on this George Gascon directive.\\\"Your documentary also has jailhouse audio of a 26-year-old transgender child molester boasting that nothing is going to happen to him and he's not going to face any prison time because George Gascon refused to prosecute him as an adult.There is no denying that George Gascon's policies treat criminals with kiddy gloves sometimes, and prosecutors in his office tell me when it comes to George Gascon, DA stands for Defense Attorney.We'll send it back to you.CARLSON: That is effectively true. Bill Melugin, a big part of that documentary. Thanks so much for all the help you gave us.So as we said, we are premiering Season Two of our series, \\\"Tucker Carlson Originals.\\\" The first one came out today and it's really the story of democracy subverted.So you take the second biggest city in the country, LA, and a tiny group of people back, a truly radical prosecutor called George Gascon, the Black Lives Matter people, the defund the police people, airhead celebrities, George Soros -- this guy takes over and one man changes life for everybody in Los Angeles. It's beyond belief.The documentary called \\\"Suicide of LA.\\\" Part one is available right now. Here's a portion of it.(BEGIN VIDEO CLIP)CARLSON: So who exactly supports this? How could a sane person vote for a maniac like George Gascon who is so clearly intent on destroying what previous generations have built?MELUGIN: George Gascon's primary supporters were Black Lives Matter crowd.PATRISSE CULLORS, FOUNDER, BLACK LIVES MATTER: Why has this been called the second most important race in the country next to the Presidency?MELUGIN: George Gascon is supported by the crowd that wants to defund law enforcement.UNIDENTIFIED MALE: Defund the police.GEORGE GASCON, LOS ANGELES COUNTY DISTRICT ATTORNEY: This is really about moving funding around.MELUGIN: He is supported by a large swath of the celebrity crowd.GASCON: Please join me in welcoming the one and only, John Legend.SOPHIA BUSH, ACTRESS: The DA's races are so, so critical to how our cities function and especially here in LA.MELUGIN: And of course, George Soros who has been bankrolling progressive DAs all across the entire country.CARLSON: George Soros donated more than $2 million to George Gascon's campaign, but he wasn't alone. The CEO of Netflix, Reed Hastings and his wife Patty Quillin donated $2.1 million.ALEX VILLANUEVA, LOS ANGELES COUNTY SHERIFF: Oh, yes, this is all funded by Soros and company, and Reed Hastings, all the billionaires up in the Bay Area.CARLSON: The only significant elected official in Los Angeles who opposes George Gascon is the County Sheriff, Alex Villanueva.VILLANUEVA: The problem is here in LA, in city and county government, they occupy every single seat. There is no other point of view other than that woke ideology.You have to operate in the real world, not their fake fantasy. From 2019 to 2021, we saw 94 percent increase in homicides, which is a mind boggling increase. I can't get the Board of Supervisors to even admit that homicides have gone up 95 percent.MELUGIN: I think the understatement of the year would be to say that there is no love lost between George Gascon and LA County Sheriff, Alex Villanueva. They are polar opposites.VILLANUEVA: My lack of relationship with the DA is unprecedented. I've had one phone call with him since he has taken office. That is it.UNIDENTIFIED MALE: Let's start with the rise in crime in LA County. You are the Sheriff. Do you bear any responsibility for that?VILLANUEVA: Well, as I am being defunded and being discredited and delegitimized by our elected officials, it is kind of hard to put the blame on the people who is doing the most of the work.For six months, with Gascon's time in office, he rejected 5,932 cases. That means all those people just walk free.(END VIDEO CLIP)CARLSON: \\\"Suicide of Los Angeles\\\" sadly not an overstatement. Available now on FOX nation. Part Two out tomorrow. You can get a free account on tuckercarlson.com and use it at FOX Nation.So you may have noticed, it's hard to miss it, unequal justice under the law is now the rule. Your punishment depends on your political views.So January 6 defendants been rotting in jail for over a year without bail for nonviolent crimes, but actual criminals like the guy who shot nine people in a South Carolina mall over the weekend, get released on bond for 25 grand. That's the system. Still, straight ahead.(COMMERCIAL BREAK)CARLSON: So the whole point of America is that American laws are applied evenly and fairly to American citizens. They are not even trying anymore to do that.After January 6, the Biden administration placed dozens of its political opponents in solitary for months and months and months, nonviolent protesters who didn't hurt anyone.Meanwhile, actual violent offenders like a guy who just shot nine people in a shopping mall in South Carolina over the weekend are already out of jail. Watch.(BEGIN VIDEO CLIP)UNIDENTIFIED MALE: In Colombia, more than a dozen people injured after gunfire erupted Saturday at a shopping mall. Police believe a dispute among at least three people led to the violence.They charged 22-year-old Jewayne Price with unlawful possession of a gun. He was released on house arrest after a Judge set bond at just $25,000.00 allowing him to go to work with an ankle monitor.(END VIDEO CLIP)CARLSON: The Judge's name in that case is Crystal Rookard by the way. That's her.Buck Sexton is one of the few former C.I.A. officers we trust, our friend, host of \\\"The Clay Travis and Buck Sexton Show,\\\" he joins us now.Buck, thanks so much for coming on. It does seem like the core promise of equal justice has been corroded.BUCK SEXTON, FORMER C.I.A. OFFICER: The Democratic Party has been using prosecutors' offices, not just for social engineering, or you could say, for replacing criminal justice with social justice, but also as a weapon.I mean, the most prominent recent example of this is January 6th, where you have people, let's remind ourselves, they're held because they're either supposed to be a threat to the public, which I don't think any person actually believes, is based in any reality when you're talking about January 6 or a flight risk, which also seems quite strange, considering many of them have already pleaded and they've gotten months, one just actually got off entirely because he was waved into the Capitol.But look at the way the D.O.J. was weaponized against Donald Trump before that. In fact, I would say Republicans have gotten far too used to this whether it's Governor Scott Walker up in Wisconsin or the John Doe Law is being used to go after him, Chris Christie in Bridge-gate. They wanted to lock up Governor McDonnell, former Governor McDonnell of Virginia, his wife for taking gifts if you all remember that at one point. They wanted to go after a Rick Perry when he was the Governor of Texas.These are all prosecutions, by the way that were either in part or in total dramatic overreach, and it always seems to go one way. Where is the equivalent on the other side? That was just off the top of my head. Meanwhile, people who shoot a lot of people get let out on bail because they're not a danger.CARLSON: Twenty five grand for shooting. Nine people are shot in a shopping mall and there is only a gun charge and 25 grand and he's out. I mean, how can that be?SEXTON: We've seen the progressive prosecutors have taken the opinion in city after city at this point, that if they're only honestly softer, more gentle on violent crime across the board, let them out sooner from either a sentence, let them out right away with catch and release bail reform laws, that somehow this will improve the community police relations that we've seen talked about a bit more now in the aftermath of the BLM protests and riots. That hasn't worked at all.They have to admit now across the country that the crime rate has gone up dramatically in pretty much every major city you can think of, not because of COVID, which is one of the lies they told not, because we were cooking the books and trying to create hysteria on TV, but because they have really stupid policies that are reckless and made things worse for everyone.And Tucker, I think they only care about it, which is obvious to everybody now, because crime is a major issue in city after city and they're going into a midterm election and they have people like Gascon and others by the way, Krasner in Philadelphia, Boudin in San Francisco, who just look like lunatics while good people are suffering. They're all Democrats.CARLSON: That's exactly right. Nicely put, the Great Buck Sexton, thank you.SEXTON: Thanks, Tucker.CARLSON: Good to see you.So, the Democratic Party is bracing for total wipeout in the midterms. At this point, they're not even trying to convince you to vote for them, they are trying to take their opponents off the ballot. They're attacking democracy, not letting you vote for people you want to vote for.Here is the latest and most shocking example, a new legal effort underway to keep Marjorie Taylor Greene off the ballot in her own district where she is very popular in the State of Georgia. On what grounds? The left is saying she is guilty of quote, \\\"insurrection.\\\"Marjorie Taylor Greene represents Georgia in this Congress, and she joins us tonight.Congresswoman, thanks so much for coming on. So, they're trying to prevent voters from voting for you. How is that democracy?REP. MARJORIE TAYLOR GREENE (R-GA): Well, it's not, Tucker. That's the thing. These people hate the people in my district so much, they look down on them, because they voted for me and sent me to Washington to fight for the things that most Americans care about, like secure borders, stopping abortion, protecting our Second Amendment, stopping the out of control spending in Washington, and stop funding, never-ending foreign wars and all the insanity that takes place in Washington.Well, I went there and I have been fighting it. And now the progressives, the people that donate, the dark money groups, you know, the 501(c) (3) and the foundations, they've hired up some attorneys from New York who hate the people in my district, and don't believe that they should have the right to elect who they want to send to Washington, which is me. I have overwhelming support in my district, and I'm so thankful for all of them.Well, now they filed a lawsuit because they're trying to rip my name off of the ballot and steal my district's ability to re-elect me and send me back to Congress.CARLSON: So if you can prevent voters from being allowed to vote for the candidate of their choice, which is their constitutional right, then the system is over. Is the Republican Party with all four paws jumping in to help you?GREENE: Not yet. I'm on my own to defend myself. Wonderful people are donating to my campaign, MTGforamerica.com, and I'm so grateful for that. But I have to protect myself. I have to go to Court on Friday and actually be questioned about something I've never been charged with and something I was completely against.And so this is how far it's going, these leftists, these progressives who would rather want -- they'd rather have the judge or bureaucrats making decisions instead of voters, they want to hand that over to them and not let the people in my district to even have the right to vote for me.But no, the Republican Party needs to fight harder, Tucker. You know, there is something that I have learned and I think this is really important. You know, if you can challenge any representative's candidacy or elected office holder, then I bet you we could round up some Republican voters who didn't like Kamala Harris funding rioters, criminal rioters out of jail, or Ilhan Omar or Cori Bush or Maxine Waters inciting riots.You know, I think there is another way to play this game.CARLSON: Well, of course. American citizens have an absolute right to vote for anyone they want to because it is their government, it is self- government, and if you take that away, it's tyranny, obviously.We appreciate your coming on tonight, Congresswoman Marjorie Taylor Greene, of Georgia. Thank you.GREENE: Thank you, Tucker.CARLSON: Pretty shocking medical mystery in the State of New Jersey. Nearly a hundred students who went to a high school nearby have developed the same extremely rare and deadly tumors. Why?Dr. Marc Siegel after the break.(COMMERCIAL BREAK)CARLSON: FOX News Alert: The mask mandate on airlines is dead. Masks are now optional on the biggest U.S. carriers that includes American, Delta, United, Alaska, Southwest. The T.S.A. has also announced it is not going to enforce the mandate anymore. You're not going to get arrested, passengers cheered on many flights when they heard this news in midair.This comes because a Florida Judge has ruled the Biden administration doesn't get to make up the rules because they're not God. This isn't a monarchy, they can't arrest you if you don't comply with their fake rules.Here's one quote from the Judge, quote: \\\"The power to constitutionally release and detain is limited to individuals entering the U.S. from a foreign country.\\\" In other words, you don't get to treat American citizens like an invading army, even as you treat the invaders like they're your own children, which is exactly what they're doing.Anyway, no more mask mandate on planes. Amen.We have been focused on COVID for two years because it's helped a lot of politicians get more powerful, but there are real public health emergencies going on in this country.One of them apparently is unfolding in New Jersey. Dozens of people connected to a single high school in Woodbridge Township are coming down with rare brain tumors. Here's a local news report on it.(BEGIN VIDEO CLIP)UNIDENTIFIED FEMALE: Al Lupiano, an environmental scientist and former resident of Woodbridge Township says he has confirmed 65 cases of people with rare brain tumors.The common denominator, they were all Colonia High School graduates or had worked there.Lupiano was diagnosed 20 years ago and still suffers lingering issues.AL LUPIANO, ENVIRONMENTAL SCIENTIST: Fast forward to August of last year, my sister received the news that she had a primary brain tumor herself, it unfortunately turned out to be stage four glioblastoma. Two hours later, we received information that my wife also had a primary brain tumor.UNIDENTIFIED FEMALE: After his sister sadly passed away less than a month ago, he posted on Facebook calling on all Colonia High School alumni asking if others had brain tumors, and the response was shocking.(END VIDEO CLIP)CARLSON: Shocking is right and scary. What is this exactly? Dr. Marc Siegel joins us tonight to assess -- Doctor.DR. MARC SIEGEL, FOX NEWS CHANNEL MEDICAL CONTRIBUTOR: Tucker, I want to start with a little bit of history. You know, the Middlesex Sampling Plant, which is less than 10 miles away from this high school is part of -- was part of the Manhattan Project that made the first atomic bomb and they had uranium there and they didn't close it until 1967, the same year that Colonia High School opened.And not only that, but reports were that they didn't fully decontaminate that Middlesex Plant until more than 10 years, more than 20 years later, until the 1990s, it wasn't fully decontaminated.And the question is, was there an association between the uranium in that plant and the soil and the radiation in what happened in this high school.Now, let's talk about this high school. Six out of 100,000 people a year get any kind of brain or spinal cord tumor. Very, very rare, but this high school has seen over a hundred brain tumors from 1975 to 2000.You saw Al Lupiano there, he has an acoustic neuroma which is very, very rare. His sister died of a glioblastoma -- terrible, terrible brain tumor.I spoke to our head of Neurosurgery, John Golfinos at NYU who is an expert at this, and also Tom Roland who is an acoustic neuroma expert at NYU, one of the world's best. Both say that a little bit of ionizing radiation, just a small amount in the area is enough to provoke these tumors.Now, we don't know for sure that this is what has gone on, but the Environmental Protection Agency is involved, and the local Department of Health in New Jersey is involved.Tucker, this is what we call a cluster and there are 1,300 students in this school today all wanting to know, do I have risk? Is there radiation here? But you know what I want to add tonight, not just the part about the Manhattan Project, not about just how we dispose of radiation and the issues of our environment, but we need to look beyond the school. We need to look at the entire area around this plant and see and test for radiation in the air and in the soil, Tucker, because people can be hurt and we need to know.CARLSON: Well, that's exactly right. Environmental poisoning is real. It's not just about climate change and COVID. There is a lot going on, and we should pay attention. I appreciate that report.Dr. Marc Siegel, thank you.SIEGEL: Thanks, Tucker.CARLSON: So you've probably eaten a Jimmy John's sandwich, they are all over the country, Jimmy John Liautaud made it. He made millions making sandwiches starting when he was just 19. He never took on debt. Amazing story. Amazing, man. We'll meet him next.(COMMERCIAL BREAK)CARLSON: You've probably had a Jimmy John's sandwich over the years, but if you're like us, you didn't know who Jimmy John was, Jimmy John is Jimmy John Liautaud, he sold most of his company and made billions. He is an amazing person, and politically aware, too.At one point, he came out against Barack Obama and paid the price for that. So we sat down with him for an hour for a new episode of \\\"Tucker Carlson Today\\\" and learned a ton, but mostly we were amused, inspired, engaged. Amazing guy. Here is part of it.(BEGIN VIDEO CLIP)JIMMY JOHN LIAUTAUD, FOUNDER AND FORMER CHAIRMAN OF JIMMY JOHN'S SANDWICH CHAIN: After Obama won his second term, I was completely annihilated and canceled.I'm a hunter, I'm overweight, I'm very, very successful and rich, right? So I get [bleep] beat out of me for all those three things, completely canceled before cancel was cool.But why I want to talk about this is, I had 2,800 restaurants open like 3,000 sold that I had deposits on to open from my operators, and these are mom and pops across America, and they changed the labor law, the definition of what a manager is, minimum wage, insurance, and all of a sudden, I had 3,000 restaurants, each one employing 40 or 50 people, right, and 3,000 restaurants where they just, you know, they just -- virtually, the business just stopped.My mom and pop franchisees were being sued by law firms that lobbied the government, right, for labor from us taking advantage of our people and just became this arduous completely complex -- our mom and pops, you have three -- the average owner had like three and a half stores.I'm incredibly successful, I'm grateful. But what happened was a travesty because it made them -- they were taking all their extra time baking cookies for people that kicked ass and sampling baseball games and building their catering businesses and learning how to run business, and we taught them how to run these businesses.And when we did it with them, and then when the labor laws changed in and middle of the game , we were just sitting there holding the bag, and these mom and pops are now in Court. And then the laws change even further and we became a co-employer, meaning I'm responsible for all the employees in all these sandwich shops all around America so that the big, big law firms could then sue so they could get their money.So I had no idea -- here I was helping Mitt Romney for America, for everybody to have the dream, and I had no idea that I was going to be attacked, and it got so -- anyway, I spend all my time in Court and I did sell the business and I'm happy I did.But this is interesting as well. I've hired a company to go back and do the forensics on where all the money came from and all the attacks, and all the canceling on me, right, it all came from politics, all from the Democratic Party, all supported by them, or their PACs or Soros or whatever it is.And anyway --(END VIDEO CLIP)CARLSON: The guy owned a sandwich shop and he dared to contribute to the wrong candidate and that's what happened to him.But the whole thing is that amazing conversation. What a guy. Jimmy John Liautaud.We're getting up earlier now so you can watch our interviews on FOX Nation, \\\"Tucker Carlson Today\\\" starting at 7:00 AM.Now, we've heard a lot -- we spent a lot of time on the documentary series and we have news to share about that, next.(COMMERCIAL BREAK)CARLSON: So how do you watch the documentaries we make? Our friend, the bestselling author, Shannon Bream is here to tell us.SHANNON BREAM, FOX NEWS CHANNEL CHIEF LEGAL CORRESPONDENT: All right, Tucker. There has been a lot of excitement for Season Two of \\\"Tucker Carlson Originals,\\\" which we got a sneak peek on Friday. I am definitely interested.It is much-watch TV, episodes like \\\"Suicide of Los Angeles,\\\" \\\"Transgressive: The Cult of Confusion,\\\" The Life of a Rock Star: Kid Rock,\\\" and of course everyone is talking about \\\"The End of Men,\\\" some very interesting things in that one.We were flooded with calls and e-mails at FOX this weekend. Everybody wants to know how to watch. I've been confused.But the good news is it's easy. You don't even need to pay for it. Go to tuckercarlson.com. On Tucker's website, you will see something that says free FOX Nation. We want you to click there. Enter your e-mail address and follow the instructions.You can then use your new free account and use it on a phone or tablet. Many say I'd rather watch this on TV, no problem. If you don't have the app on your TV or a TV that doesn't have apps, pick up something like a Roku. It's cheap. It's at a place like Walmart, Target, or Best Buy.Again, it's called Roku, it's about 30 bucks. I am not getting a kickback. I just want you to know.So to recap go to tuckercarlson.com, get the free account, then use it on your phone, iPad or TV. It is that easy.And Tucker, I will not lie, it has been hard for me to sign up, but now I've got it.Content and Programming Copyright 2022 Fox News Network, LLC. ALL RIGHTS RESERVED. Copyright 2022 VIQ Media Transcription, Inc. All materials herein are protected by United States copyright law and may not be reproduced, distributed, transmitted, displayed, published or broadcast without the prior written permission of VIQ Media Transcription, Inc. You may not alter or remove any trademark, copyright or other notice from copies of the content.\"\r\n\r\nSelectorGadget\r\nIf you, like me, do not have a deep knowledge of HTML tags and CSS\r\nselectors that you can deploy to find the right element on a page, then\r\nthe SelectorGadget\r\ncomes in handy! This is an in-browser tool that was developed alongside\r\nthe rvest package, which allows you to visit the page\r\nyou’re scraping and determine what input to html_elements()\r\nwill get you the section of the page you want.\r\n\r\n\r\n\r\nFigure 1: Keep the text you want, leave out the Amazon ads\r\nand whatever Bette Midler tweeted about the baby formula shortage.\r\n\r\n\r\n\r\nIf we use the SelectorGadget on our webpage here, highlighting the\r\nelements we want in green and the elements we don’t want in\r\nred, it tells us to use the selector\r\n.speakable:nth-child(6). Here’s what that complete pipeline\r\nlooks like:\r\n\r\n\r\ntext <- page |> \r\n  html_elements('.speakable:nth-child(6)') |> \r\n  html_text()\r\n  \r\ntext\r\n\r\n\r\n[1] \"TUCKER CARLSON, FOX NEWS CHANNEL HOST, TUCKER CARLSON TONIGHT: Good evening. Welcome to TUCKER CARLSON TONIGHT. Happy Monday.Knowing what we know about our current leadership class, the one thing we can be absolutely certain of is they always go too far. They always get over their skis. They just can't help themselves. That's who they are.So, in February, when the Russian military invaded Ukraine, there was always out there the chance that the Biden administration would find a way to turn what was a regional tragedy into something bigger, like a historical global catastrophe. That was always possible.So even before Russian forces entered into Ukraine, the White House promised us that would not happen, quote: \\\"There is no intention or interest or desire by the President to send troops to Ukraine.\\\" That was the word from Biden's publicist from the White House. She said the very same thing almost identically the next month.The month after that, she said it again, quote: \\\"Joe Biden does not have the intention of sending U.S. troops to Ukraine,\\\" Jen Psaki solemnly pledged to the nation. Why did Jen Psaki keep saying the same thing over and over? Well, because it wasn't true. She had to repeat it because it was a lie. That's how lying works. It's not believable so you have to say it again and again and again.If you want if someone is lying, count the times they assert something. The more often they assert it, the less likely it is to be true, but ultimately the truth does come out and this weekend it began to.Joe Biden's top surrogate in the Congress -- that would be Senator Chris Coons of Delaware, a former Biden intern -- appeared on CBS News and said the opposite of what the White House has been telling us for months. Coons demanded that the Pentagon deploy American troops to Ukraine to fight Russian soldiers. Watch.(BEGIN VIDEO CLIP)MARGARET BRENNAN, CBS ANCHOR: In some public remarks this week, you said the country needs to talk about when it might be willing to send troops to Ukraine.SEN. CHRIS COONS (D-DE): If Vladimir Putin, who has shown us how brutal he can be, is allowed to just continue to massacre civilians, to commit war crimes throughout Ukraine, without NATO, without the West coming more forcefully to his aid, I deeply worry that what is going to happen next is that we will see Ukraine turn into Syria.The American people cannot turn away from this tragedy in Ukraine. I think the history of the 21st Century turns on how fiercely we defend freedom in Ukraine and that Putin will only stop when we stop him.(END VIDEO CLIP)CARLSON: \\\"Without the West coming more forcefully to the aid of Ukraine.\\\" So, the Ukrainian military has been trained by NATO. It uses American arms. In some cases, it's being led by Americans. The Ukrainian government is advised directly moment-to-moment by Americans. There are many Americans in Ukraine right now doing that.So, what Chris Coons is calling for is land war with Russia. Now, that's not a small thing, given that once again, Chris Coons is Joe Biden's closest ally in the Senate. Chris Coons is an unfailing and faithful spokesman for the administration's policies, whatever those policies happen to be.So, when Chris Coons calls for war with Russia, he does not do it accidentally and in this case, the White House has not distanced itself from what Chris Coons said. Oh, why? Well, because war with Russia is the administration's actual policy.Leaders of the Democratic Party want to topple the Russian government by force and effect regime change in Russia. They have wanted this since the day that Hillary Clinton lost the 2016 presidential race. We know this because they've said so many, many times and the only reason the rest of us missed it is because we didn't take them seriously enough, but we should have.A Hot War with Vladimir Putin, using American troops, is the logical, maybe inevitable end stage of Russiagate. So, the whole thing began with Hillary Clinton complaining, then the pee tape and now it's moving toward nuclear war.So, what would that look like? A Hot War with Russia? How many Americans would die during that war? How likely is it to escalate to nuclear conflict? And if we succeed, if we remove Vladimir Putin from office, who will replace him as the head of Russia in charge of 6,000 nuclear weapons?Those are some of the first questions that jump to mind when a war with Russia is discussed. But the news reader on CBS, who was interviewing Chris Coons didn't bother to ask any of those questions, nor did she ask the one thing you would need to know if you were running a functioning democracy, which is how many Americans, how many voters, actually want war with Russia? What percentage of the American population believes that Ukraine's borders are worth dying for?That's a central question in a democracy, and as it happens we know the answer, because CBS itself ran a poll on that topic just a week ago, and they asked this question: Should the United States send troops to Ukraine?Answer? Fully 75 percent of Americans said no, the United States should not send troops to Ukraine. And yet, somehow the CBS News reader forgot to mention any of this to Chris Coons. Why? Well, you know why, because in Washington, what you think is irrelevant.Our foreign policy matters of life and death, decisions that destroy nations, are made entirely by people with no skin in the game, people who face no conceivable risk of injury, people like John Bolton, and Max Boot, and Toria Nuland. Your opinion doesn't factor into the equation at any point.So, if you bothered to ask American citizens what they think -- and if you cared about democracy, you would -- they'd likely tell you that their borders are more important to them than Ukraine's borders are. That makes sense because they're Americans, not Ukrainians.And if you ask deeper, you would find out that at the very top of their concerns is not Ukraine. They feel sympathy for the people of Ukraine, but they're not taking up too much disk space brooding about Ukraine moment to moment, because they have other things to worry about, starting with their own economy, especially the cost of food, energy, and housing. They're worried about these things and they're right to be worried.At this point, the United States is looking at a grim economic picture.(BEGIN VIDEO CLIP)NBC REPORTER: Before you pull out that credit card today, beware. If you don't pay it off in full, your interest rate is probably about to jump.The Federal Reserve, the nation's Central Bank, is expected to raise rates by a quarter point today as it tries to throw cold water on runaway inflation.UNIDENTIFIED FEMALE: Every time you go and shop, every, like a price of something has gone up by so much. It's insane.NBC REPORTER: Every American shopper has seen it firsthand. Clothing up 6.5 percent, food up 8.5 percent, electricity up nine percent, used cars up 41 percent, and gasoline up nearly $1.50 from a year ago.UNIDENTIFIED FEMALE: Milk, flour, sugar, cartons of oil, like frying oil - - that's been insane. That's like tripled in price. So, that's been crazy.NBC REPORTER: The highest inflation in 40 years.(END VIDEO CLIP)CARLSON: Highest inflation in 40 years. Those are the official numbers, which of course, bear no resemblance to the day-to-day reality. Everything is much more expensive and that's especially true of the big things. The big things are the most expensive of all. Why is that? Simple, the declining power of U.S. currency has created an unprecedented asset bubble. That means investors around the world are rushing to convert increasingly worthless U.S. dollars into objects that might hold value over time.So, anything tangible costs a lot more, a lot more than it did a year ago. There's no mystery in this. This is exactly what happens when you pump too much money into an economy. The money becomes worth less.So, where is this going? How is it going to unfold? Nobody believes the interest rate rises we're seeing will get inflation under control quickly. So, what happens? Well, at some point consumers will begin to run out of cash to spend. Assets will become too expensive to buy and the average person will have less money to buy them.At the same time that prices are rising, so are taxes. Property taxes are rising in many places in tandem with the real estate bubble. So even if you didn't buy a new house, you will suffer because of that. State income taxes have risen dramatically in places like New York.So, they're getting it from both ends and that means that some people, maybe a lot of people, will start to go broke and as they do go broke, they'll be forced to curtail what they buy.In an economy driven largely by consumer spending, this is a very scary trend. When people stop buying things, the crash comes. So, you can see very clearly where this is going. Everyone in Washington understands exactly where it's going, but instead of taking real steps to fix it, like stop writing these massive spending bills, they're taking everything they can. All the money is still on the table while there still is money.Janet Yellen, the secretary of the Treasury, declared last week, for example, that quote: \\\"We must redouble our efforts to decarbonize our economy.\\\" So, what does that mean exactly? How do you decarbonize an economy? Well, by spending trillions in new stimulus spending on renewable energy schemes that, by the way, are owned by the Chinese government and Democratic donors.See how that works? You pass the cash around while it still exists. Another name for this is looting. It cannot go on forever, by definition, because if the economy tanks, everything resets, not just the economic questions. There could be genuine social and political volatility.Our current conversation can only happen in a country that still believes itself to be rich, but once the country doesn't think it is rich, everything changes. This tells you exactly why our leaders seem so jumpy.It's why they're more determined than ever to move the conversation away from economics, \\\"No talking about economics.\\\" And toward questions of race and obscure sexual politics.Every new moral panic they create -- and they create them by the dozens -- diverts attention away from themselves. They've been doing this for quite a while, since at least the financial crisis.Since that time -- 2008-2009 -- our leaders have been telling us over and over and over again, many books have been written about it, that the central divide in America, the seeping wound, the original sin, is race.Consider the timing. At exactly the moment the U.S. government bailed out Wall Street, not a popular move, use of the terms \\\"race\\\" and \\\"racism\\\" in \\\"The Washington Post,\\\" \\\"The New York Times,\\\" and \\\"U.S.A. Today\\\" jumped by more than 700 percent.So, the official message was really clear: You've got problems and White men caused those problems. The White guys are taking all the money and the perks for themselves and they're holding everyone else down.Now to this day, you hear that constantly, including from Joe Biden. It's the most divisive possible message. It's also, factually speaking, a lie. According to Federal statistics, White men aren't even close to the richest group in the United States. Indian-Americans, Chinese-Americans, Filipinos, Koreans, Indonesians, among others, all have much higher median household incomes than Whites.So, the story is not only destructive of the social fabric, it's not even true. The actual fault line in American life is not color, it is money.The real problem isn't racism, it is wealth distribution. A small number of people, smaller every year, have become richer than anyone else in history. Meanwhile, the rest of the country has stagnated and if you don't believe that, drive out 20 miles from the city center and see how people are doing.So, if the population understood this -- that effectively it's an economics game, it's got nothing to do with racism or transgenders -- the population would be pretty mad about that. Joe Biden and his donors fear that. They don't want you to think about economics. They prefer to keep you paralyzed by guilt and shame and if that doesn't work, they'd rather you worried about Ukraine. \\\"Ukraine is the real scandal where economic problems are Putin's fault.\\\" Watch Joe Biden.(BEGIN VIDEO CLIP)JOE BIDEN (D), PRESIDENT OF THE UNITED STATES: I'm doing everything within my power by executive orders to bring down the price and address the Putin price hike. In fact, we've already made progress since March inflation data was collected.Your family budget, your ability to fill up your tank, none of it should hinge on whether a dictator declares war and commits genocide and a half a world away.(END VIDEO CLIP)CARLSON: So this is their last desperate talking point and it's highly familiar to anyone who's watched American politics for the last six years, \\\"Putin did it.\\\" But it's ridiculous and we feel it's always a moral obligation to rebut it with facts. This chart clearly shows it.Here are several core measures of inflation, they go back to 2011. You notice that all of those measures began spiking in what year -- 2021, right after Joe Biden took office. It was the spending that did it. He's not the only one who spent too much, but he spent the most.Now, Biden could claim the spike started this February and the media would probably protect him, but things have gotten so obvious, economic decline being the one thing you really can't hide because people feel it every single day, that the media are beginning to stop defending him. Watch this.(BEGIN VIDEO CLIP)ED O'KEEFE, CBS NEWS CORRESPONDENT: The White House says those price jumps are happening because of the war in what they call, quote, \\\"Putin's price hike,\\\" But remember, prices started spiking well before the war in Ukraine began.JONATHAN LEMIRE, WHITE HOUSE BUREAU CHIEF, POLITICO: And every time we talk about gas prices, Democrats do, President Biden does as always \\\"Putin's price hike.\\\" They're trying to blame, of course, the Russian President and the invasion of Ukraine for the jump in prices, but of course, as polling suggests, this President is going to take a lot of the blame here.ABC NEW REPORTER: Biden has called it a \\\"Putin price hike,\\\" but most Americans aren't buying it.DON LEMON, CNN ANCHOR: Despite what President Biden says, inflation was a major concern way before Putin's invasion.(END VIDEO CLIP)CARLSON: So, no one's buying it because everyone understands the most simple principle in all economics, which is supply and demand. If you create a lot more money, the money loses its value, obviously, and yet this administration proposes spending even more money at a scale that this country has never seen.The founder of FedEx, Fred Smith, summed it up this way, quote: \\\"Had we passed the Build Back Better bill that Biden wanted, my guess is that we would be Weimar Germany right now. We'd have 25 percent inflation rather than nine percent or 10 percent.\\\"That's all very obvious, and again, you don't need to be running the Fed to understand it. Joe Biden may be the only person who doesn't understand. In this case, he gets a pass because in Joe Biden's head, he's far away, he's somewhere else.Yesterday, for example, Biden was fumbling through questions about foreign policy when a staffer in an Easter Bunny costume appeared out of nowhere and led him away. It's that bad now? How bad is it? Here is Biden in one of the saddest moments of his or any other presidency.This is from last week. Watch.(BEGIN VIDEO CLIP)BIDEN: There's not a single thing America can't do when we do it together as the United States of America. God bless you all.(END VIDEO CLIP)CARLSON: Shaking imaginary hands, wandering off into the great distance. So the one thing we can say with certainty about the White House in the spring of 2022 is that Joe Biden is not running it. Joe Biden is gone. In his place are unelected ideologues, people who do not care what the price of gas is. They don't care if you can afford dinner at Applebee's with your kids. They don't care even if war with Russia could cause the destruction of entire populations. Those things are not interesting to them. They care about theories, not reality.So, improving the life of Americans, elevating the American population, doesn't even rate on their scale of concern. It's a pure afterthought.Now we've known that for a while. They've been in charge for a while and that's been sustainable as long as everyone's getting a check, as long as everyone feels like \\\"I've got enough money,\\\" but the second the economy turns south, the calculation changes completely. Everything is different in a country that believes it is becoming poor. The short-term effect is a peaceful revolt by voting.So, you can be certain of this: If there are free and fair elections this November, neo liberalism will be swept away entirely, revealed as the joke it is. So, the thing to worry about right now is not public opinion. Public opinion is settled. Nobody likes this.The thing to worry about right now is voting, the mechanics of voting. If the public is allowed to express its preference in the midterms, we're done, but will they be allowed? That should be the thing people are paying attention to.So as the Biden administration tells us, once again that the territorial integrity of Ukraine is worth dying for, our country's territorial integrity is not only an afterthought, but a historical footnote. It doesn't exist.Drug cartels, human traffickers, even terrorists are walking into the United States of America totally unimpeded. FOX's Bill Melugin broke this story, as he has so many, he is live on the border for us tonight. Hey, Bill.BILL MELUGIN, FOX NEWS CHANNEL NATIONAL CORRESPONDENT: Tucker, good evening to you.I was able to obtain a C.B.P. record through a Freedom of Information Act request that reveals at least 23 possible terror linked individuals were stopped here at the U.S. Southern border last year. Take a look at this graphic. We'll get right into the numbers.What you're looking at are hits on the T.S.D.B. that is the Terrorist Screening Database of known and suspected terrorists maintained by the F.B.I. and what you see are the different Border Patrol sectors where these hits happened. Four in San Diego sector, four in El Centro, two in Yuma, two in Tucson, three in El Paso sector, four in Del Rio sector and four in the Rio Grande Valley sector, again totaling up to 23.But keep in mind, those are only the ones they caught, only the ones they know about, Tucker.Back out here live, that is a major concern because C.B.P. officials tell us in the last six months alone here at the border, there have been more than 300,000 known gotaways.I'll send it back to you.CARLSON: Amazing. Bill, as you've chronicled for months now, millions of foreign nationals come over, they don't stay in the Rio Grande Valley. There is not the economy or the services to support them. An awful lot of them wind up in Southern California, and when they get there, it is very easy to break the law and not get punished for it.You were instrumental in the documentary we just finished on Los Angeles, it started streaming today, and you have news to report relevant to that story for us tonight. What is it?MELUGIN: Yes, Tucker. That's right. So look, ever since LA DA George Gascon was elected, it's no secret that criminals have been celebrating his soft on crime policies, but never have we seen such a blatant example of this as what your viewers are about to see and hear right now.FOX News has obtained exclusive jailhouse audio of a convicted gang murderer named Luis Angel Hernandez boasting that he is going to get George Gascon's name tattooed on his face and he calls George Gascon a champ for dropping his gang and gun enhancements. Take a listen.(BEGIN AUDIO CLIP)LUIS ANGEL HERNANDEZ, CONVICTED GANG MURDERED: That [bleep] looking real good. Now, we've got a new DA in LA, so they're going to -- I got caught on the 14th, fool.Right there in Compton on Thursday, so, they're going to drop the gang or like gun enhancement, my gang enhancement. My gang enhancement is 10 years, fool, for being a gang member. And then, the gun in the commission of crime.UNIDENTIFIED MALE: Whatever (INAUDIBLE) Gascon or whatever the [bleep].HERNANDEZ: I'm going to get [bleep] name on my face. That's a champ right there. [Bleep] Gascon.UNIDENTIFIED MALE: (INAUDIBLE) for misdemeanor and stuff.HERNANDEZ: That's the [bleep] right there, bro. He is making historic changes for all of us, fool. You know, so, I am just grateful, fool, like I've got good news off that [bleep].So, at least now, I know like, they're' like, \\\"You're coming home, blood.\\\" Like, they are already told me, my lawyer told me, \\\"You're coming home.\\\"(END AUDIO CLIP)MELUGIN: And Tucker, what you heard right there certainly is not an isolated incident. Your documentary also has another murderer from behind his cell in prison toasting one of his cellmates with prison moonshine smiling, saying, \\\"Hey, we're going home on this George Gascon directive.\\\"Your documentary also has jailhouse audio of a 26-year-old transgender child molester boasting that nothing is going to happen to him and he's not going to face any prison time because George Gascon refused to prosecute him as an adult.There is no denying that George Gascon's policies treat criminals with kiddy gloves sometimes, and prosecutors in his office tell me when it comes to George Gascon, DA stands for Defense Attorney.We'll send it back to you.CARLSON: That is effectively true. Bill Melugin, a big part of that documentary. Thanks so much for all the help you gave us.So as we said, we are premiering Season Two of our series, \\\"Tucker Carlson Originals.\\\" The first one came out today and it's really the story of democracy subverted.So you take the second biggest city in the country, LA, and a tiny group of people back, a truly radical prosecutor called George Gascon, the Black Lives Matter people, the defund the police people, airhead celebrities, George Soros -- this guy takes over and one man changes life for everybody in Los Angeles. It's beyond belief.The documentary called \\\"Suicide of LA.\\\" Part one is available right now. Here's a portion of it.(BEGIN VIDEO CLIP)CARLSON: So who exactly supports this? How could a sane person vote for a maniac like George Gascon who is so clearly intent on destroying what previous generations have built?MELUGIN: George Gascon's primary supporters were Black Lives Matter crowd.PATRISSE CULLORS, FOUNDER, BLACK LIVES MATTER: Why has this been called the second most important race in the country next to the Presidency?MELUGIN: George Gascon is supported by the crowd that wants to defund law enforcement.UNIDENTIFIED MALE: Defund the police.GEORGE GASCON, LOS ANGELES COUNTY DISTRICT ATTORNEY: This is really about moving funding around.MELUGIN: He is supported by a large swath of the celebrity crowd.GASCON: Please join me in welcoming the one and only, John Legend.SOPHIA BUSH, ACTRESS: The DA's races are so, so critical to how our cities function and especially here in LA.MELUGIN: And of course, George Soros who has been bankrolling progressive DAs all across the entire country.CARLSON: George Soros donated more than $2 million to George Gascon's campaign, but he wasn't alone. The CEO of Netflix, Reed Hastings and his wife Patty Quillin donated $2.1 million.ALEX VILLANUEVA, LOS ANGELES COUNTY SHERIFF: Oh, yes, this is all funded by Soros and company, and Reed Hastings, all the billionaires up in the Bay Area.CARLSON: The only significant elected official in Los Angeles who opposes George Gascon is the County Sheriff, Alex Villanueva.VILLANUEVA: The problem is here in LA, in city and county government, they occupy every single seat. There is no other point of view other than that woke ideology.You have to operate in the real world, not their fake fantasy. From 2019 to 2021, we saw 94 percent increase in homicides, which is a mind boggling increase. I can't get the Board of Supervisors to even admit that homicides have gone up 95 percent.MELUGIN: I think the understatement of the year would be to say that there is no love lost between George Gascon and LA County Sheriff, Alex Villanueva. They are polar opposites.VILLANUEVA: My lack of relationship with the DA is unprecedented. I've had one phone call with him since he has taken office. That is it.UNIDENTIFIED MALE: Let's start with the rise in crime in LA County. You are the Sheriff. Do you bear any responsibility for that?VILLANUEVA: Well, as I am being defunded and being discredited and delegitimized by our elected officials, it is kind of hard to put the blame on the people who is doing the most of the work.For six months, with Gascon's time in office, he rejected 5,932 cases. That means all those people just walk free.(END VIDEO CLIP)CARLSON: \\\"Suicide of Los Angeles\\\" sadly not an overstatement. Available now on FOX nation. Part Two out tomorrow. You can get a free account on tuckercarlson.com and use it at FOX Nation.So you may have noticed, it's hard to miss it, unequal justice under the law is now the rule. Your punishment depends on your political views.So January 6 defendants been rotting in jail for over a year without bail for nonviolent crimes, but actual criminals like the guy who shot nine people in a South Carolina mall over the weekend, get released on bond for 25 grand. That's the system. Still, straight ahead.(COMMERCIAL BREAK)CARLSON: So the whole point of America is that American laws are applied evenly and fairly to American citizens. They are not even trying anymore to do that.After January 6, the Biden administration placed dozens of its political opponents in solitary for months and months and months, nonviolent protesters who didn't hurt anyone.Meanwhile, actual violent offenders like a guy who just shot nine people in a shopping mall in South Carolina over the weekend are already out of jail. Watch.(BEGIN VIDEO CLIP)UNIDENTIFIED MALE: In Colombia, more than a dozen people injured after gunfire erupted Saturday at a shopping mall. Police believe a dispute among at least three people led to the violence.They charged 22-year-old Jewayne Price with unlawful possession of a gun. He was released on house arrest after a Judge set bond at just $25,000.00 allowing him to go to work with an ankle monitor.(END VIDEO CLIP)CARLSON: The Judge's name in that case is Crystal Rookard by the way. That's her.Buck Sexton is one of the few former C.I.A. officers we trust, our friend, host of \\\"The Clay Travis and Buck Sexton Show,\\\" he joins us now.Buck, thanks so much for coming on. It does seem like the core promise of equal justice has been corroded.BUCK SEXTON, FORMER C.I.A. OFFICER: The Democratic Party has been using prosecutors' offices, not just for social engineering, or you could say, for replacing criminal justice with social justice, but also as a weapon.I mean, the most prominent recent example of this is January 6th, where you have people, let's remind ourselves, they're held because they're either supposed to be a threat to the public, which I don't think any person actually believes, is based in any reality when you're talking about January 6 or a flight risk, which also seems quite strange, considering many of them have already pleaded and they've gotten months, one just actually got off entirely because he was waved into the Capitol.But look at the way the D.O.J. was weaponized against Donald Trump before that. In fact, I would say Republicans have gotten far too used to this whether it's Governor Scott Walker up in Wisconsin or the John Doe Law is being used to go after him, Chris Christie in Bridge-gate. They wanted to lock up Governor McDonnell, former Governor McDonnell of Virginia, his wife for taking gifts if you all remember that at one point. They wanted to go after a Rick Perry when he was the Governor of Texas.These are all prosecutions, by the way that were either in part or in total dramatic overreach, and it always seems to go one way. Where is the equivalent on the other side? That was just off the top of my head. Meanwhile, people who shoot a lot of people get let out on bail because they're not a danger.CARLSON: Twenty five grand for shooting. Nine people are shot in a shopping mall and there is only a gun charge and 25 grand and he's out. I mean, how can that be?SEXTON: We've seen the progressive prosecutors have taken the opinion in city after city at this point, that if they're only honestly softer, more gentle on violent crime across the board, let them out sooner from either a sentence, let them out right away with catch and release bail reform laws, that somehow this will improve the community police relations that we've seen talked about a bit more now in the aftermath of the BLM protests and riots. That hasn't worked at all.They have to admit now across the country that the crime rate has gone up dramatically in pretty much every major city you can think of, not because of COVID, which is one of the lies they told not, because we were cooking the books and trying to create hysteria on TV, but because they have really stupid policies that are reckless and made things worse for everyone.And Tucker, I think they only care about it, which is obvious to everybody now, because crime is a major issue in city after city and they're going into a midterm election and they have people like Gascon and others by the way, Krasner in Philadelphia, Boudin in San Francisco, who just look like lunatics while good people are suffering. They're all Democrats.CARLSON: That's exactly right. Nicely put, the Great Buck Sexton, thank you.SEXTON: Thanks, Tucker.CARLSON: Good to see you.So, the Democratic Party is bracing for total wipeout in the midterms. At this point, they're not even trying to convince you to vote for them, they are trying to take their opponents off the ballot. They're attacking democracy, not letting you vote for people you want to vote for.Here is the latest and most shocking example, a new legal effort underway to keep Marjorie Taylor Greene off the ballot in her own district where she is very popular in the State of Georgia. On what grounds? The left is saying she is guilty of quote, \\\"insurrection.\\\"Marjorie Taylor Greene represents Georgia in this Congress, and she joins us tonight.Congresswoman, thanks so much for coming on. So, they're trying to prevent voters from voting for you. How is that democracy?REP. MARJORIE TAYLOR GREENE (R-GA): Well, it's not, Tucker. That's the thing. These people hate the people in my district so much, they look down on them, because they voted for me and sent me to Washington to fight for the things that most Americans care about, like secure borders, stopping abortion, protecting our Second Amendment, stopping the out of control spending in Washington, and stop funding, never-ending foreign wars and all the insanity that takes place in Washington.Well, I went there and I have been fighting it. And now the progressives, the people that donate, the dark money groups, you know, the 501(c) (3) and the foundations, they've hired up some attorneys from New York who hate the people in my district, and don't believe that they should have the right to elect who they want to send to Washington, which is me. I have overwhelming support in my district, and I'm so thankful for all of them.Well, now they filed a lawsuit because they're trying to rip my name off of the ballot and steal my district's ability to re-elect me and send me back to Congress.CARLSON: So if you can prevent voters from being allowed to vote for the candidate of their choice, which is their constitutional right, then the system is over. Is the Republican Party with all four paws jumping in to help you?GREENE: Not yet. I'm on my own to defend myself. Wonderful people are donating to my campaign, MTGforamerica.com, and I'm so grateful for that. But I have to protect myself. I have to go to Court on Friday and actually be questioned about something I've never been charged with and something I was completely against.And so this is how far it's going, these leftists, these progressives who would rather want -- they'd rather have the judge or bureaucrats making decisions instead of voters, they want to hand that over to them and not let the people in my district to even have the right to vote for me.But no, the Republican Party needs to fight harder, Tucker. You know, there is something that I have learned and I think this is really important. You know, if you can challenge any representative's candidacy or elected office holder, then I bet you we could round up some Republican voters who didn't like Kamala Harris funding rioters, criminal rioters out of jail, or Ilhan Omar or Cori Bush or Maxine Waters inciting riots.You know, I think there is another way to play this game.CARLSON: Well, of course. American citizens have an absolute right to vote for anyone they want to because it is their government, it is self- government, and if you take that away, it's tyranny, obviously.We appreciate your coming on tonight, Congresswoman Marjorie Taylor Greene, of Georgia. Thank you.GREENE: Thank you, Tucker.CARLSON: Pretty shocking medical mystery in the State of New Jersey. Nearly a hundred students who went to a high school nearby have developed the same extremely rare and deadly tumors. Why?Dr. Marc Siegel after the break.(COMMERCIAL BREAK)CARLSON: FOX News Alert: The mask mandate on airlines is dead. Masks are now optional on the biggest U.S. carriers that includes American, Delta, United, Alaska, Southwest. The T.S.A. has also announced it is not going to enforce the mandate anymore. You're not going to get arrested, passengers cheered on many flights when they heard this news in midair.This comes because a Florida Judge has ruled the Biden administration doesn't get to make up the rules because they're not God. This isn't a monarchy, they can't arrest you if you don't comply with their fake rules.Here's one quote from the Judge, quote: \\\"The power to constitutionally release and detain is limited to individuals entering the U.S. from a foreign country.\\\" In other words, you don't get to treat American citizens like an invading army, even as you treat the invaders like they're your own children, which is exactly what they're doing.Anyway, no more mask mandate on planes. Amen.We have been focused on COVID for two years because it's helped a lot of politicians get more powerful, but there are real public health emergencies going on in this country.One of them apparently is unfolding in New Jersey. Dozens of people connected to a single high school in Woodbridge Township are coming down with rare brain tumors. Here's a local news report on it.(BEGIN VIDEO CLIP)UNIDENTIFIED FEMALE: Al Lupiano, an environmental scientist and former resident of Woodbridge Township says he has confirmed 65 cases of people with rare brain tumors.The common denominator, they were all Colonia High School graduates or had worked there.Lupiano was diagnosed 20 years ago and still suffers lingering issues.AL LUPIANO, ENVIRONMENTAL SCIENTIST: Fast forward to August of last year, my sister received the news that she had a primary brain tumor herself, it unfortunately turned out to be stage four glioblastoma. Two hours later, we received information that my wife also had a primary brain tumor.UNIDENTIFIED FEMALE: After his sister sadly passed away less than a month ago, he posted on Facebook calling on all Colonia High School alumni asking if others had brain tumors, and the response was shocking.(END VIDEO CLIP)CARLSON: Shocking is right and scary. What is this exactly? Dr. Marc Siegel joins us tonight to assess -- Doctor.DR. MARC SIEGEL, FOX NEWS CHANNEL MEDICAL CONTRIBUTOR: Tucker, I want to start with a little bit of history. You know, the Middlesex Sampling Plant, which is less than 10 miles away from this high school is part of -- was part of the Manhattan Project that made the first atomic bomb and they had uranium there and they didn't close it until 1967, the same year that Colonia High School opened.And not only that, but reports were that they didn't fully decontaminate that Middlesex Plant until more than 10 years, more than 20 years later, until the 1990s, it wasn't fully decontaminated.And the question is, was there an association between the uranium in that plant and the soil and the radiation in what happened in this high school.Now, let's talk about this high school. Six out of 100,000 people a year get any kind of brain or spinal cord tumor. Very, very rare, but this high school has seen over a hundred brain tumors from 1975 to 2000.You saw Al Lupiano there, he has an acoustic neuroma which is very, very rare. His sister died of a glioblastoma -- terrible, terrible brain tumor.I spoke to our head of Neurosurgery, John Golfinos at NYU who is an expert at this, and also Tom Roland who is an acoustic neuroma expert at NYU, one of the world's best. Both say that a little bit of ionizing radiation, just a small amount in the area is enough to provoke these tumors.Now, we don't know for sure that this is what has gone on, but the Environmental Protection Agency is involved, and the local Department of Health in New Jersey is involved.Tucker, this is what we call a cluster and there are 1,300 students in this school today all wanting to know, do I have risk? Is there radiation here? But you know what I want to add tonight, not just the part about the Manhattan Project, not about just how we dispose of radiation and the issues of our environment, but we need to look beyond the school. We need to look at the entire area around this plant and see and test for radiation in the air and in the soil, Tucker, because people can be hurt and we need to know.CARLSON: Well, that's exactly right. Environmental poisoning is real. It's not just about climate change and COVID. There is a lot going on, and we should pay attention. I appreciate that report.Dr. Marc Siegel, thank you.SIEGEL: Thanks, Tucker.CARLSON: So you've probably eaten a Jimmy John's sandwich, they are all over the country, Jimmy John Liautaud made it. He made millions making sandwiches starting when he was just 19. He never took on debt. Amazing story. Amazing, man. We'll meet him next.(COMMERCIAL BREAK)CARLSON: You've probably had a Jimmy John's sandwich over the years, but if you're like us, you didn't know who Jimmy John was, Jimmy John is Jimmy John Liautaud, he sold most of his company and made billions. He is an amazing person, and politically aware, too.At one point, he came out against Barack Obama and paid the price for that. So we sat down with him for an hour for a new episode of \\\"Tucker Carlson Today\\\" and learned a ton, but mostly we were amused, inspired, engaged. Amazing guy. Here is part of it.(BEGIN VIDEO CLIP)JIMMY JOHN LIAUTAUD, FOUNDER AND FORMER CHAIRMAN OF JIMMY JOHN'S SANDWICH CHAIN: After Obama won his second term, I was completely annihilated and canceled.I'm a hunter, I'm overweight, I'm very, very successful and rich, right? So I get [bleep] beat out of me for all those three things, completely canceled before cancel was cool.But why I want to talk about this is, I had 2,800 restaurants open like 3,000 sold that I had deposits on to open from my operators, and these are mom and pops across America, and they changed the labor law, the definition of what a manager is, minimum wage, insurance, and all of a sudden, I had 3,000 restaurants, each one employing 40 or 50 people, right, and 3,000 restaurants where they just, you know, they just -- virtually, the business just stopped.My mom and pop franchisees were being sued by law firms that lobbied the government, right, for labor from us taking advantage of our people and just became this arduous completely complex -- our mom and pops, you have three -- the average owner had like three and a half stores.I'm incredibly successful, I'm grateful. But what happened was a travesty because it made them -- they were taking all their extra time baking cookies for people that kicked ass and sampling baseball games and building their catering businesses and learning how to run business, and we taught them how to run these businesses.And when we did it with them, and then when the labor laws changed in and middle of the game , we were just sitting there holding the bag, and these mom and pops are now in Court. And then the laws change even further and we became a co-employer, meaning I'm responsible for all the employees in all these sandwich shops all around America so that the big, big law firms could then sue so they could get their money.So I had no idea -- here I was helping Mitt Romney for America, for everybody to have the dream, and I had no idea that I was going to be attacked, and it got so -- anyway, I spend all my time in Court and I did sell the business and I'm happy I did.But this is interesting as well. I've hired a company to go back and do the forensics on where all the money came from and all the attacks, and all the canceling on me, right, it all came from politics, all from the Democratic Party, all supported by them, or their PACs or Soros or whatever it is.And anyway --(END VIDEO CLIP)CARLSON: The guy owned a sandwich shop and he dared to contribute to the wrong candidate and that's what happened to him.But the whole thing is that amazing conversation. What a guy. Jimmy John Liautaud.We're getting up earlier now so you can watch our interviews on FOX Nation, \\\"Tucker Carlson Today\\\" starting at 7:00 AM.Now, we've heard a lot -- we spent a lot of time on the documentary series and we have news to share about that, next.(COMMERCIAL BREAK)CARLSON: So how do you watch the documentaries we make? Our friend, the bestselling author, Shannon Bream is here to tell us.SHANNON BREAM, FOX NEWS CHANNEL CHIEF LEGAL CORRESPONDENT: All right, Tucker. There has been a lot of excitement for Season Two of \\\"Tucker Carlson Originals,\\\" which we got a sneak peek on Friday. I am definitely interested.It is much-watch TV, episodes like \\\"Suicide of Los Angeles,\\\" \\\"Transgressive: The Cult of Confusion,\\\" The Life of a Rock Star: Kid Rock,\\\" and of course everyone is talking about \\\"The End of Men,\\\" some very interesting things in that one.We were flooded with calls and e-mails at FOX this weekend. Everybody wants to know how to watch. I've been confused.But the good news is it's easy. You don't even need to pay for it. Go to tuckercarlson.com. On Tucker's website, you will see something that says free FOX Nation. We want you to click there. Enter your e-mail address and follow the instructions.You can then use your new free account and use it on a phone or tablet. Many say I'd rather watch this on TV, no problem. If you don't have the app on your TV or a TV that doesn't have apps, pick up something like a Roku. It's cheap. It's at a place like Walmart, Target, or Best Buy.Again, it's called Roku, it's about 30 bucks. I am not getting a kickback. I just want you to know.So to recap go to tuckercarlson.com, get the free account, then use it on your phone, iPad or TV. It is that easy.And Tucker, I will not lie, it has been hard for me to sign up, but now I've got it.Content and Programming Copyright 2022 Fox News Network, LLC. ALL RIGHTS RESERVED. Copyright 2022 VIQ Media Transcription, Inc. All materials herein are protected by United States copyright law and may not be reproduced, distributed, transmitted, displayed, published or broadcast without the prior written permission of VIQ Media Transcription, Inc. You may not alter or remove any trademark, copyright or other notice from copies of the content.\"\r\n\r\nBeing Polite\r\nIf you plan to use lots of webscraping for your research, it would be\r\nwise to brush up on etiquette. Some sites (like Facebook) explicitly ban\r\nscraping in their Terms of Service, and even sites that do permit\r\nscraping would prefer you didn’t overload them with automated requests.\r\nYou know how people talk about bots destroying the Internet? Well,\r\nyou’ve just created a bot, and it’s incumbent on you to use it wisely.\r\nThe polite\r\npackage is a good place to get started.\r\nPractice Problems\r\nScrape the transcript from the Rachel\r\nMaddow Show on May 11, 2022.\r\nScrape the text of Federalist\r\nPapers No. 10.\r\nScrape the text of the US\r\nCongressional Record (Senate - May 16, 2022).\r\nScrape the text of The\r\nPatient Protection and Affordable Care Act (March 23,\r\n2010).\r\n\r\n\r\n\r\n",
      "last_modified": "2022-06-15T07:43:34-04:00"
    },
    {
      "path": "word-embeddings.html",
      "title": "Word Embeddings",
      "description": "How to quantify what a word *means*.\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nPractice Problems\r\nFurther Reading\r\n\r\nWe’d like not just to count the frequency of words, but also get a\r\nsense of what the words mean. In a bag of words representation, we treat the\r\nwords “president” and “executive” as having completely unique meaning.\r\nBy comparison, a word embeddings approach encodes the fact that these\r\nwords have some overlapping meaning by placing them close together in a\r\nvector space.\r\n\r\n\r\n\r\nThe textdata package is a useful interface for\r\ndownloading pre-trained word embeddings like GloVe. These off-the-shelf\r\nword embeddings tend to do a pretty good job at capturing meaning, even\r\nfor political science specific applications (Rodriguez and Spirling 2021).\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(textdata)\r\n\r\n\r\n\r\nFor expository purposes, let’s just get 400,000 of the\r\n100-dimensional word embeddings:\r\n\r\n\r\nglove <- embedding_glove6b(dimensions = 100)\r\n\r\n\r\n\r\nThis comes to us as a matrix, but we can pivot it into a tidy\r\ndataframe.\r\n\r\n\r\ntidy_glove <- glove |>\r\n  pivot_longer(contains(\"d\"),\r\n               names_to = \"dimension\")\r\n\r\ntidy_glove\r\n\r\n\r\n# A tibble: 40,000,000 x 3\r\n   token dimension   value\r\n   <chr> <chr>       <dbl>\r\n 1 the   d1        -0.0382\r\n 2 the   d2        -0.245 \r\n 3 the   d3         0.728 \r\n 4 the   d4        -0.400 \r\n 5 the   d5         0.0832\r\n 6 the   d6         0.0440\r\n 7 the   d7        -0.391 \r\n 8 the   d8         0.334 \r\n 9 the   d9        -0.575 \r\n10 the   d10        0.0875\r\n# ... with 39,999,990 more rows\r\n\r\nIt’s difficult to visualize and interpret a 100-dimensional vector\r\nspace, but we can explore which words have the highest cosine\r\nsimilarity. By looking at a word’s “nearest neighbors”, we can get a\r\nsense of the meaning that GloVe is assigning to it. The following\r\nfunction, adapted from Emil\r\nHvitfeldt and Julia Silge, performs that computation.\r\n\r\n\r\nlibrary(widyr)\r\nnearest_neighbors <- function(df, token) {\r\n  df |>\r\n    rename(item1 = token) |>\r\n    widely(\r\n      ~ {\r\n        y <- .[rep(token, nrow(.)), ]\r\n        res <- rowSums(. * y) /\r\n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\r\n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\r\n      },\r\n      sort = TRUE,\r\n      maximum_size = NULL\r\n    )(item1, dimension, value) %>%\r\n    select(-item2)\r\n}\r\n\r\n\r\n\r\nWhat words are most closely associated with the word “democracy”?\r\n\r\n\r\nnearest_neighbors(tidy_glove, 'democracy')\r\n\r\n\r\n# A tibble: 400,000 x 2\r\n   item1        value\r\n   <chr>        <dbl>\r\n 1 democracy    1    \r\n 2 freedom      0.739\r\n 3 unity        0.710\r\n 4 independence 0.700\r\n 5 political    0.677\r\n 6 movement     0.669\r\n 7 opposition   0.666\r\n 8 peace        0.665\r\n 9 freedoms     0.663\r\n10 peaceful     0.663\r\n# ... with 399,990 more rows\r\n\r\nPractice Problems\r\nExplore some of the stereotypes reflected in the GloVe embeddings.\r\nHow close is the word “professor” to female names compared to male\r\nnames? Hispanic names?\r\nWhat about words that are ambiguous without context, like “bill” or\r\n“share”?\r\nFurther Reading\r\nGrimmer, Stewart, and Roberts (2021), Chapter 8.\r\nHvitfeldt &\r\nSilge, Chapter 5.\r\n\r\n\r\n\r\nGrimmer, Justin, Brandon M. Stewart, and Margaret E. Roberts. 2021.\r\nText as Data: A New Framework for Machine Learning and the Social\r\nSciences. S.l.: Princeton University Press.\r\n\r\n\r\nRodriguez, Pedro L., and Arthur Spirling. 2021. “Word Embeddings:\r\nWhat Works, What Doesn’t, and How to Tell the Difference\r\nfor Applied Research.” The Journal of Politics, May,\r\n000–000. https://doi.org/10.1086/715162.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-06-15T07:43:44-04:00"
    }
  ],
  "collections": []
}
