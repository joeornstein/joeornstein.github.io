[
  {
    "objectID": "spring-2025.html#week-2-127",
    "href": "spring-2025.html#week-2-127",
    "title": "Spring 2025 Predictions",
    "section": "Week 2 (1/27)",
    "text": "Week 2 (1/27)"
  },
  {
    "objectID": "spring-2025.html#week-3-23",
    "href": "spring-2025.html#week-3-23",
    "title": "Spring 2025 Predictions",
    "section": "Week 3 (2/3)",
    "text": "Week 3 (2/3)"
  },
  {
    "objectID": "spring-2025.html#week-4-210",
    "href": "spring-2025.html#week-4-210",
    "title": "Spring 2025 Predictions",
    "section": "Week 4 (2/10)",
    "text": "Week 4 (2/10)"
  },
  {
    "objectID": "spring-2025.html#week-5-217",
    "href": "spring-2025.html#week-5-217",
    "title": "Spring 2025 Predictions",
    "section": "Week 5 (2/17)",
    "text": "Week 5 (2/17)"
  },
  {
    "objectID": "spring-2025.html#week-6-224",
    "href": "spring-2025.html#week-6-224",
    "title": "Spring 2025 Predictions",
    "section": "Week 6 (2/24)",
    "text": "Week 6 (2/24)"
  },
  {
    "objectID": "spring-2025.html#week-8-310",
    "href": "spring-2025.html#week-8-310",
    "title": "Spring 2025 Predictions",
    "section": "Week 8 (3/10)",
    "text": "Week 8 (3/10)"
  },
  {
    "objectID": "spring-2025.html#week-9-317",
    "href": "spring-2025.html#week-9-317",
    "title": "Spring 2025 Predictions",
    "section": "Week 9 (3/17)",
    "text": "Week 9 (3/17)"
  },
  {
    "objectID": "spring-2025.html#week-10-324",
    "href": "spring-2025.html#week-10-324",
    "title": "Spring 2025 Predictions",
    "section": "Week 10 (3/24)",
    "text": "Week 10 (3/24)"
  },
  {
    "objectID": "spring-2025.html#week-11-331",
    "href": "spring-2025.html#week-11-331",
    "title": "Spring 2025 Predictions",
    "section": "Week 11 (3/31)",
    "text": "Week 11 (3/31)"
  },
  {
    "objectID": "spring-2025.html#week-12-47",
    "href": "spring-2025.html#week-12-47",
    "title": "Spring 2025 Predictions",
    "section": "Week 12 (4/7)",
    "text": "Week 12 (4/7)"
  },
  {
    "objectID": "spring-2025.html#week-13-414",
    "href": "spring-2025.html#week-13-414",
    "title": "Spring 2025 Predictions",
    "section": "Week 13 (4/14)",
    "text": "Week 13 (4/14)"
  },
  {
    "objectID": "spring-2025.html#week-14-421",
    "href": "spring-2025.html#week-14-421",
    "title": "Spring 2025 Predictions",
    "section": "Week 14 (4/21)",
    "text": "Week 14 (4/21)"
  },
  {
    "objectID": "spring-2025.html#leaderboard",
    "href": "spring-2025.html#leaderboard",
    "title": "Spring 2025 Predictions",
    "section": "Leaderboard",
    "text": "Leaderboard\n\n\n\n\n\nRank\nName\nCorrect Calls\nBrier Score\n\n\n\n\n1\nTiyanah Rowtham\n25/28\n0.0877\n\n\n2\nJoe Ornstein\n30/36\n0.1056\n\n\n3\nBrooke Cheney\n24/29\n0.1118\n\n\n4\nAndy Wyatt\n27/33\n0.1139\n\n\n5\nAyaan Fazal\n26/32\n0.1179\n\n\n6\nEthan Joyce\n23/28\n0.1199\n\n\n7\nBenjamin Jacobstein\n29/34\n0.1226\n\n\n8\nAngela Jacobs\n27/30\n0.1276\n\n\n9\nHannah Carson\n30/36\n0.1295\n\n\n10\nKaitlyn Scoggins\n27/34\n0.1309\n\n\n11\nCrowd (Extremized)\n28/36\n0.1379\n\n\n12\nIsabella Molina\n28/31\n0.1427\n\n\n13\nVangeli Tsiaras\n24/30\n0.1443\n\n\n14\nMilan Yadav\n26/33\n0.1453\n\n\n15\nGrace Mccann\n24/30\n0.1459\n\n\n16\nClass Average\n28/36\n0.1479\n\n\n17\nMeera Srinivasan\n24/30\n0.1511\n\n\n18\nJoshua Kang\n26/33\n0.1523\n\n\n19\nPablo Martinez\n23/29\n0.1576\n\n\n20\nQuintin Gallardo\n25/31\n0.1576\n\n\n21\nArchita Gaur\n26/32\n0.1596\n\n\n22\nBennie Trujillano\n27/36\n0.1613\n\n\n23\nHampton Barrineau\n24/31\n0.1618\n\n\n24\nLillie Puhrmann\n25/33\n0.1619\n\n\n25\nTownsend Turner\n24/32\n0.1623\n\n\n26\nBailey Mattox\n25/33\n0.1640\n\n\n27\nJessy Lee\n25/32\n0.1665\n\n\n28\nSreya Pandyaram\n26/33\n0.1668\n\n\n29\nIsaac Fleischer\n21/30\n0.1681\n\n\n30\nLawton Smith\n23/31\n0.1695\n\n\n31\nRiley Batz\n25/32\n0.1701\n\n\n32\nJulia Loggins\n25/32\n0.1736\n\n\n33\nRebecca Mcadam\n27/35\n0.1743\n\n\n34\nChinelo Ireh\n24/32\n0.1745\n\n\n35\nAva Herring\n25/32\n0.1755\n\n\n36\nThomas Smith\n25/32\n0.1755\n\n\n37\nEmma Allen\n27/34\n0.1762\n\n\n38\nEmma Peterson\n27/35\n0.1779\n\n\n39\nMatthew Newby\n16/19\n0.1789\n\n\n40\nJoey Khashan\n23/31\n0.1790\n\n\n41\nAnna Rachwalski\n23/33\n0.1816\n\n\n42\nJenny Qiu\n24/32\n0.1861\n\n\n43\nMaston Corn\n23/33\n0.1876\n\n\n44\nMabry Cartwright\n27/35\n0.1882\n\n\n45\nLevi Thomas\n23/32\n0.1894\n\n\n46\nMia Pisani\n25/34\n0.1898\n\n\n47\nChristian Barr\n10/14\n0.1915\n\n\n48\nJake Garrard\n20/28\n0.1921\n\n\n49\nAddisen Kameron\n24/31\n0.1933\n\n\n50\nSenait Pirani\n18/26\n0.1936\n\n\n51\nWyatt Dasher\n21/33\n0.1948\n\n\n52\nSarah Joyce\n22/33\n0.1954\n\n\n53\nMargaret Pope\n24/33\n0.1963\n\n\n54\nGrace Reichelderfer\n23/31\n0.1987\n\n\n55\nDiane Johnson\n23/31\n0.2006\n\n\n56\nRuby Douglas\n17/29\n0.2021\n\n\n57\nChloe Cooper\n25/34\n0.2034\n\n\n58\nWyatt Stevenson\n20/30\n0.2043\n\n\n59\nEthan Marcum\n24/33\n0.2097\n\n\n60\nTristan Thompson\n24/31\n0.2100\n\n\n61\nKyle Smith\n21/31\n0.2112\n\n\n62\nAaliyah Diaz\n20/28\n0.2117\n\n\n63\nEli Henderson\n20/27\n0.2131\n\n\n64\nKelsey Dyar\n17/24\n0.2151\n\n\n65\nTeya Hagy\n20/31\n0.2166\n\n\n66\nEmma Pastor\n25/35\n0.2197\n\n\n67\nAndreas Papandreou\n20/30\n0.2205\n\n\n68\nJacob Weiss\n21/31\n0.2314\n\n\n69\nElliana Williams\n16/29\n0.2418\n\n\n70\nNathaniel Long\n21/32\n0.2518\n\n\n71\nJames Martin\n16/26\n0.2526\n\n\n72\nJerusalem Greenberg\n19/28\n0.2542\n\n\n73\nDa In Song\n17/28\n0.2787\n\n\n74\nNiomi Nurse\n14/29\n0.3183"
  },
  {
    "objectID": "spring-2025.html#calibration-plots",
    "href": "spring-2025.html#calibration-plots",
    "title": "Spring 2025 Predictions",
    "section": "Calibration Plots",
    "text": "Calibration Plots"
  },
  {
    "objectID": "slides/prediction-markets.html#hedgehog-professor-vs.-the-market",
    "href": "slides/prediction-markets.html#hedgehog-professor-vs.-the-market",
    "title": "Prediction Markets",
    "section": "Hedgehog Professor vs. The Market",
    "text": "Hedgehog Professor vs. The Market"
  },
  {
    "objectID": "slides/prediction-markets.html#todays-agenda",
    "href": "slides/prediction-markets.html#todays-agenda",
    "title": "Prediction Markets",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nWhat are prediction markets?\nHow do they work?\nWhen should you trust them?\nWhen should you be skeptical?"
  },
  {
    "objectID": "slides/prediction-markets.html#probability-as-a-bet",
    "href": "slides/prediction-markets.html#probability-as-a-bet",
    "title": "Prediction Markets",
    "section": "Probability As A Bet",
    "text": "Probability As A Bet\n\nSuppose I offered you a contract that pays $1 if it rains tomorrow.\nPonder: How much would you be willing to pay for this contract?\nIt depends on the probability of rain!\nIf \\(P(\\text{rain}) = 0.2\\), then buying the contract for any price less than 20¢, will, on average, earn you a profit."
  },
  {
    "objectID": "slides/prediction-markets.html#prediction-markets",
    "href": "slides/prediction-markets.html#prediction-markets",
    "title": "Prediction Markets",
    "section": "Prediction Markets",
    "text": "Prediction Markets\n\nA prediction market is an exchange where people buy and sell these types of contracts.\nThe potential for profit creates strong incentives for participants to seek out good information and make good predictions (Arrow et al. 2008).\nThe “market price” for each contract reflects participants’ beliefs about the probability of that event."
  },
  {
    "objectID": "slides/prediction-markets.html#class-activity",
    "href": "slides/prediction-markets.html#class-activity",
    "title": "Prediction Markets",
    "section": "Class Activity",
    "text": "Class Activity\n\nTo demonstrate how prediction markets work, we’re going to create our own.\nEveryone starts the game with a sheet of 300 foxcoin and a stack of contracts.\n\nYour goal is to have the most foxcoin by November 25.\n\nWe’ll record the order book for each contract on the board.\n\nIf you want to buy a contract, put your name and the price you’re wiling to pay in the “Bid” column.\nIf you want to sell a contract, put your name and the price you’re willing to pay in the “Ask” column."
  },
  {
    "objectID": "slides/prediction-markets.html#when-do-markets-work-best",
    "href": "slides/prediction-markets.html#when-do-markets-work-best",
    "title": "Prediction Markets",
    "section": "When do markets work best?",
    "text": "When do markets work best?\n\nTrading volume is high.\n\nJust like we saw with the Condorcet Jury Theorem, the “wisdom of crowds” works best when the crowd is large.\n\nTime horizons are short.\n\nMarkets tend to be more accurate the closer you get to an event.\nProfit motive is much weaker for long-term contracts.\n\n(Why put in the effort to accurately price a 10-year prediction contract when you could just earn 50% compound interest on a US Treasury Bond?)"
  },
  {
    "objectID": "slides/prediction-markets.html#when-should-you-be-skeptical",
    "href": "slides/prediction-markets.html#when-should-you-be-skeptical",
    "title": "Prediction Markets",
    "section": "When should you be skeptical?",
    "text": "When should you be skeptical?\n\n\n\n\n\n\n\nAnyone who thinks the answer is less than 20% has no incentive to participate in this market!"
  },
  {
    "objectID": "slides/prediction-markets.html#references",
    "href": "slides/prediction-markets.html#references",
    "title": "Prediction Markets",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nArrow, Kenneth J., Robert Forsythe, Michael Gorham, Robert Hahn, Robin Hanson, John O. Ledyard, Saul Levmore, et al. 2008. “The Promise of Prediction Markets.” Science 320 (5878): 877–78. https://doi.org/10.1126/science.1157679."
  },
  {
    "objectID": "slides/long-tails.html#todays-agenda",
    "href": "slides/long-tails.html#todays-agenda",
    "title": "Long Tails",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nReview the Central Limit Theorem (CLT)\nDescribe the problem of tail risk\nConsider when the assumptions of CLT may be violated\nIntroduce long tail probability distributions"
  },
  {
    "objectID": "slides/long-tails.html#warmup",
    "href": "slides/long-tails.html#warmup",
    "title": "Long Tails",
    "section": "Warmup",
    "text": "Warmup\n\nUsing the definition of the Central Limit Theorem, explain why the Galton Board produces a bell curve shape."
  },
  {
    "objectID": "slides/long-tails.html#tail-risk",
    "href": "slides/long-tails.html#tail-risk",
    "title": "Long Tails",
    "section": "Tail Risk",
    "text": "Tail Risk\n\nA common forecasting problem is to try to quantify “tail risk”. What’s the probability of an extreme result, far outside what we ordinarily expect?\nLast time, we discussed the margin of error, which is a measure of tail risk in political polling.\nBut this type of problem crops up everywhere: financial analysis, military planning, emergency management, etc."
  },
  {
    "objectID": "slides/long-tails.html#bell-curves-have-low-tail-risk",
    "href": "slides/long-tails.html#bell-curves-have-low-tail-risk",
    "title": "Long Tails",
    "section": "Bell Curves Have Low Tail Risk",
    "text": "Bell Curves Have Low Tail Risk\n\n\n\n\n\nStandard deviation \\((\\sigma)\\) is a measure of the spread of the distribution. It measures how far, on average, observations lie from their expected value.\nA “2 sigma” event should happen only 5% of the time.\nA “3 sigma” event should happen only 0.3% of the time."
  },
  {
    "objectID": "slides/long-tails.html#financial-markets-example",
    "href": "slides/long-tails.html#financial-markets-example",
    "title": "Long Tails",
    "section": "Financial Markets Example",
    "text": "Financial Markets Example"
  },
  {
    "objectID": "slides/long-tails.html#financial-markets-example-1",
    "href": "slides/long-tails.html#financial-markets-example-1",
    "title": "Long Tails",
    "section": "Financial Markets Example",
    "text": "Financial Markets Example\n\n\n\n\n\n\n\n\n\n\n\n\nIt…looks like a bell curve?\n\n\\(\\sigma =\\) 0.84\nSo we should expect price movements greater than 1.68% only on about 1 in every 20 trading days.\nAnd that’s basically true! On 94% of trading days, price movements are within that range."
  },
  {
    "objectID": "slides/long-tails.html#so-whats-the-problem",
    "href": "slides/long-tails.html#so-whats-the-problem",
    "title": "Long Tails",
    "section": "So what’s the problem?",
    "text": "So what’s the problem?"
  },
  {
    "objectID": "slides/long-tails.html#so-whats-the-problem-1",
    "href": "slides/long-tails.html#so-whats-the-problem-1",
    "title": "Long Tails",
    "section": "So what’s the problem?",
    "text": "So what’s the problem?"
  },
  {
    "objectID": "slides/long-tails.html#long-tails",
    "href": "slides/long-tails.html#long-tails",
    "title": "Long Tails",
    "section": "Long Tails",
    "text": "Long Tails\n\nOn March 16, 2020 the S&P 500 index dropped 11.98%.\nThis was a “14 sigma” event.\nThe probability of such an event, according to the standard bell curve, is approximately 0.0000000000000000000000000000000000000000007793537%.\nTo express that somewhat dramatically: if the New York Stock Exchange had operated every single day since the birth of the universe 13.8 billion years ago, we still wouldn’t expect to see a price movement that large.\n\n\n3.9282933^{-32}"
  },
  {
    "objectID": "slides/long-tails.html#long-tails-1",
    "href": "slides/long-tails.html#long-tails-1",
    "title": "Long Tails",
    "section": "Long Tails",
    "text": "Long Tails\n\nSo, financial markets are clearly have greater tail risk than a bell curve would expect (Taleb 2007).\nDiscuss: Why do you think that is? Which part of the Central Limit Theorem is violated here?\nIn the rest of the lecture, we’ll discuss two types of violations, and the long tail distributions they generate."
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes\n\nImagine a group of gamblers who each start with $100.\nThey all make a series of wagers that either cause them to win or lose 10% of their money, with equal probability.\nWhat will the wealth distribution look like after a large number of these gambles?\n\nAs in CLT, wealth is the result of a large number of independent random events.\nBut wealth is the product of these events, not the sum."
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-1",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-1",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-2",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-2",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-3",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-3",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-4",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-4",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-5",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-5",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-6",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-6",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-7",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-7",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-8",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-8",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-9",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-9",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-10",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-10",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-11",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-11",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-12",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-12",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-13",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-13",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-14",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-14",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-15",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-15",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes"
  },
  {
    "objectID": "slides/long-tails.html#violation-1-multiplicative-processes-16",
    "href": "slides/long-tails.html#violation-1-multiplicative-processes-16",
    "title": "Long Tails",
    "section": "Violation 1: Multiplicative Processes",
    "text": "Violation 1: Multiplicative Processes\nThis is called a lognormal distribution, because here’s what it looks like on a logarithmic scale:"
  },
  {
    "objectID": "slides/long-tails.html#lognormal-distributions",
    "href": "slides/long-tails.html#lognormal-distributions",
    "title": "Long Tails",
    "section": "Lognormal Distributions",
    "text": "Lognormal Distributions\n\nLognormal distributions are common when outcomes are the result of multiplicative growth over time.\nFor example, the distribution of city sizes in the United States."
  },
  {
    "objectID": "slides/long-tails.html#lognormal-distributions-1",
    "href": "slides/long-tails.html#lognormal-distributions-1",
    "title": "Long Tails",
    "section": "Lognormal Distributions",
    "text": "Lognormal Distributions"
  },
  {
    "objectID": "slides/long-tails.html#lognormal-distributions-2",
    "href": "slides/long-tails.html#lognormal-distributions-2",
    "title": "Long Tails",
    "section": "Lognormal Distributions",
    "text": "Lognormal Distributions"
  },
  {
    "objectID": "slides/long-tails.html#violation-2-interdependence",
    "href": "slides/long-tails.html#violation-2-interdependence",
    "title": "Long Tails",
    "section": "Violation 2: Interdependence",
    "text": "Violation 2: Interdependence\n\nNext, let’s consider what happens when the independence assumption of CLT is violated.\nImagine a process of students forming clubs.\nEach student can choose either to start a new club or join an existing club.\nThis process exhibits “preferential attachment”. You’re more likely to join a popular club.\nSo club size is the sum of a large number of student choices.\nBut these choices are not independent. A club is more likely to grow if it’s already popular."
  },
  {
    "objectID": "slides/long-tails.html#violation-2-interdependence-1",
    "href": "slides/long-tails.html#violation-2-interdependence-1",
    "title": "Long Tails",
    "section": "Violation 2: Interdependence",
    "text": "Violation 2: Interdependence"
  },
  {
    "objectID": "slides/long-tails.html#violation-2-interdependence-2",
    "href": "slides/long-tails.html#violation-2-interdependence-2",
    "title": "Long Tails",
    "section": "Violation 2: Interdependence",
    "text": "Violation 2: Interdependence"
  },
  {
    "objectID": "slides/long-tails.html#violation-2-interdependence-3",
    "href": "slides/long-tails.html#violation-2-interdependence-3",
    "title": "Long Tails",
    "section": "Violation 2: Interdependence",
    "text": "Violation 2: Interdependence"
  },
  {
    "objectID": "slides/long-tails.html#violation-2-interdependence-4",
    "href": "slides/long-tails.html#violation-2-interdependence-4",
    "title": "Long Tails",
    "section": "Violation 2: Interdependence",
    "text": "Violation 2: Interdependence"
  },
  {
    "objectID": "slides/long-tails.html#violation-2-interdependence-5",
    "href": "slides/long-tails.html#violation-2-interdependence-5",
    "title": "Long Tails",
    "section": "Violation 2: Interdependence",
    "text": "Violation 2: Interdependence"
  },
  {
    "objectID": "slides/long-tails.html#violation-2-interdependence-6",
    "href": "slides/long-tails.html#violation-2-interdependence-6",
    "title": "Long Tails",
    "section": "Violation 2: Interdependence",
    "text": "Violation 2: Interdependence"
  },
  {
    "objectID": "slides/long-tails.html#power-laws",
    "href": "slides/long-tails.html#power-laws",
    "title": "Long Tails",
    "section": "Power Laws",
    "text": "Power Laws\nDistributions generated by this type of process are called power laws. They have a very distinct shape."
  },
  {
    "objectID": "slides/long-tails.html#power-laws-1",
    "href": "slides/long-tails.html#power-laws-1",
    "title": "Long Tails",
    "section": "Power Laws",
    "text": "Power Laws"
  },
  {
    "objectID": "slides/long-tails.html#power-laws-2",
    "href": "slides/long-tails.html#power-laws-2",
    "title": "Long Tails",
    "section": "Power Laws",
    "text": "Power Laws"
  },
  {
    "objectID": "slides/long-tails.html#power-laws-3",
    "href": "slides/long-tails.html#power-laws-3",
    "title": "Long Tails",
    "section": "Power Laws",
    "text": "Power Laws"
  },
  {
    "objectID": "slides/long-tails.html#power-laws-4",
    "href": "slides/long-tails.html#power-laws-4",
    "title": "Long Tails",
    "section": "Power Laws",
    "text": "Power Laws"
  },
  {
    "objectID": "slides/long-tails.html#power-laws-5",
    "href": "slides/long-tails.html#power-laws-5",
    "title": "Long Tails",
    "section": "Power Laws",
    "text": "Power Laws\n\nPower laws are the result of processes where large events are more likely grow than small ones.\nEach of these outcomes – social media subscribers, word usage, earthquakes, armed conflict – exhibit “snowball effects”.\nThis violates the independence assumption of CLT."
  },
  {
    "objectID": "slides/long-tails.html#takeaways",
    "href": "slides/long-tails.html#takeaways",
    "title": "Long Tails",
    "section": "Takeaways",
    "text": "Takeaways\n\nIf your outcome falls on a bell curve, then tail risk is low. Remember 68-95-99.7%.\nBut if the sum or independence assumptions of CLT are violated, expect to observe much longer tails.\nLognormal and power law distributions are dominated by extreme events, when occur much more frequently than the bell curve would suggest."
  },
  {
    "objectID": "slides/long-tails.html#references",
    "href": "slides/long-tails.html#references",
    "title": "Long Tails",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nTaleb, Nassim Nicholas. 2007. The black swan: the impact of the highly improbable. London: Allen Lane."
  },
  {
    "objectID": "slides/jury-theorems.html#warmup",
    "href": "slides/jury-theorems.html#warmup",
    "title": "Jury Theorems",
    "section": "Warmup",
    "text": "Warmup"
  },
  {
    "objectID": "slides/jury-theorems.html#warmup-1",
    "href": "slides/jury-theorems.html#warmup-1",
    "title": "Jury Theorems",
    "section": "Warmup",
    "text": "Warmup"
  },
  {
    "objectID": "slides/jury-theorems.html#warmup-2",
    "href": "slides/jury-theorems.html#warmup-2",
    "title": "Jury Theorems",
    "section": "Warmup",
    "text": "Warmup\nPonder: What if I asked the class to vote on the answers? How many correct answers do you think the majority would get?"
  },
  {
    "objectID": "slides/jury-theorems.html#warmup-3",
    "href": "slides/jury-theorems.html#warmup-3",
    "title": "Jury Theorems",
    "section": "Warmup",
    "text": "Warmup"
  },
  {
    "objectID": "slides/jury-theorems.html#warmup-4",
    "href": "slides/jury-theorems.html#warmup-4",
    "title": "Jury Theorems",
    "section": "Warmup",
    "text": "Warmup"
  },
  {
    "objectID": "slides/jury-theorems.html#warmup-5",
    "href": "slides/jury-theorems.html#warmup-5",
    "title": "Jury Theorems",
    "section": "Warmup",
    "text": "Warmup\nPonder: What if I only took the majority vote of students who were really confident in their answers (&gt;95%)? Would that result be better or worse?"
  },
  {
    "objectID": "slides/jury-theorems.html#warmup-6",
    "href": "slides/jury-theorems.html#warmup-6",
    "title": "Jury Theorems",
    "section": "Warmup",
    "text": "Warmup"
  },
  {
    "objectID": "slides/jury-theorems.html#warmup-7",
    "href": "slides/jury-theorems.html#warmup-7",
    "title": "Jury Theorems",
    "section": "Warmup",
    "text": "Warmup"
  },
  {
    "objectID": "slides/jury-theorems.html#wisdom-of-crowds",
    "href": "slides/jury-theorems.html#wisdom-of-crowds",
    "title": "Jury Theorems",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds\n\nThis phenomenon is known as the Wisdom of Crowds.\n\nGroups of people often outperform the individuals that make them up.\n\nOver the next few weeks, we’ll discuss how to harness the Wisdom of Crowds to make better predictions.\nWe’ll also explore the conditions under which groups of people perform worse than individuals (the Madness of Crowds)."
  },
  {
    "objectID": "slides/jury-theorems.html#marquis-de-condorcet-1743-1794",
    "href": "slides/jury-theorems.html#marquis-de-condorcet-1743-1794",
    "title": "Jury Theorems",
    "section": "Marquis de Condorcet (1743-1794)",
    "text": "Marquis de Condorcet (1743-1794)"
  },
  {
    "objectID": "slides/jury-theorems.html#condorcet-jury-theorem",
    "href": "slides/jury-theorems.html#condorcet-jury-theorem",
    "title": "Jury Theorems",
    "section": "Condorcet Jury Theorem",
    "text": "Condorcet Jury Theorem\n\nAssumptions:\n\nA group is voting on a binary (Yes/No) decision.\nEach voter has probability \\(p &gt; \\frac{1}{2}\\) of making the correct choice.\nIndividual votes are independent of one another.\n\n\n\nTheorem:\n\nAs the size of the group \\(n\\) gets large:\n\nThe probability that the majority makes the correct choice approaches 100%."
  },
  {
    "objectID": "slides/jury-theorems.html#condorcet-jury-theorem-1",
    "href": "slides/jury-theorems.html#condorcet-jury-theorem-1",
    "title": "Jury Theorems",
    "section": "Condorcet Jury Theorem",
    "text": "Condorcet Jury Theorem\nPonder: Does the Condorcet Jury Theorem remind you of anything?"
  },
  {
    "objectID": "slides/jury-theorems.html#condorcet-jury-theorem-2",
    "href": "slides/jury-theorems.html#condorcet-jury-theorem-2",
    "title": "Jury Theorems",
    "section": "Condorcet Jury Theorem",
    "text": "Condorcet Jury Theorem\nExample: A jury with 5 members, where each individual has a 60% probability of choosing the right answer.\n\n\n\n\n\n\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\nStart Voting\n\n\n\n1R,0W\n\n\n\n0R,1W\n\n\n\n2R,0W\n\n\n\n1R,1W\n\n\n\n0R,2W\n\n\n\n3R,0W\n\n\n\n2R,1W\n\n\n\n1R,2W\n\n\n\n0R,3W\n\n\n\n4R,0W\n\n\n\n3R,1W\n\n\n\n2R,2W\n\n\n\n1R,3W\n\n\n\n0R,4W\n\n\n\n5R,0W (7.8%)\n\n\n\n4R,1W (25.9%)\n\n\n\n3R,2W (34.6%)\n\n\n\n2R,3W (23%)\n\n\n\n1R,4W (7.7%)\n\n\n\n0R,5W (1%)"
  },
  {
    "objectID": "slides/jury-theorems.html#condorcet-jury-theorem-3",
    "href": "slides/jury-theorems.html#condorcet-jury-theorem-3",
    "title": "Jury Theorems",
    "section": "Condorcet Jury Theorem",
    "text": "Condorcet Jury Theorem"
  },
  {
    "objectID": "slides/jury-theorems.html#condorcet-jury-theorem-4",
    "href": "slides/jury-theorems.html#condorcet-jury-theorem-4",
    "title": "Jury Theorems",
    "section": "Condorcet Jury Theorem",
    "text": "Condorcet Jury Theorem"
  },
  {
    "objectID": "slides/jury-theorems.html#condorcet-jury-theorem-5",
    "href": "slides/jury-theorems.html#condorcet-jury-theorem-5",
    "title": "Jury Theorems",
    "section": "Condorcet Jury Theorem",
    "text": "Condorcet Jury Theorem"
  },
  {
    "objectID": "slides/jury-theorems.html#condorcet-jury-theorem-6",
    "href": "slides/jury-theorems.html#condorcet-jury-theorem-6",
    "title": "Jury Theorems",
    "section": "Condorcet Jury Theorem",
    "text": "Condorcet Jury Theorem"
  },
  {
    "objectID": "slides/jury-theorems.html#condorcet-jury-theorem-7",
    "href": "slides/jury-theorems.html#condorcet-jury-theorem-7",
    "title": "Jury Theorems",
    "section": "Condorcet Jury Theorem",
    "text": "Condorcet Jury Theorem"
  },
  {
    "objectID": "slides/jury-theorems.html#condorcet-jury-theorem-8",
    "href": "slides/jury-theorems.html#condorcet-jury-theorem-8",
    "title": "Jury Theorems",
    "section": "Condorcet Jury Theorem",
    "text": "Condorcet Jury Theorem"
  },
  {
    "objectID": "slides/jury-theorems.html#condorcet-jury-theorem-9",
    "href": "slides/jury-theorems.html#condorcet-jury-theorem-9",
    "title": "Jury Theorems",
    "section": "Condorcet Jury Theorem",
    "text": "Condorcet Jury Theorem"
  },
  {
    "objectID": "slides/jury-theorems.html#condorcet-jury-theorem-10",
    "href": "slides/jury-theorems.html#condorcet-jury-theorem-10",
    "title": "Jury Theorems",
    "section": "Condorcet Jury Theorem",
    "text": "Condorcet Jury Theorem\n\nMajority rule is a powerful method for finding the right answer, if:\n\nIndividuals are competent \\((p &gt; \\frac{1}{2})\\).\nChoices are made independently.\nThe group is large enough."
  },
  {
    "objectID": "slides/jury-theorems.html#wisdom-of-crowds-1",
    "href": "slides/jury-theorems.html#wisdom-of-crowds-1",
    "title": "Jury Theorems",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds\n\nOne last wrinkle: what if the decision being made isn’t binary (Yes/No)?\nWhat if the group is trying to make a decision about a continuous value?\nConsider the ox weight-judging competition described in Tetlock Chapter 3.\nParticipants paid sixpence to enter a competition to guess the weight of a “fat ox” (Galton 1907)."
  },
  {
    "objectID": "slides/jury-theorems.html#wisdom-of-crowds-2",
    "href": "slides/jury-theorems.html#wisdom-of-crowds-2",
    "title": "Jury Theorems",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds\n\n\nWhat is the “majority judgment” here?"
  },
  {
    "objectID": "slides/jury-theorems.html#wisdom-of-crowds-3",
    "href": "slides/jury-theorems.html#wisdom-of-crowds-3",
    "title": "Jury Theorems",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds\nImagine we broke this down into a series of binary (Yes/No) votes."
  },
  {
    "objectID": "slides/jury-theorems.html#wisdom-of-crowds-4",
    "href": "slides/jury-theorems.html#wisdom-of-crowds-4",
    "title": "Jury Theorems",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds\nIf the majority says “it weighs more than that”, adjust the question higher:"
  },
  {
    "objectID": "slides/jury-theorems.html#wisdom-of-crowds-5",
    "href": "slides/jury-theorems.html#wisdom-of-crowds-5",
    "title": "Jury Theorems",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds\nIf the majority says “it weighs less than that”, adjust the question lower:"
  },
  {
    "objectID": "slides/jury-theorems.html#wisdom-of-crowds-6",
    "href": "slides/jury-theorems.html#wisdom-of-crowds-6",
    "title": "Jury Theorems",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds\nThis process will finally stop when we get to the median voter (Black 1948)."
  },
  {
    "objectID": "slides/jury-theorems.html#wisdom-of-crowds-7",
    "href": "slides/jury-theorems.html#wisdom-of-crowds-7",
    "title": "Jury Theorems",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds\nIn Galton’s experiment, the median voter was off by only 9 pounds!"
  },
  {
    "objectID": "slides/jury-theorems.html#median-voter-theorem",
    "href": "slides/jury-theorems.html#median-voter-theorem",
    "title": "Jury Theorems",
    "section": "Median Voter Theorem",
    "text": "Median Voter Theorem\n\nThe median voter theorem (Black 1948) says that the median is the only position that cannot be defeated by a majority vote.\nThis follows from the definition of the median:\n\n50% of the group lies to the right.\n50% of the group lies to the left.\nAny majority voting bloc (&gt;50%) must include the median.\n\nSo we can think of the “majority judgment” as the position of the median voter."
  },
  {
    "objectID": "slides/jury-theorems.html#takeaways",
    "href": "slides/jury-theorems.html#takeaways",
    "title": "Jury Theorems",
    "section": "Takeaways",
    "text": "Takeaways\n\nThe Condorcet Jury Theorem shows when we can expect majority rule to yield good judgments.\n\nIndividuals must be competent/independent, and you need a large group.\n\nThe median voter theorem says that, when you’re making judgments on a continuous spectrum, majority = median.\n\nThis is why the “Crowd” point on our course website is the median forecast.\n\nNext Time: What to do when we don’t quite believe that independence assumption…"
  },
  {
    "objectID": "slides/jury-theorems.html#references",
    "href": "slides/jury-theorems.html#references",
    "title": "Jury Theorems",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBlack, Duncan. 1948. “On the Rationale of Group Decision-Making.” Journal of Political Economy 56 (1): 23–34. https://doi.org/10.1086/256633.\n\n\nGalton, Francis. 1907. “Vox Populi.” Nature 75 (1949): 450–51. https://doi.org/10.1038/075450a0."
  },
  {
    "objectID": "slides/fundamentals.html#prediction-is-hard",
    "href": "slides/fundamentals.html#prediction-is-hard",
    "title": "Fudamentals of Probability",
    "section": "Prediction Is Hard",
    "text": "Prediction Is Hard\n\n\n\n\n“Stocks have reached what looks like a permanently high plateau.”\n—Irving Fisher, Economist, October 1929"
  },
  {
    "objectID": "slides/fundamentals.html#prediction-is-hard-1",
    "href": "slides/fundamentals.html#prediction-is-hard-1",
    "title": "Fudamentals of Probability",
    "section": "Prediction Is Hard",
    "text": "Prediction Is Hard\n\n\n\n“I believe it is peace for our time.”\n—Neville Chamberlain, September 30, 1938\n\n\n\n\n\nHitler invaded Poland less than a year later, on September 1, 1939."
  },
  {
    "objectID": "slides/fundamentals.html#prediction-is-hard-2",
    "href": "slides/fundamentals.html#prediction-is-hard-2",
    "title": "Fudamentals of Probability",
    "section": "Prediction Is Hard",
    "text": "Prediction Is Hard\n\n\n\n“There’s no chance that the iPhone is going to get any significant market share. No chance.”\n—Steve Ballmer, Microsoft CEO, 2007"
  },
  {
    "objectID": "slides/fundamentals.html#prediction-is-hard-3",
    "href": "slides/fundamentals.html#prediction-is-hard-3",
    "title": "Fudamentals of Probability",
    "section": "Prediction Is Hard",
    "text": "Prediction Is Hard"
  },
  {
    "objectID": "slides/fundamentals.html#prediction-is-hard-4",
    "href": "slides/fundamentals.html#prediction-is-hard-4",
    "title": "Fudamentals of Probability",
    "section": "Prediction Is Hard",
    "text": "Prediction Is Hard\n\n\n\n“The overall risk to the American public does remain low.”\n—CDC Director Robert Redfield, March 6, 2020\n\n\n\n\n\nExactly one week before the White House declared a national emergency."
  },
  {
    "objectID": "slides/fundamentals.html#prediction-is-hard-5",
    "href": "slides/fundamentals.html#prediction-is-hard-5",
    "title": "Fudamentals of Probability",
    "section": "Prediction Is Hard",
    "text": "Prediction Is Hard\n\n\nFor those keeping track at home, that’s 15 days before the official start of the invasion. The picture is of a convoy of Russian armored vehicles heading towards Ukraine."
  },
  {
    "objectID": "slides/fundamentals.html#prediction-is-hard-6",
    "href": "slides/fundamentals.html#prediction-is-hard-6",
    "title": "Fudamentals of Probability",
    "section": "Prediction Is Hard",
    "text": "Prediction Is Hard\n\nPredictions guide important decisions—in politics, public policy, business, and daily life.\nOverconfidence and neglect of uncertainty can have serious consequences.\nThe future is inherently uncertain—we need better ways to think about and communicate uncertainty."
  },
  {
    "objectID": "slides/fundamentals.html#probability-1",
    "href": "slides/fundamentals.html#probability-1",
    "title": "Fudamentals of Probability",
    "section": "Probability",
    "text": "Probability\n\nThe language of uncertainty\nAllows us to communicate clearly and precisely how certain we are that an event will happen."
  },
  {
    "objectID": "slides/fundamentals.html#probability-2",
    "href": "slides/fundamentals.html#probability-2",
    "title": "Fudamentals of Probability",
    "section": "Probability",
    "text": "Probability\nIn some contexts, probability statements seem natural:\n\n40% chance of rain on Tuesday\n12% chance the Dodgers win the World Series\n2.7% chance of rolling double sixes\n\n\nIn other contexts, feels weird:\n\n\n\n“I believe there is 42% chance of peace in our time!”\n-Neville Chamberlain, September 30, 1938\n\n\n\n(My goal is to get you comfortable with such statements.)"
  },
  {
    "objectID": "slides/fundamentals.html#definitions",
    "href": "slides/fundamentals.html#definitions",
    "title": "Fudamentals of Probability",
    "section": "Definitions",
    "text": "Definitions\n\nA probability is a number between 0 and 1 that expresses how likely it is that an event will happen.\n\n\n\n0% means “impossible”\n50% means “equally likely to happen or not”\n100% means “certain”\n\n\n\nDeveloping an intuition for other probability values is trickier…"
  },
  {
    "objectID": "slides/fundamentals.html#definitions-1",
    "href": "slides/fundamentals.html#definitions-1",
    "title": "Fudamentals of Probability",
    "section": "Definitions",
    "text": "Definitions\n\nMany of us develop an intuition for probability by playing games of chance."
  },
  {
    "objectID": "slides/fundamentals.html#definitions-2",
    "href": "slides/fundamentals.html#definitions-2",
    "title": "Fudamentals of Probability",
    "section": "Definitions",
    "text": "Definitions\n\nIf you flip a coin 200 times, you’d expect it to come up heads roughly half the time.\nThis illustrates the frequentist interpretation of probability:\n\nProbability = how frequently we expect an event to occur in a large number of repeated trials.\n\nFormalized in the Law of Large Numbers:\n\nIf the probability of an event is \\(p\\), then the proportion of times that event occurs will approach \\(p\\) in a (very) large number of repeated trials."
  },
  {
    "objectID": "slides/fundamentals.html#law-of-large-numbers",
    "href": "slides/fundamentals.html#law-of-large-numbers",
    "title": "Fudamentals of Probability",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\nThe entire business model of casinos/lotteries rests on the Law of Large Numbers.\n\n\n\n\n\nMaybe this guy comes in one night and puts $35,000 on lucky number 32. If 32 hits, the casino pays out over $1.2 million!"
  },
  {
    "objectID": "slides/fundamentals.html#law-of-large-numbers-1",
    "href": "slides/fundamentals.html#law-of-large-numbers-1",
    "title": "Fudamentals of Probability",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\nBut if you get 10,000 of those guys placing bets like that, 37 lose for every 1 that wins…"
  },
  {
    "objectID": "slides/fundamentals.html#definitions-3",
    "href": "slides/fundamentals.html#definitions-3",
    "title": "Fudamentals of Probability",
    "section": "Definitions",
    "text": "Definitions\nSometimes it will be useful to express probability as odds.\n\n50% \\(\\rightarrow\\) 1:1 (“equally likely to happen or not”)\n75% \\(\\rightarrow\\) 3:1 (“three times more likely to happen than not”)\n10% \\(\\rightarrow\\) 1:9 (“nine times less likely to happen than not”)\n\n\nTo convert probability \\(p\\) to odds, divide \\(\\frac{p}{1-p}\\).\n\n\nTo convert odds \\(p:q\\) to probability, divide \\(\\frac{p}{p+q}\\)."
  },
  {
    "objectID": "slides/fundamentals.html#practice-problems",
    "href": "slides/fundamentals.html#practice-problems",
    "title": "Fudamentals of Probability",
    "section": "Practice Problems",
    "text": "Practice Problems\n\nWhat is 80% expressed as odds?\nWhat is 2:1 expressed as a probability?"
  },
  {
    "objectID": "slides/fundamentals.html#what-if-the-event-cant-be-repeated",
    "href": "slides/fundamentals.html#what-if-the-event-cant-be-repeated",
    "title": "Fudamentals of Probability",
    "section": "What if the event can’t be repeated?",
    "text": "What if the event can’t be repeated?\n\nFor events that can be repeated a large number of times, frequentist interpretation is pretty intuitive.\n\nBut what about the sorts of questions I asked in the intro survey?"
  },
  {
    "objectID": "slides/fundamentals.html#subjective-bayesian-interpretation",
    "href": "slides/fundamentals.html#subjective-bayesian-interpretation",
    "title": "Fudamentals of Probability",
    "section": "Subjective (Bayesian) Interpretation",
    "text": "Subjective (Bayesian) Interpretation\n\nProbability is not an innate characteristic of the event, but a statement about the observer’s knowledge."
  },
  {
    "objectID": "slides/fundamentals.html#subjective-bayesian-interpretation-1",
    "href": "slides/fundamentals.html#subjective-bayesian-interpretation-1",
    "title": "Fudamentals of Probability",
    "section": "Subjective (Bayesian) Interpretation",
    "text": "Subjective (Bayesian) Interpretation\n\nIf you can precisely measure a coin’s initial velocity and rate of spin, you can perfectly predict what side it will land on (Diaconis et al. 2007)."
  },
  {
    "objectID": "slides/fundamentals.html#subjective-bayesian-interpretation-2",
    "href": "slides/fundamentals.html#subjective-bayesian-interpretation-2",
    "title": "Fudamentals of Probability",
    "section": "Subjective (Bayesian) Interpretation",
    "text": "Subjective (Bayesian) Interpretation\nProbability is not an innate characteristic of the event, but a statement about the observer’s knowledge.\n\nProbability = “degree of belief”\nA probability statement (e.g. 80% chance of rain) communicates the forecaster’s confidence that the event will occur.\n\n“Based on the information I have, I think it is four times more likely to rain than to not rain.”"
  },
  {
    "objectID": "slides/fundamentals.html#are-people-good-at-this",
    "href": "slides/fundamentals.html#are-people-good-at-this",
    "title": "Fudamentals of Probability",
    "section": "Are people good at this?",
    "text": "Are people good at this?\n\nNo. Assessing probability is not something that comes naturally to people."
  },
  {
    "objectID": "slides/fundamentals.html#mental-trick-to-improve-calibration",
    "href": "slides/fundamentals.html#mental-trick-to-improve-calibration",
    "title": "Fudamentals of Probability",
    "section": "Mental Trick to Improve Calibration",
    "text": "Mental Trick to Improve Calibration\n\n\n\n\nReframe the problem as a bet.\n“Would I rather have $100 if Karl Marx was born before Queen Victoria or if I draw a red ball from this jar?”\nAlter the contents of the jar until you’re indifferent between the two bets.\nThis is called the Equivalent Bet Test (Spetzler & Staël Von Holstein, 1975; Galef, 2021)"
  },
  {
    "objectID": "slides/fundamentals.html#practice",
    "href": "slides/fundamentals.html#practice",
    "title": "Fudamentals of Probability",
    "section": "Practice",
    "text": "Practice\n\nTake 10 minutes to play “Two Truths and a Lie” with your tablemates. Pick a person to tell two truths and a lie about themselves, and everyone else must guess which is the lie.\nOur twist on the game is that everyone must estimate a probability that their chosen statement is a lie. Use the Equivalent Bet Test to help calibrate your probability.\n(Feel free to play multiple rounds if there is time remaining.)"
  },
  {
    "objectID": "slides/conditional-probability.html#todays-agenda",
    "href": "slides/conditional-probability.html#todays-agenda",
    "title": "Conditional Probability",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nLearn how to estimate conditional probability\nIntroduce a common approach used by successful forecasters: balancing Inside View and Outside View.\nShow how all this connects to a central problem in statistics: the bias-variance tradeoff."
  },
  {
    "objectID": "slides/conditional-probability.html#motivating-problem",
    "href": "slides/conditional-probability.html#motivating-problem",
    "title": "Conditional Probability",
    "section": "Motivating Problem",
    "text": "Motivating Problem\nWill the movie Roofman be “Certified Fresh” by Rotten Tomatoes?\n\n\nHow would you go about this prediction? What information would you research first?"
  },
  {
    "objectID": "slides/conditional-probability.html#two-different-approaches",
    "href": "slides/conditional-probability.html#two-different-approaches",
    "title": "Conditional Probability",
    "section": "Two Different Approaches",
    "text": "Two Different Approaches\n\nInside View: Focus on the details of the case at hand.\n\nWhat characteristics of Roofman suggest that it will be Certified Fresh?\n\nOutside View: Ignore the details of the case at hand. Focus on what happened previously with similar cases.\n\nHow often does any film achieve a Certified Fresh rating?\n\nMost people naturally gravitate towards the Inside View (Kahneman and Lovallo 1993), but the best forecasters combine both approaches!"
  },
  {
    "objectID": "slides/conditional-probability.html#outside-view",
    "href": "slides/conditional-probability.html#outside-view",
    "title": "Conditional Probability",
    "section": "Outside View",
    "text": "Outside View\n\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Tomatometer\n                Total\n                %\n              \n        \n        \n        \n                \n                  Certified-Fresh\n                  3,259\n                  18.4\n                \n                \n                  Fresh\n                  6,844\n                  38.7\n                \n                \n                  Rotten\n                  7,565\n                  42.8"
  },
  {
    "objectID": "slides/conditional-probability.html#outside-view-1",
    "href": "slides/conditional-probability.html#outside-view-1",
    "title": "Conditional Probability",
    "section": "Outside View",
    "text": "Outside View\n\nThis base rate is a useful starting point.\n\nAchieving a Certified Fresh rating is historically very rare. Serves as an “anchor” for our prediction.\n\nWhat’s the problem with this approach? Why not stop there?\n\nThe pure Outside View yields a biased prediction, because it doesn’t incorporate any information about the case in question.\nMore details about the movie Roofman should help us refine our prediction."
  },
  {
    "objectID": "slides/conditional-probability.html#unconditional-probability",
    "href": "slides/conditional-probability.html#unconditional-probability",
    "title": "Conditional Probability",
    "section": "Unconditional Probability",
    "text": "Unconditional Probability\n\n\n\n\n\n\n\n\n\n18.5%\n\n\n\n81.5%\n\n\n\nAll Movies\n\n\n\nCertified Fresh\n\n\n\nNot CF"
  },
  {
    "objectID": "slides/conditional-probability.html#conditional-probability",
    "href": "slides/conditional-probability.html#conditional-probability",
    "title": "Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\n\n\n\n\n\n\n\n36%\n\n\n\n64%\n\n\n\n22%\n\n\n\n78%\n\n\n\n16.5%\n\n\n\n83.5%\n\n\n\nAll Movies\n\n\n\nRated R\n\n\n\nNot Rated R\n\n\n\nCertified Fresh\n\n\n\nNot CF\n\n\n\nCertified Fresh\n\n\n\nNot CF"
  },
  {
    "objectID": "slides/conditional-probability.html#conditional-probability-1",
    "href": "slides/conditional-probability.html#conditional-probability-1",
    "title": "Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability"
  },
  {
    "objectID": "slides/conditional-probability.html#conditional-probability-2",
    "href": "slides/conditional-probability.html#conditional-probability-2",
    "title": "Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\n\n\n\n\n\n\n\n36%\n\n\n\n64%\n\n\n\n63.4%\n\n\n\n36.6%\n\n\n\n25.5%\n\n\n\n74.5%\n\n\n\nAll Movies\n\n\n\nRated R\n\n\n\nNot Rated R\n\n\n\nReleased After 2000\n\n\n\nReleased Before 2000\n\n\n\nCertified Fresh\n\n\n\nNot CF"
  },
  {
    "objectID": "slides/conditional-probability.html#conditional-probability-3",
    "href": "slides/conditional-probability.html#conditional-probability-3",
    "title": "Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability"
  },
  {
    "objectID": "slides/conditional-probability.html#conditional-probability-4",
    "href": "slides/conditional-probability.html#conditional-probability-4",
    "title": "Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability"
  },
  {
    "objectID": "slides/conditional-probability.html#conditional-probability-5",
    "href": "slides/conditional-probability.html#conditional-probability-5",
    "title": "Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability"
  },
  {
    "objectID": "slides/conditional-probability.html#conditional-probability-6",
    "href": "slides/conditional-probability.html#conditional-probability-6",
    "title": "Conditional Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nHow far should we take this???\n\n\n\n\n\n\n\n\n\n\n36%\n\n\n\n64%\n\n\n\n63.4%\n\n\n\n36.6%\n\n\n\n0.05%\n\n\n\n99.95%\n\n\n\n100%\n\n\n\n0%\n\n\n\nAll Movies\n\n\n\nRated R\n\n\n\nNot Rated R\n\n\n\nReleased After 2000\n\n\n\nReleased Before 2000\n\n\n\nDirected by Cianfrance\n\n\n\nNot Cianfrance\n\n\n\nCertified Fresh\n\n\n\nNot CF"
  },
  {
    "objectID": "slides/conditional-probability.html#inside-view",
    "href": "slides/conditional-probability.html#inside-view",
    "title": "Conditional Probability",
    "section": "Inside View",
    "text": "Inside View\n\nThe “Inside View” is very confident about Roofman. R-movies directed by Derek Cianfrance are always Certified Fresh.\nWhat’s the problem with this approach?"
  },
  {
    "objectID": "slides/conditional-probability.html#inside-view-1",
    "href": "slides/conditional-probability.html#inside-view-1",
    "title": "Conditional Probability",
    "section": "Inside View",
    "text": "Inside View\n\nWell, there are only 3 films in the entire database directed by Derek Cianfrance."
  },
  {
    "objectID": "slides/conditional-probability.html#practice",
    "href": "slides/conditional-probability.html#practice",
    "title": "Conditional Probability",
    "section": "Practice",
    "text": "Practice\nIs this strong evidence that Derek Cianfrance is an above-average director? \\(P(\\text{Certified Fresh}) &gt; 0.25\\)?\n\n\nIf Cianfrance was just an average director, what is the probability that all three of his films would be Certified Fresh?\nIf Cianfrance was just an average director, what is the probability that at least 2 out of his 3 films would be Certified Fresh?"
  },
  {
    "objectID": "slides/conditional-probability.html#practice-1",
    "href": "slides/conditional-probability.html#practice-1",
    "title": "Conditional Probability",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\n\n\n\n25%\n\n\n\n75%\n\n\n\n25%\n\n\n\n75%\n\n\n\n25%\n\n\n\n75%\n\n\n\n25%\n\n\n\n75%\n\n\n\n25%\n\n\n\n75%\n\n\n\n25%\n\n\n\n75%\n\n\n\n25%\n\n\n\n75%\n\n\n\nStart of Career\n\n\n\nFilm 1: Fresh\n\n\n\nFilm 1: Not Fresh\n\n\n\nFilm 2: Fresh\n\n\n\nFilm 2: Not Fresh\n\n\n\nFilm 2: Fresh\n\n\n\nFilm 2: Not Fresh\n\n\n\nFilm 3: Fresh (1.5%)\n\n\n\nFilm 3: Not Fresh (4.7%)\n\n\n\nFilm 3: Fresh (4.7%)\n\n\n\nFilm 3: Not Fresh (14.1%)\n\n\n\nFilm 3: Fresh (4.7%)\n\n\n\nFilm 3: Not Fresh (14.1%)\n\n\n\nFilm 3: Fresh (14.1%)\n\n\n\nFilm 3: Not Fresh (42.2%)"
  },
  {
    "objectID": "slides/conditional-probability.html#inside-view-2",
    "href": "slides/conditional-probability.html#inside-view-2",
    "title": "Conditional Probability",
    "section": "Inside View",
    "text": "Inside View\n\nWith only three data points, we cannot claim with confidence that Cianfrance’s next movie has a \\(\\frac{2}{3}\\) probability of achieving Certified Freshness.\nIt could be that his movies are better-than-average.\nBut he also could have gotten lucky."
  },
  {
    "objectID": "slides/conditional-probability.html#bias-variance-tradeoff",
    "href": "slides/conditional-probability.html#bias-variance-tradeoff",
    "title": "Conditional Probability",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\nOn one end of the spectrum (pure Outside View), we have a biased but precise prediction.\n\n18.5% of all movies are Certified Fresh.\n\nOn the opposite end (pure Inside View), we have an less-biased but highly uncertain prediction.\n\n66.6% of Derek Cianfrance’s movies are Certified Fresh.\n\nThe truth probably lies somewhere in the middle.\nThe “art” of forecasting is learning how to balance this tradeoff. How much should you weigh the Outside View vs. the Inside View?"
  },
  {
    "objectID": "slides/conditional-probability.html#next-time",
    "href": "slides/conditional-probability.html#next-time",
    "title": "Conditional Probability",
    "section": "Next Time",
    "text": "Next Time\nBayes Rule as a method for computing conditional probability, and adjusting your predictions in light of available evidence."
  },
  {
    "objectID": "slides/conditional-probability.html#review-problems",
    "href": "slides/conditional-probability.html#review-problems",
    "title": "Conditional Probability",
    "section": "Review Problems",
    "text": "Review Problems\n\nA basketball player with an 80% free throw shooting rate goes to the line to shoot 3 free throws. What’s the probability she makes 1 or fewer?\nYou’re volunteering with a phone bank for a political campaign. People only answer their phone 10% of the time. Out of the next 3 numbers on your call sheet, what’s the probability that anyone answers your call?"
  },
  {
    "objectID": "slides/conditional-probability.html#review-problems-1",
    "href": "slides/conditional-probability.html#review-problems-1",
    "title": "Conditional Probability",
    "section": "Review Problems",
    "text": "Review Problems\n\nLet’s slightly modify the movie director problem from before. The average director starts out with a 25% chance of making a Certified Fresh film. But that probability can go up or down over time. If the director makes a Certified Fresh film, he gets a boost of morale, and the probability increases by 10% for the next film he directs. Otherwise, he takes a hit to morale, and the probability decreases by 10% for the next film he directs. What’s the probability that a director gets at least 2 Certified Fresh films out of his first 3?"
  },
  {
    "objectID": "slides/conditional-probability.html#references",
    "href": "slides/conditional-probability.html#references",
    "title": "Conditional Probability",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nKahneman, Daniel, and Dan Lovallo. 1993. “Timid Choices and Bold Forecasts: A Cognitive Perspective on Risk Taking.” Management Science 39 (1): 17–31. https://www.jstor.org/stable/2661517."
  },
  {
    "objectID": "slides/brier-scores.html#todays-agenda",
    "href": "slides/brier-scores.html#todays-agenda",
    "title": "Brier Scores",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nWhat is the best way to evaluate probabilistic predictions?\nIntroduce Brier Scores, the method we’ll use to assess accuracy in our forecasting challenge this semester."
  },
  {
    "objectID": "slides/brier-scores.html#warmup",
    "href": "slides/brier-scores.html#warmup",
    "title": "Brier Scores",
    "section": "Warmup",
    "text": "Warmup\nWas this a bad prediction?\n\n\n\n“There’s no chance that the iPhone is going to get any significant market share. No chance.”\n—Steve Ballmer, Microsoft CEO, 2007"
  },
  {
    "objectID": "slides/brier-scores.html#warmup-1",
    "href": "slides/brier-scores.html#warmup-1",
    "title": "Brier Scores",
    "section": "Warmup",
    "text": "Warmup\nWhat about this prediction?"
  },
  {
    "objectID": "slides/brier-scores.html#warmup-2",
    "href": "slides/brier-scores.html#warmup-2",
    "title": "Brier Scores",
    "section": "Warmup",
    "text": "Warmup\nWhat about this one?"
  },
  {
    "objectID": "slides/brier-scores.html#scoring-predictions",
    "href": "slides/brier-scores.html#scoring-predictions",
    "title": "Brier Scores",
    "section": "Scoring Predictions",
    "text": "Scoring Predictions\n\nEvaluating individual probability statements is quite tricky.\nIf something happens that the forecaster said was unlikely, was the forecaster wrong or unlucky?\nMuch better to evaluate a forecaster’s track record over many predictions.\n\nWhen the forecaster says 70% chance, does the event happen about 70% of the time?"
  },
  {
    "objectID": "slides/brier-scores.html#calibration",
    "href": "slides/brier-scores.html#calibration",
    "title": "Brier Scores",
    "section": "Calibration",
    "text": "Calibration\nAcross thousands of predictions about politics and sports, Nate Silver has a pretty impressive track record…"
  },
  {
    "objectID": "slides/brier-scores.html#scoring-predictions-1",
    "href": "slides/brier-scores.html#scoring-predictions-1",
    "title": "Brier Scores",
    "section": "Scoring Predictions",
    "text": "Scoring Predictions\n\nCalibration isn’t the only thing we care about, though.\n\nA doctor who tells his pregnant patient there is a 50% chance the baby will be a boy is perfectly-calibrated…but the forecast isn’t terribly useful.\nThe ultrasound tech who can predict with100% confidence is much more useful. Same level of calibration though!\n\n\n\nWe want forecasts that are both well-calibrated and as confident as possible."
  },
  {
    "objectID": "slides/brier-scores.html#scoring-predictions-2",
    "href": "slides/brier-scores.html#scoring-predictions-2",
    "title": "Brier Scores",
    "section": "Scoring Predictions",
    "text": "Scoring Predictions\n\n\n\nConsider a scoring system where we evaluate predictions based on average error.\nThis has some nice properties: you score better if you are confidently correct, and score worse if you are confidently incorrect.\n\n\n\n\n\nPrediction\nOutcome\nError\n\n\n\n\n70%\n1\n0.3\n\n\n20%\n0\n0.2\n\n\n70%\n0\n0.7\n\n\n40%\n1\n0.6\n\n\n\n\nBut there’s a huge problem with this scoring system…it encourages lying!"
  },
  {
    "objectID": "slides/brier-scores.html#average-error-is-a-bad-score",
    "href": "slides/brier-scores.html#average-error-is-a-bad-score",
    "title": "Brier Scores",
    "section": "Average Error is a Bad Score",
    "text": "Average Error is a Bad Score\n\nSuppose you are a TV meteorologist.\nYour weather model says it’s going to be a rainy week. 80% chance of rain each day.\nWhat do you report to your viewers if you want the best average error for the week?"
  },
  {
    "objectID": "slides/brier-scores.html#average-error-is-a-bad-score-1",
    "href": "slides/brier-scores.html#average-error-is-a-bad-score-1",
    "title": "Brier Scores",
    "section": "Average Error is a Bad Score",
    "text": "Average Error is a Bad Score"
  },
  {
    "objectID": "slides/brier-scores.html#average-error-is-a-bad-score-2",
    "href": "slides/brier-scores.html#average-error-is-a-bad-score-2",
    "title": "Brier Scores",
    "section": "Average Error is a Bad Score",
    "text": "Average Error is a Bad Score"
  },
  {
    "objectID": "slides/brier-scores.html#average-error-is-a-bad-score-3",
    "href": "slides/brier-scores.html#average-error-is-a-bad-score-3",
    "title": "Brier Scores",
    "section": "Average Error is a Bad Score",
    "text": "Average Error is a Bad Score"
  },
  {
    "objectID": "slides/brier-scores.html#average-error-is-a-bad-score-4",
    "href": "slides/brier-scores.html#average-error-is-a-bad-score-4",
    "title": "Brier Scores",
    "section": "Average Error is a Bad Score",
    "text": "Average Error is a Bad Score"
  },
  {
    "objectID": "slides/brier-scores.html#brier-scores",
    "href": "slides/brier-scores.html#brier-scores",
    "title": "Brier Scores",
    "section": "Brier Scores",
    "text": "Brier Scores\n\nWe would like a scoring rule that encourages honesty (a strictly proper scoring rule).\nPenalizing extremely wrong predictions helps.\nThe Brier Score does this by taking the average squared error (Brier, 1950)."
  },
  {
    "objectID": "slides/brier-scores.html#why-squared-error",
    "href": "slides/brier-scores.html#why-squared-error",
    "title": "Brier Scores",
    "section": "Why Squared Error?",
    "text": "Why Squared Error?\n\nNotice that the penalty is particularly steep when predictions is wrong and overconfident."
  },
  {
    "objectID": "slides/brier-scores.html#brier-scores-1",
    "href": "slides/brier-scores.html#brier-scores-1",
    "title": "Brier Scores",
    "section": "Brier Scores",
    "text": "Brier Scores"
  },
  {
    "objectID": "slides/brier-scores.html#brier-scores-2",
    "href": "slides/brier-scores.html#brier-scores-2",
    "title": "Brier Scores",
    "section": "Brier Scores",
    "text": "Brier Scores"
  },
  {
    "objectID": "slides/brier-scores.html#brier-scores-3",
    "href": "slides/brier-scores.html#brier-scores-3",
    "title": "Brier Scores",
    "section": "Brier Scores",
    "text": "Brier Scores"
  },
  {
    "objectID": "slides/brier-scores.html#brier-scores-4",
    "href": "slides/brier-scores.html#brier-scores-4",
    "title": "Brier Scores",
    "section": "Brier Scores",
    "text": "Brier Scores"
  },
  {
    "objectID": "slides/brier-scores.html#brier-scores-5",
    "href": "slides/brier-scores.html#brier-scores-5",
    "title": "Brier Scores",
    "section": "Brier Scores",
    "text": "Brier Scores"
  },
  {
    "objectID": "slides/brier-scores.html#brier-scores-6",
    "href": "slides/brier-scores.html#brier-scores-6",
    "title": "Brier Scores",
    "section": "Brier Scores",
    "text": "Brier Scores"
  },
  {
    "objectID": "slides/brier-scores.html#key-takeaways",
    "href": "slides/brier-scores.html#key-takeaways",
    "title": "Brier Scores",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nBrier Scores are a sort of mathematical truth serum.\nYour optimal strategy in the forecasting challenge is to report your honest beliefs.\nA forecaster’s expected Brier Score cannot be improved by exaggerating or hedging probabilities.\n\n\nIt’s rare in political science to have a system where everyone is incentivized to tell the truth. It’s quite refreshing to have a such a system here!"
  },
  {
    "objectID": "slides/bayes-rule.html#warmup",
    "href": "slides/bayes-rule.html#warmup",
    "title": "Bayes Rule",
    "section": "Warmup",
    "text": "Warmup\nMany federal law enforcement and national security agencies use polygraph tests to screen potential hires."
  },
  {
    "objectID": "slides/bayes-rule.html#warmup-1",
    "href": "slides/bayes-rule.html#warmup-1",
    "title": "Bayes Rule",
    "section": "Warmup",
    "text": "Warmup\nFor this exercise, let’s make the following assumptions:\n\n1% of applicants for a job at a national security agency are secretly foreign intelligence assets. You want to avoid hiring these people.\nPolygraph screening is quite good at detecting lies. If you ask someone whether they are a spy, it will correctly identify whether they are telling the truth or lying 90% of the time.\n\n\nIf an applicant fails this question on a polygraph screening, what is the probability that they are a spy?\n\nhttps://www.npr.org/2025/04/30/g-s1-63349/fbi-polygraph-lie-detector-leak-investigation"
  },
  {
    "objectID": "slides/bayes-rule.html#whats-going-on",
    "href": "slides/bayes-rule.html#whats-going-on",
    "title": "Bayes Rule",
    "section": "What’s Going On?",
    "text": "What’s Going On?\nIt can be helpful to think about this problem in terms of frequencies."
  },
  {
    "objectID": "slides/bayes-rule.html#whats-going-on-1",
    "href": "slides/bayes-rule.html#whats-going-on-1",
    "title": "Bayes Rule",
    "section": "What’s Going On?",
    "text": "What’s Going On?\nIt can be helpful to think about this problem in terms of frequencies."
  },
  {
    "objectID": "slides/bayes-rule.html#whats-going-on-2",
    "href": "slides/bayes-rule.html#whats-going-on-2",
    "title": "Bayes Rule",
    "section": "What’s Going On?",
    "text": "What’s Going On?\nIt can be helpful to think about this problem in terms of frequencies."
  },
  {
    "objectID": "slides/bayes-rule.html#whats-going-on-3",
    "href": "slides/bayes-rule.html#whats-going-on-3",
    "title": "Bayes Rule",
    "section": "What’s Going On?",
    "text": "What’s Going On?\nIt can be helpful to think about this problem in terms of frequencies."
  },
  {
    "objectID": "slides/bayes-rule.html#whats-going-on-4",
    "href": "slides/bayes-rule.html#whats-going-on-4",
    "title": "Bayes Rule",
    "section": "What’s Going On?",
    "text": "What’s Going On?\nIt can be helpful to think about this problem in terms of frequencies."
  },
  {
    "objectID": "slides/bayes-rule.html#whats-going-on-5",
    "href": "slides/bayes-rule.html#whats-going-on-5",
    "title": "Bayes Rule",
    "section": "What’s Going On?",
    "text": "What’s Going On?\nHere’s the problem as a probability tree:\n\n\n\n\n\n\n\n\n\n99%\n\n\n\n1%\n\n\n\n90%\n\n\n\n10%\n\n\n\n90%\n\n\n\n10%\n\n\n\nAll Applicants\n\n\n\nNot A Spy\n\n\n\nSpy\n\n\n\nPass Polygraph (89.1%)\n\n\n\nFail Polygraph (9.9%)\n\n\n\nFail Polygraph (0.9%)\n\n\n\nPass Polygraph (0.1%)"
  },
  {
    "objectID": "slides/bayes-rule.html#bayes-rule",
    "href": "slides/bayes-rule.html#bayes-rule",
    "title": "Bayes Rule",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nLet’s use this example to derive Bayes Rule, a formula for updating predictions in light of evidence.\n\nBefore observing any evidence from the polygraph test, our prior odds on an applicant being a spy are 1:99.\nWe observe the applicant fail the polygraph test. This result is 9 times more likely if they are a spy than if they are not a spy. Call this value the strength of evidence.\nSo after observing this evidence, our predicted odds that the applicant is a spy should be exactly 9 times higher.\nOur posterior odds are 9:99, or 1:11."
  },
  {
    "objectID": "slides/bayes-rule.html#bayes-rule-1",
    "href": "slides/bayes-rule.html#bayes-rule-1",
    "title": "Bayes Rule",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\\[\n\\text{Posterior Odds} = \\text{Prior Odds} \\times \\text{Strength of Evidence}\n\\]\n\nIn our example:\n\\[\n\\text{Posterior Odds} = \\underbrace{1:99}_\\text{Prior Odds} \\times \\underbrace{9:1}_\\text{Strength of Evidence} = 1:11\n\\]\n\n\nRemember, to convert odds to probability, divide \\(\\frac{p}{p+q}\\). So our posterior probability is \\(\\frac{1}{12} \\approx 8.3\\%\\)"
  },
  {
    "objectID": "slides/bayes-rule.html#practice",
    "href": "slides/bayes-rule.html#practice",
    "title": "Bayes Rule",
    "section": "Practice",
    "text": "Practice\nYou’re trying to predict whether a bill will pass the US House of Representatives.\n\nHistorically, only 10% of bills introduced get a floor vote and pass.\nAmong bills that pass the House, 60% have bipartisan cosponsors.\nAmong bills that don’t pass the House, 15% have bipartisan cosponsors.\n\n\nIf a bill has bipartisan cosponsors, what are the odds it will pass?"
  },
  {
    "objectID": "slides/bayes-rule.html#takeaways",
    "href": "slides/bayes-rule.html#takeaways",
    "title": "Bayes Rule",
    "section": "Takeaways",
    "text": "Takeaways\n\nAs we discussed last time, a common mistake when making predictions is to focus too much on the details of a particular case (Inside View), ignoring prior knowledge about similar cases (Outside View).\nBayes Rule is a useful way to discipline your thinking, because it requires you to place equal weight on prior beliefs and evidence."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fall 2025 Predictions",
    "section": "",
    "text": "You can find all this semester’s briefings here."
  },
  {
    "objectID": "index.html#week-1-819",
    "href": "index.html#week-1-819",
    "title": "Fall 2025 Predictions",
    "section": "Week 1 (8/19)",
    "text": "Week 1 (8/19)"
  },
  {
    "objectID": "index.html#week-2-826",
    "href": "index.html#week-2-826",
    "title": "Fall 2025 Predictions",
    "section": "Week 2 (8/26)",
    "text": "Week 2 (8/26)"
  },
  {
    "objectID": "index.html#week-3-92",
    "href": "index.html#week-3-92",
    "title": "Fall 2025 Predictions",
    "section": "Week 3 (9/2)",
    "text": "Week 3 (9/2)"
  },
  {
    "objectID": "index.html#week-4-99",
    "href": "index.html#week-4-99",
    "title": "Fall 2025 Predictions",
    "section": "Week 4 (9/9)",
    "text": "Week 4 (9/9)"
  },
  {
    "objectID": "index.html#week-5-916",
    "href": "index.html#week-5-916",
    "title": "Fall 2025 Predictions",
    "section": "Week 5 (9/16)",
    "text": "Week 5 (9/16)"
  },
  {
    "objectID": "index.html#week-6-923",
    "href": "index.html#week-6-923",
    "title": "Fall 2025 Predictions",
    "section": "Week 6 (9/23)",
    "text": "Week 6 (9/23)"
  },
  {
    "objectID": "index.html#week-7-930",
    "href": "index.html#week-7-930",
    "title": "Fall 2025 Predictions",
    "section": "Week 7 (9/30)",
    "text": "Week 7 (9/30)"
  },
  {
    "objectID": "index.html#week-8-107",
    "href": "index.html#week-8-107",
    "title": "Fall 2025 Predictions",
    "section": "Week 8 (10/7)",
    "text": "Week 8 (10/7)"
  },
  {
    "objectID": "index.html#leaderboard",
    "href": "index.html#leaderboard",
    "title": "Fall 2025 Predictions",
    "section": "Leaderboard",
    "text": "Leaderboard\n\n\n\n\n\n\n\n\n\n\n\n\nRank\nName\nSubmissions\nCorrect Calls\nBrier Score\n\n\n\n\n1\nCaleb Lux\n17\n4/6\n0.1743\n\n\n2\nChris Haswell\n17\n4/6\n0.1801\n\n\n3\nPaul Frilingos\n14\n5/6\n0.1801\n\n\n4\nZafar Ali\n16\n3/6\n0.1810\n\n\n5\nJhaycee Barnes\n16\n3/6\n0.1960\n\n\n6\nAkshara Singh\n16\n4/6\n0.1962\n\n\n7\nOwen Johnston\n16\n3/6\n0.1979\n\n\n8\nNic Curtis\n16\n3/6\n0.2028\n\n\n9\nJames Hunter Darke\n15\n4/6\n0.2054\n\n\n10\nJackson Bost\n17\n4/6\n0.2099\n\n\n11\nRaquel Beatriz Caldas Laranjeira\n17\n4/6\n0.2117\n\n\n12\nCierra Green\n17\n4/6\n0.2150\n\n\n13\nCrowd\n17\n4/6\n0.2153\n\n\n14\nLiz Yancey\n16\n3/6\n0.2177\n\n\n15\nSarah Kate Maher\n17\n3/6\n0.2192\n\n\n16\nHunter Murray\n14\n3/6\n0.2215\n\n\n17\nAlika Otero-Ricalde\n13\n3/6\n0.2233\n\n\n18\nAnna Barton-Caucci\n15\n4/6\n0.2234\n\n\n19\nManuv Datta\n14\n3/6\n0.2290\n\n\n20\nSierra Cross-Thompson\n11\n2/6\n0.2299\n\n\n21\nAnna Kneser\n16\n3/6\n0.2340\n\n\n22\nRuhi Doddamani\n15\n3/6\n0.2344\n\n\n23\nMateo Ceron\n13\n3/6\n0.2345\n\n\n24\nShaheer Ul-Islam\n16\n3/6\n0.2376\n\n\n25\nEllie Null\n15\n3/6\n0.2390\n\n\n26\nReese Fabritius\n15\n4/6\n0.2411\n\n\n27\nCaroline Mattox\n12\n3/6\n0.2452\n\n\n28\nFrancesca Mariano\n16\n3/6\n0.2473\n\n\n29\nAna Valencia\n15\n4/6\n0.2479\n\n\n30\nJoe Ornstein\n17\n4/6\n0.2542\n\n\n31\nBreanne Talley\n13\n2/6\n0.2596\n\n\n32\nAlizah Mudaliar\n17\n2/6\n0.2603\n\n\n33\nSasha Mahtani\n15\n3/6\n0.2607\n\n\n34\nBlake Martin\n15\n2/6\n0.2645\n\n\n35\nAmanda Neighbour\n17\n2/6\n0.2736\n\n\n36\nJack Lutz\n15\n3/6\n0.2755\n\n\n37\nSade Ezekiel\n17\n2/6\n0.2835\n\n\n38\nKira Fleischer\n14\n3/6\n0.2879\n\n\n39\nRyan Marx\n12\n2/6\n0.2888\n\n\n40\nKristoffer Fields\n14\n2/6\n0.2933\n\n\n41\nElizabeth Nelson\n15\n3/6\n0.2954\n\n\n42\nArshia Charkhian\n13\n3/6\n0.2973"
  },
  {
    "objectID": "fall-2023.html#week-2-913",
    "href": "fall-2023.html#week-2-913",
    "title": "Fall 2023 Predictions",
    "section": "Week 2 (9/13)",
    "text": "Week 2 (9/13)"
  },
  {
    "objectID": "fall-2023.html#week-3-920",
    "href": "fall-2023.html#week-3-920",
    "title": "Fall 2023 Predictions",
    "section": "Week 3 (9/20)",
    "text": "Week 3 (9/20)"
  },
  {
    "objectID": "fall-2023.html#week-4-927",
    "href": "fall-2023.html#week-4-927",
    "title": "Fall 2023 Predictions",
    "section": "Week 4 (9/27)",
    "text": "Week 4 (9/27)"
  },
  {
    "objectID": "fall-2023.html#week-5-104",
    "href": "fall-2023.html#week-5-104",
    "title": "Fall 2023 Predictions",
    "section": "Week 5 (10/4)",
    "text": "Week 5 (10/4)"
  },
  {
    "objectID": "fall-2023.html#week-6-1011",
    "href": "fall-2023.html#week-6-1011",
    "title": "Fall 2023 Predictions",
    "section": "Week 6 (10/11)",
    "text": "Week 6 (10/11)"
  },
  {
    "objectID": "fall-2023.html#week-7-1018",
    "href": "fall-2023.html#week-7-1018",
    "title": "Fall 2023 Predictions",
    "section": "Week 7 (10/18)",
    "text": "Week 7 (10/18)"
  },
  {
    "objectID": "fall-2023.html#week-8-1025",
    "href": "fall-2023.html#week-8-1025",
    "title": "Fall 2023 Predictions",
    "section": "Week 8 (10/25)",
    "text": "Week 8 (10/25)"
  },
  {
    "objectID": "fall-2023.html#week-9-111",
    "href": "fall-2023.html#week-9-111",
    "title": "Fall 2023 Predictions",
    "section": "Week 9 (11/1)",
    "text": "Week 9 (11/1)"
  },
  {
    "objectID": "fall-2023.html#week-10-118",
    "href": "fall-2023.html#week-10-118",
    "title": "Fall 2023 Predictions",
    "section": "Week 10 (11/8)",
    "text": "Week 10 (11/8)"
  },
  {
    "objectID": "fall-2023.html#week-11-1115",
    "href": "fall-2023.html#week-11-1115",
    "title": "Fall 2023 Predictions",
    "section": "Week 11 (11/15)",
    "text": "Week 11 (11/15)"
  },
  {
    "objectID": "fall-2023.html#leaderboard",
    "href": "fall-2023.html#leaderboard",
    "title": "Fall 2023 Predictions",
    "section": "Leaderboard",
    "text": "Leaderboard\n\n\n\n\n\nRank\nName\nCorrect Calls\nBrier Score\n\n\n\n\n1\nJoe Ornstein\n27/34\n0.1102\n\n\n2\nHarini Mohan\n25/34\n0.1850\n\n\n3\nAnsley Austin\n23/34\n0.1896\n\n\n4\nClass Average\n22/34\n0.2041\n\n\n5\nHarshini Mohan\n22/34\n0.2111\n\n\n6\nMaggie Fertig\n22/34\n0.2152\n\n\n7\nRyan Wolkofsky\n22/34\n0.2208\n\n\n8\nSamiha Sarwar\n19/34\n0.2210\n\n\n9\nBain Fulcher\n22/34\n0.2263\n\n\n10\nSimryn Patel\n20/34\n0.2674\n\n\n11\nTori Harrill\n18/34\n0.2755\n\n\n12\nRease Bauer\n17/34\n0.2759\n\n\n13\nJacob Fortner\n18/34\n0.2787\n\n\n14\nHita Boddu\n15/34\n0.3126\n\n\n15\nPeyton Murry\n16/34\n0.3638\n\n\n16\nJack Whipple\n17/34\n0.3857\n\n\n17\nKang Lee\n15/34\n0.4013\n\n\n18\nDuy Do\n13/34\n0.4599"
  },
  {
    "objectID": "fall-2023.html#calibration-plots",
    "href": "fall-2023.html#calibration-plots",
    "title": "Fall 2023 Predictions",
    "section": "Calibration Plots",
    "text": "Calibration Plots"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Two Principles\n\nBe humble. Think in terms of probabilities rather than certainties.\nConsider multiple perspectives."
  },
  {
    "objectID": "fall-2022.html#week-2-97",
    "href": "fall-2022.html#week-2-97",
    "title": "Fall 2022 Predictions",
    "section": "Week 2 (9/7)",
    "text": "Week 2 (9/7)"
  },
  {
    "objectID": "fall-2022.html#week-3-921",
    "href": "fall-2022.html#week-3-921",
    "title": "Fall 2022 Predictions",
    "section": "Week 3 (9/21)",
    "text": "Week 3 (9/21)"
  },
  {
    "objectID": "fall-2022.html#week-4-928",
    "href": "fall-2022.html#week-4-928",
    "title": "Fall 2022 Predictions",
    "section": "Week 4 (9/28)",
    "text": "Week 4 (9/28)"
  },
  {
    "objectID": "fall-2022.html#week-5-105",
    "href": "fall-2022.html#week-5-105",
    "title": "Fall 2022 Predictions",
    "section": "Week 5 (10/5)",
    "text": "Week 5 (10/5)"
  },
  {
    "objectID": "fall-2022.html#week-6-1012",
    "href": "fall-2022.html#week-6-1012",
    "title": "Fall 2022 Predictions",
    "section": "Week 6 (10/12)",
    "text": "Week 6 (10/12)"
  },
  {
    "objectID": "fall-2022.html#week-7-1019",
    "href": "fall-2022.html#week-7-1019",
    "title": "Fall 2022 Predictions",
    "section": "Week 7 (10/19)",
    "text": "Week 7 (10/19)"
  },
  {
    "objectID": "fall-2022.html#week-8-1026",
    "href": "fall-2022.html#week-8-1026",
    "title": "Fall 2022 Predictions",
    "section": "Week 8 (10/26)",
    "text": "Week 8 (10/26)"
  },
  {
    "objectID": "fall-2022.html#week-9-118",
    "href": "fall-2022.html#week-9-118",
    "title": "Fall 2022 Predictions",
    "section": "Week 9 (11/8)",
    "text": "Week 9 (11/8)"
  },
  {
    "objectID": "fall-2022.html#leaderboard",
    "href": "fall-2022.html#leaderboard",
    "title": "Fall 2022 Predictions",
    "section": "Leaderboard",
    "text": "Leaderboard\nNote: Updated December 22, 2022.1\n\n\n\n\n\nRank\nName\nCorrect Calls\nBrier Score\n\n\n\n\n1\nClass Average\n30/39\n0.1824\n\n\n2\nJeremy Lee\n27/39\n0.1887\n\n\n3\nDavid Barrero\n26/39\n0.1959\n\n\n4\nJoe Ornstein\n25/39\n0.2124\n\n\n5\nNicholas Chao\n24/39\n0.2166\n\n\n6\nAdam Szczupak\n28/39\n0.2170\n\n\n7\nArianna Ycaza\n24/39\n0.2319\n\n\n8\nPatrick Egan\n26/39\n0.2332\n\n\n9\nHannah Jones\n22/39\n0.2418\n\n\n10\nAudrey Vaughan\n25/39\n0.2452\n\n\n11\nAndrew Decker\n22/39\n0.2477\n\n\n12\nJoe Kennedy\n22/39\n0.2477\n\n\n13\nNoah Herndon\n23/39\n0.2490\n\n\n14\nMeredith Stevens\n19/39\n0.2903\n\n\n15\nOlivia West\n16/39\n0.3081\n\n\n16\nEdward Cox\n20/39\n0.3169\n\n\n17\nMac Kinnebrew\n17/39\n0.3172\n\n\n18\nCharlie Estabrook\n14/39\n0.3776\n\n\n19\nIan Driggers\n14/39\n0.4218"
  },
  {
    "objectID": "fall-2022.html#calibration-plots",
    "href": "fall-2022.html#calibration-plots",
    "title": "Fall 2022 Predictions",
    "section": "Calibration Plots",
    "text": "Calibration Plots"
  },
  {
    "objectID": "fall-2022.html#footnotes",
    "href": "fall-2022.html#footnotes",
    "title": "Fall 2022 Predictions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI had incorrectly resolved the Netanyahu question as “yes” following the November 1 Israeli elections, but he did not secure a coalition agreement to become Prime Minister by the November 30 deadline.↩︎"
  },
  {
    "objectID": "fall-2024.html#week-2-94",
    "href": "fall-2024.html#week-2-94",
    "title": "Fall 2024 Predictions",
    "section": "Week 2 (9/4)",
    "text": "Week 2 (9/4)"
  },
  {
    "objectID": "fall-2024.html#week-3-99",
    "href": "fall-2024.html#week-3-99",
    "title": "Fall 2024 Predictions",
    "section": "Week 3 (9/9)",
    "text": "Week 3 (9/9)"
  },
  {
    "objectID": "fall-2024.html#week-4-916",
    "href": "fall-2024.html#week-4-916",
    "title": "Fall 2024 Predictions",
    "section": "Week 4 (9/16)",
    "text": "Week 4 (9/16)"
  },
  {
    "objectID": "fall-2024.html#week-5-923",
    "href": "fall-2024.html#week-5-923",
    "title": "Fall 2024 Predictions",
    "section": "Week 5 (9/23)",
    "text": "Week 5 (9/23)"
  },
  {
    "objectID": "fall-2024.html#week-6-930",
    "href": "fall-2024.html#week-6-930",
    "title": "Fall 2024 Predictions",
    "section": "Week 6 (9/30)",
    "text": "Week 6 (9/30)"
  },
  {
    "objectID": "fall-2024.html#week-7-107",
    "href": "fall-2024.html#week-7-107",
    "title": "Fall 2024 Predictions",
    "section": "Week 7 (10/7)",
    "text": "Week 7 (10/7)"
  },
  {
    "objectID": "fall-2024.html#week-8-1014",
    "href": "fall-2024.html#week-8-1014",
    "title": "Fall 2024 Predictions",
    "section": "Week 8 (10/14)",
    "text": "Week 8 (10/14)"
  },
  {
    "objectID": "fall-2024.html#week-9-1021",
    "href": "fall-2024.html#week-9-1021",
    "title": "Fall 2024 Predictions",
    "section": "Week 9 (10/21)",
    "text": "Week 9 (10/21)"
  },
  {
    "objectID": "fall-2024.html#week-10-1028",
    "href": "fall-2024.html#week-10-1028",
    "title": "Fall 2024 Predictions",
    "section": "Week 10 (10/28)",
    "text": "Week 10 (10/28)"
  },
  {
    "objectID": "fall-2024.html#week-11-114",
    "href": "fall-2024.html#week-11-114",
    "title": "Fall 2024 Predictions",
    "section": "Week 11 (11/4)",
    "text": "Week 11 (11/4)"
  },
  {
    "objectID": "fall-2024.html#week-12-1111",
    "href": "fall-2024.html#week-12-1111",
    "title": "Fall 2024 Predictions",
    "section": "Week 12 (11/11)",
    "text": "Week 12 (11/11)"
  },
  {
    "objectID": "fall-2024.html#week-13-1118",
    "href": "fall-2024.html#week-13-1118",
    "title": "Fall 2024 Predictions",
    "section": "Week 13 (11/18)",
    "text": "Week 13 (11/18)"
  },
  {
    "objectID": "fall-2024.html#week-14-1125",
    "href": "fall-2024.html#week-14-1125",
    "title": "Fall 2024 Predictions",
    "section": "Week 14 (11/25)",
    "text": "Week 14 (11/25)"
  },
  {
    "objectID": "fall-2024.html#leaderboard",
    "href": "fall-2024.html#leaderboard",
    "title": "Fall 2024 Predictions",
    "section": "Leaderboard",
    "text": "Leaderboard\n\n\n\n\n\nRank\nName\nCorrect Calls\nBrier Score\n\n\n\n\n1\nJoe Ornstein\n32/36\n0.0633\n\n\n2\nCrowd\n31/36\n0.0874\n\n\n3\nCarson Ankeny\n27/31\n0.0962\n\n\n4\nCorinne O’Dell\n28/34\n0.0995\n\n\n5\nAlexis Frazer\n30/35\n0.1036\n\n\n6\nDavid Daniels\n30/36\n0.1036\n\n\n7\nMatthew Harvey\n28/32\n0.1081\n\n\n8\nClass Average\n31/36\n0.1115\n\n\n9\nLiv Payne\n24/31\n0.1149\n\n\n10\nAbigail Harris\n25/30\n0.1154\n\n\n11\nShayna Van Glish\n24/30\n0.1163\n\n\n12\nKira Liscombe\n24/30\n0.1197\n\n\n13\nGracie Culpepper\n24/31\n0.1208\n\n\n14\nReece Slater\n26/33\n0.1225\n\n\n15\nLeo Zegarra\n25/33\n0.1239\n\n\n16\nVikram Bharadwaj\n28/36\n0.1247\n\n\n17\nAlex Vrolijk\n27/33\n0.1290\n\n\n18\nVictor Avila\n19/24\n0.1307\n\n\n19\nAbby Crowe\n28/34\n0.1319\n\n\n20\nColin Barry\n29/35\n0.1333\n\n\n21\nBrooke Mcdermott\n26/32\n0.1336\n\n\n22\nHaley Ford\n26/34\n0.1340\n\n\n23\nJohn Mathis\n23/30\n0.1341\n\n\n24\nSimon Gersten\n26/32\n0.1353\n\n\n25\nFranco Bravo\n23/29\n0.1395\n\n\n26\nEdward Tsui\n25/33\n0.1401\n\n\n27\nKristalee Gonzalez-Perez\n26/33\n0.1422\n\n\n28\nJessica Armour\n28/35\n0.1454\n\n\n29\nHayden Morehouse\n20/27\n0.1494\n\n\n30\nRommy Etzion\n14/16\n0.1498\n\n\n31\nWilliam Willis\n25/34\n0.1564\n\n\n32\nRyan Foley\n22/29\n0.1595\n\n\n33\nErin Ho\n24/34\n0.1596\n\n\n34\nLadi Fashogbon\n20/29\n0.1623\n\n\n35\nMinha Ahmad\n25/33\n0.1654\n\n\n36\nJeffrey Markowitz\n22/31\n0.1682\n\n\n37\nSara Whitmire\n14/21\n0.1687\n\n\n38\nFindleigh Ague\n21/33\n0.1693"
  },
  {
    "objectID": "fall-2024.html#calibration-plots",
    "href": "fall-2024.html#calibration-plots",
    "title": "Fall 2024 Predictions",
    "section": "Calibration Plots",
    "text": "Calibration Plots"
  },
  {
    "objectID": "slides/aggregating-forecasts.html#motivating-problem",
    "href": "slides/aggregating-forecasts.html#motivating-problem",
    "title": "Aggregating Forecasts",
    "section": "Motivating Problem",
    "text": "Motivating Problem\n\nSuppose the class is trying to predict whether the Federal Reserve will cut interest rates at its next meeting.\nThree students submit briefings, and all three say 75%.\nWhat should you predict after observing those three forecasts?\nPreviously, we considered taking the median forecast (i.e. majority rule).\n\nBut that strategy required some strong assumptions.\nAre those assumptions met here?"
  },
  {
    "objectID": "slides/aggregating-forecasts.html#todays-agenda",
    "href": "slides/aggregating-forecasts.html#todays-agenda",
    "title": "Aggregating Forecasts",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nHow best to aggregate individual forecasts into a group forecast?\nI’ll try to convince you that, if three individual forecasts are at 75%, the best aggregation is probably &gt;75%.\n\nThis is called extremizing—increasing the confidence of the group forecast when multiple individuals all make a prediction in the same direction.\n\nWe’ll motivate this idea from the perspective of Bayesian updating."
  },
  {
    "objectID": "slides/aggregating-forecasts.html#warmup",
    "href": "slides/aggregating-forecasts.html#warmup",
    "title": "Aggregating Forecasts",
    "section": "Warmup",
    "text": "Warmup\n\nSuppose you’re writing a briefing about the interest rates question.\nAt first, you have no idea what will happen (prior odds = 1:1).\nThen you find a statement from a Fed official expressing concern about unemployment.\nHistorically, he makes statements like this three times more often before an interest rate cut than he does otherwise.\nProblem: What should your new forecast be, after observing this evidence?"
  },
  {
    "objectID": "slides/aggregating-forecasts.html#warmup-1",
    "href": "slides/aggregating-forecasts.html#warmup-1",
    "title": "Aggregating Forecasts",
    "section": "Warmup",
    "text": "Warmup\nRemember Bayes Rule:\n\\[\n\\text{Posterior Odds} = \\text{Prior Odds} \\times \\text{Strength of Evidence}\n\\]"
  },
  {
    "objectID": "slides/aggregating-forecasts.html#warmup-2",
    "href": "slides/aggregating-forecasts.html#warmup-2",
    "title": "Aggregating Forecasts",
    "section": "Warmup",
    "text": "Warmup\nRemember Bayes Rule:\n\\[\n\\underbrace{\\text{Posterior Odds}}_{\\text{New Forecast}} = \\underbrace{\\text{Prior Odds}}_{\\text{Old Forecast}}\\times  \\underbrace{\\text{Strength of Evidence}}_{\\text{Likelihood Ratio}}\n\\]\n\nPrior Odds: 1:1\nLikelihood Ratio: 3:1\nPosterior Odds: 3:1\nNew Forecast: 75%"
  },
  {
    "objectID": "slides/aggregating-forecasts.html#bayesian-updating",
    "href": "slides/aggregating-forecasts.html#bayesian-updating",
    "title": "Aggregating Forecasts",
    "section": "Bayesian Updating",
    "text": "Bayesian Updating\n\nNow suppose you read that JP Morgan’s analysts are predicting an interest rate cut.\nHistorically, this appears to have the same strength of evidence as the Fed official’s statements.\nBayes Rule handles this problem exactly like the first one!\n\nPrior Odds: 3:1\nLikelihood Ratio: 3:1\nPosterior Odds: \\(3:1 \\times 3:1 = 9:1\\)\nNew Prediction: 90%"
  },
  {
    "objectID": "slides/aggregating-forecasts.html#bayesian-updating-1",
    "href": "slides/aggregating-forecasts.html#bayesian-updating-1",
    "title": "Aggregating Forecasts",
    "section": "Bayesian Updating",
    "text": "Bayesian Updating\n\nThis process of Bayesian updating can proceed indefinitely.\nFor each piece of evidence:\n\nAssess its likelihood ratio—conditional on everything you already know, how much more likely are you to observe this evidence if the event happens?\nAdjust your prediction up or down using Bayes Rule."
  },
  {
    "objectID": "slides/aggregating-forecasts.html#back-to-the-motivating-problem",
    "href": "slides/aggregating-forecasts.html#back-to-the-motivating-problem",
    "title": "Aggregating Forecasts",
    "section": "Back to the Motivating Problem",
    "text": "Back to the Motivating Problem\n\nConsider the three students writing briefings. All three start with prior odds 1:1.\nStudent A only finds the statement by the Fed official (3:1 likelihood ratio).\n\nStudent B finds the JP Morgan report (3:1 likelihood ratio).\nStudent C finds a third piece of evidence (also 3:1).\nIf nobody shares information with each other, what would each student predict?\nIf everyone shared their information, what would they predict (using Bayesian updating)?"
  },
  {
    "objectID": "slides/aggregating-forecasts.html#extremizing",
    "href": "slides/aggregating-forecasts.html#extremizing",
    "title": "Aggregating Forecasts",
    "section": "Extremizing",
    "text": "Extremizing\n\nThis is the intuition behind extremizing predictions.\nIf a group of forecasters have independent pieces of information, they should become more confident if they shared what they know.\nWe can simulate this process by making everyone’s forecast a bit more confident before taking the average.\nHow much you extremize depends on how much independent information you think the crowd has."
  },
  {
    "objectID": "slides/aggregating-forecasts.html#extremizing-1",
    "href": "slides/aggregating-forecasts.html#extremizing-1",
    "title": "Aggregating Forecasts",
    "section": "Extremizing",
    "text": "Extremizing\nIf every individual has fully independent evidence informing their prediction:\n\n\\[\no_{group} = o_1 \\times o_2 \\times o_3 \\times \\ldots \\times o_n\n\\]\n\n\nThis is full Bayesian updating; multiply everyone’s odds together to get the posterior."
  },
  {
    "objectID": "slides/aggregating-forecasts.html#extremizing-2",
    "href": "slides/aggregating-forecasts.html#extremizing-2",
    "title": "Aggregating Forecasts",
    "section": "Extremizing",
    "text": "Extremizing\n\nBut that independence assumption is wildly optimistic!\nForecasters are usually relying on the same sources of information (Google searches, chatbots, newspapers, etc.)\nMore reasonable assumption: the crowd has some independent pieces of information."
  },
  {
    "objectID": "slides/aggregating-forecasts.html#extremizing-3",
    "href": "slides/aggregating-forecasts.html#extremizing-3",
    "title": "Aggregating Forecasts",
    "section": "Extremizing",
    "text": "Extremizing\nIf each individual has some independent information:\n\n\\[\no_{group} = (o_1 \\times o_2 \\times o_3 \\times \\ldots \\times o_n)^\\frac{\\alpha}{n}\n\\]\n\n\n\n\\(\\alpha\\) represents the amount of independent information.\n\\(\\alpha = n\\): Everyone has completely independent information. Group forecast is full Bayesian updating.\n\\(\\alpha = 1\\): No one has independent information. Group forecast is just the average individual forecast.\nTruth is probably somewhere in the middle. For superforecasters, Satopää et al. (2014) use \\(\\alpha = 2.5\\)."
  },
  {
    "objectID": "slides/aggregating-forecasts.html#example",
    "href": "slides/aggregating-forecasts.html#example",
    "title": "Aggregating Forecasts",
    "section": "Example",
    "text": "Example\n\nFive forecasters predict 40%, 50%, 60%, 75%, and 75%.\nThe median forecast is 60%.\nThe extremized forecast \\((\\alpha = 2.5)\\) is:\n\n\n\\[\n(2:3 \\times 1:1 \\times 3:2 \\times 3:1 \\times 3:1)^\\frac{2.5}{5} =\n(9:1)^\\frac{1}{2} = 3:1 = 75\\%\n\\]\n\nThis isn’t the sort of math I’d ask you to perform on an exam.\nI just want you to have it in your toolbox."
  },
  {
    "objectID": "slides/aggregating-forecasts.html#takeaways",
    "href": "slides/aggregating-forecasts.html#takeaways",
    "title": "Aggregating Forecasts",
    "section": "Takeaways",
    "text": "Takeaways\n\nConfident predictions are justified if multiple, independent pieces of evidence point towards the same conclusion (Bayesian updating, dragonfly-eyed foxes).\nWhen reading briefings, consider whether each author is bringing a different perspective, or if everyone is relying on the same evidence.\n\nIf the former, then the group forecast should be more confident than the individual forecasts (extremizing).\n\nOrganize teams so that they share information rather than just sharing conclusions."
  },
  {
    "objectID": "slides/aggregating-forecasts.html#references",
    "href": "slides/aggregating-forecasts.html#references",
    "title": "Aggregating Forecasts",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nSatopää, Ville A., Jonathan Baron, Dean P. Foster, Barbara A. Mellers, Philip E. Tetlock, and Lyle H. Ungar. 2014. “Combining Multiple Probability Predictions Using a Simple Logit Model.” International Journal of Forecasting 30 (2): 344–56. https://doi.org/10.1016/j.ijforecast.2013.09.009."
  },
  {
    "objectID": "slides/bell-curves.html#todays-agenda",
    "href": "slides/bell-curves.html#todays-agenda",
    "title": "Bell Curves",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nIntroduce the bell curve probability distribution\n\nAKA the normal distribution or the Gaussian distribution\n\nUnderstand the conditions that create bell curves (the Central Limit Theorem)\nExplore some useful features of bell curves for making predictions"
  },
  {
    "objectID": "slides/bell-curves.html#warmup",
    "href": "slides/bell-curves.html#warmup",
    "title": "Bell Curves",
    "section": "Warmup",
    "text": "Warmup\n\nIn an upcoming Congressional election:\n\n60% of voters plan to vote for the Republican candidate.\n40% of voters plan to vote for the Democratic candidate.\n\nA polling firm randomly calls voters in the Congressional district and asks who they plan to vote for.\n\n\nDraw a probability tree representing the first two voters contacted by the polling firm. What is the probability of every possible poll result?"
  },
  {
    "objectID": "slides/bell-curves.html#warmup-1",
    "href": "slides/bell-curves.html#warmup-1",
    "title": "Bell Curves",
    "section": "Warmup",
    "text": "Warmup\n\n\n\n\n\n\n\n\n\n60%\n\n\n\n40%\n\n\n\nStart Poll\n\n\n\nR\n\n\n\nD"
  },
  {
    "objectID": "slides/bell-curves.html#warmup-2",
    "href": "slides/bell-curves.html#warmup-2",
    "title": "Bell Curves",
    "section": "Warmup",
    "text": "Warmup\n\n\n\n\n\n\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\nStart Poll\n\n\n\nR\n\n\n\nD\n\n\n\nRR (36%)\n\n\n\nRD (24%)\n\n\n\nDR (24%)\n\n\n\nDD (16%)"
  },
  {
    "objectID": "slides/bell-curves.html#simplifying-assumption",
    "href": "slides/bell-curves.html#simplifying-assumption",
    "title": "Bell Curves",
    "section": "Simplifying Assumption",
    "text": "Simplifying Assumption\nBecause we don’t care about the order of responses (just the counts), we can combine some outcomes:\n\n\n\n\n\n\n\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\nStart Poll\n\n\n\n1R,0D\n\n\n\n0R,1D\n\n\n\n2R,0D (36%)\n\n\n\n1R,1D (48%)\n\n\n\n0R,2D (16%)"
  },
  {
    "objectID": "slides/bell-curves.html#things-to-notice",
    "href": "slides/bell-curves.html#things-to-notice",
    "title": "Bell Curves",
    "section": "Things To Notice",
    "text": "Things To Notice\n\nA poll with two responses is pretty worthless.\n\n48% of the time you get an equal number of R’s and D’s\n16% of the time, you get all D’s\n36% of the time, you get all R’s\n\nTwo big problems:\n\nThe poll is biased. Result is more likely to be wrong in one direction than the other.\nIt also has high variance. Result is, on average, very far from the truth."
  },
  {
    "objectID": "slides/bell-curves.html#increasing-poll-size",
    "href": "slides/bell-curves.html#increasing-poll-size",
    "title": "Bell Curves",
    "section": "Increasing Poll Size",
    "text": "Increasing Poll Size\nOver the next few slides, we’ll show that increasing the size of the poll \\((n)\\) does three things:\n\nEliminates bias\nReduces variance\nGives the polling errors a particular (bell curve) shape."
  },
  {
    "objectID": "slides/bell-curves.html#n3",
    "href": "slides/bell-curves.html#n3",
    "title": "Bell Curves",
    "section": "n=3",
    "text": "n=3\n\n\n\n\n\n\n\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\nStart Poll\n\n\n\n1R,0D\n\n\n\n0R,1D\n\n\n\n2R,0D\n\n\n\n1R,1D\n\n\n\n0R,2D\n\n\n\n3R,0D (21.6%)\n\n\n\n2R,1D (43.2%)\n\n\n\n1R,2D (28.8%)\n\n\n\n0R,3D (6.4%)"
  },
  {
    "objectID": "slides/bell-curves.html#section",
    "href": "slides/bell-curves.html#section",
    "title": "Bell Curves",
    "section": "",
    "text": "We can plot these compound probabilities on a bar chart."
  },
  {
    "objectID": "slides/bell-curves.html#n4",
    "href": "slides/bell-curves.html#n4",
    "title": "Bell Curves",
    "section": "n=4",
    "text": "n=4\n\n\n\n\n\n\n\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\nStart Poll\n\n\n\n1R,0D\n\n\n\n0R,1D\n\n\n\n2R,0D\n\n\n\n1R,1D\n\n\n\n0R,2D\n\n\n\n3R,0D\n\n\n\n2R,1D\n\n\n\n1R,2D\n\n\n\n0R,3D\n\n\n\n4R,0D (13%)\n\n\n\n3R,1D (34.5%)\n\n\n\n2R,2D (34.5%)\n\n\n\n1R,3D (15.4%)\n\n\n\n0R,4D (2.6%)\n\n\n\n\n\n\n\n\n\n\nI would never make you do this by hand."
  },
  {
    "objectID": "slides/bell-curves.html#n5",
    "href": "slides/bell-curves.html#n5",
    "title": "Bell Curves",
    "section": "n=5",
    "text": "n=5\n\n\n\n\n\n\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\nStart Poll\n\n\n\n1R,0D\n\n\n\n0R,1D\n\n\n\n2R,0D\n\n\n\n1R,1D\n\n\n\n0R,2D\n\n\n\n3R,0D\n\n\n\n2R,1D\n\n\n\n1R,2D\n\n\n\n0R,3D\n\n\n\n4R,0D\n\n\n\n3R,1D\n\n\n\n2R,2D\n\n\n\n1R,3D\n\n\n\n0R,4D\n\n\n\n5R,0D (7.8%)\n\n\n\n4R,1D (25.9%)\n\n\n\n3R,2D (34.6%)\n\n\n\n2R,3D (23%)\n\n\n\n1R,4D (7.7%)\n\n\n\n0R,5D (1%)\n\n\n\n\n\n\n\n\n\nIntuition Check: Why is the probability of 3R,2D so big and the probability of 0R,5D so small?"
  },
  {
    "objectID": "slides/bell-curves.html#n6",
    "href": "slides/bell-curves.html#n6",
    "title": "Bell Curves",
    "section": "n=6",
    "text": "n=6\n\n\n\n\n\n\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\n60%\n\n\n\n40%\n\n\n\nStart Poll\n\n\n\n1R,0D\n\n\n\n0R,1D\n\n\n\n2R,0D\n\n\n\n1R,1D\n\n\n\n0R,2D\n\n\n\n3R,0D\n\n\n\n2R,1D\n\n\n\n1R,2D\n\n\n\n0R,3D\n\n\n\n4R,0D\n\n\n\n3R,1D\n\n\n\n2R,2D\n\n\n\n1R,3D\n\n\n\n0R,4D\n\n\n\n5R,0D\n\n\n\n4R,1D\n\n\n\n3R,2D\n\n\n\n2R,3D\n\n\n\n1R,4D\n\n\n\n0R,5D\n\n\n\n6D,0R (4.7%)\n\n\n\n5D,1R (18.7%)\n\n\n\n4D,2R (31.1%)\n\n\n\n3D,3R (27.6%)\n\n\n\n2D,4R (13.8%)\n\n\n\n1D,5R (3.7%)\n\n\n\n0D,6R (0.4%)"
  },
  {
    "objectID": "slides/bell-curves.html#things-to-notice-1",
    "href": "slides/bell-curves.html#things-to-notice-1",
    "title": "Bell Curves",
    "section": "Things To Notice",
    "text": "Things To Notice\n\nWith a large enough sample, the poll results are unbiased. Centered on the truth, and equally likely to be too high or too low.\nVariance shrinks with poll size.\n\nIn a poll of 50 voters, there’s a strong chance you get a result off by 10 percentage points and call the election incorrectly.\nIn a poll with 500 voters, it’s practically impossible that the result will be off by more than 10 percentage points."
  },
  {
    "objectID": "slides/bell-curves.html#central-limit-theorem",
    "href": "slides/bell-curves.html#central-limit-theorem",
    "title": "Bell Curves",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nAs poll size gets larger, the shape of the errors takes on that gorgeous bell curve shape.\nThis is one of the most foundational ideas in all of statistics.\n\n\nCentral Limit Theorem\n\nIf an outcome is the sum of a large number of independent random events, then it will fall on a bell curve."
  },
  {
    "objectID": "slides/bell-curves.html#bell-curves-in-the-wild",
    "href": "slides/bell-curves.html#bell-curves-in-the-wild",
    "title": "Bell Curves",
    "section": "Bell Curves In The Wild",
    "text": "Bell Curves In The Wild\nHuman height is the sum of a large number of independent genetic and environmental factors, so…"
  },
  {
    "objectID": "slides/bell-curves.html#bell-curves-in-the-wild-1",
    "href": "slides/bell-curves.html#bell-curves-in-the-wild-1",
    "title": "Bell Curves",
    "section": "Bell Curves In The Wild",
    "text": "Bell Curves In The Wild\nHuman height is the sum of a large number of independent genetic and environmental factors, so… . . ."
  },
  {
    "objectID": "slides/bell-curves.html#bell-curves-in-the-wild-2",
    "href": "slides/bell-curves.html#bell-curves-in-the-wild-2",
    "title": "Bell Curves",
    "section": "Bell Curves In The Wild",
    "text": "Bell Curves In The Wild\nStandardized test scores are the sum of a large number of independent question scores, so…"
  },
  {
    "objectID": "slides/bell-curves.html#bell-curves-in-the-wild-3",
    "href": "slides/bell-curves.html#bell-curves-in-the-wild-3",
    "title": "Bell Curves",
    "section": "Bell Curves In The Wild",
    "text": "Bell Curves In The Wild\nCollege football scores are the sum of a large number of independent successes / failures to get the ball to the other end of the field, so…\n\n\n\n\nCollege football scores (relative to the Vegas “spread”)"
  },
  {
    "objectID": "slides/bell-curves.html#bell-curves-are-nice",
    "href": "slides/bell-curves.html#bell-curves-are-nice",
    "title": "Bell Curves",
    "section": "Bell Curves Are Nice",
    "text": "Bell Curves Are Nice\n\nWhen outcomes fall on a bell curve, it makes prediction a lot easier.\nThat’s because outcomes are very unlikely to stray far from their expected values."
  },
  {
    "objectID": "slides/bell-curves.html#bell-curves-are-nice-1",
    "href": "slides/bell-curves.html#bell-curves-are-nice-1",
    "title": "Bell Curves",
    "section": "Bell Curves Are Nice",
    "text": "Bell Curves Are Nice\n95% of poll results will be one of the red bars."
  },
  {
    "objectID": "slides/bell-curves.html#bell-curves-are-nice-2",
    "href": "slides/bell-curves.html#bell-curves-are-nice-2",
    "title": "Bell Curves",
    "section": "Bell Curves Are Nice",
    "text": "Bell Curves Are Nice\n95% of poll results will be one of the red bars."
  },
  {
    "objectID": "slides/bell-curves.html#margin-of-error",
    "href": "slides/bell-curves.html#margin-of-error",
    "title": "Bell Curves",
    "section": "Margin of Error",
    "text": "Margin of Error\n\nDefine the margin of error as the range within which you’re 95% sure your polling error will fall.\nThe back-of-the-envelope approximation of a poll’s margin of error is \\(\\frac{100\\%}{\\sqrt{n}}\\).\n\nSo, for a poll with 100 respondents, margin of error is roughly \\(\\frac{100\\%}{\\sqrt{100}} = 10\\%\\).\nPractice: what’s the margin of error for a poll with 400 respondents?"
  },
  {
    "objectID": "slides/bell-curves.html#wrap-up",
    "href": "slides/bell-curves.html#wrap-up",
    "title": "Bell Curves",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nYour outcome will fall on a bell curve if it is the sum of a large number of independent random events (Central Limit Theorem).\nIf the theorem holds, it’s great for making predictions, because bell curves are easy to work with.\n\nIn a few weeks, we’ll talk about the ways in which real-world polls fall short of this idealized model.\n\nNext Time: What happens when that independence assumption is violated?"
  },
  {
    "objectID": "slides/compound-probability.html#warmup",
    "href": "slides/compound-probability.html#warmup",
    "title": "Compound Probability",
    "section": "Warmup",
    "text": "Warmup"
  },
  {
    "objectID": "slides/compound-probability.html#compound-probability",
    "href": "slides/compound-probability.html#compound-probability",
    "title": "Compound Probability",
    "section": "Compound Probability",
    "text": "Compound Probability\n\nLast time, we defined probability as a number between 0 and 1 describing the likelihood of an event.\nToday, our focus is compound probability, the likelihood of some combination of two or more events.\nCompound probability is particularly counterintuitive.\nBy the end of the lecture, you’ll be equipped with some tools for dealing with these sorts of problems."
  },
  {
    "objectID": "slides/compound-probability.html#probability-trees-1",
    "href": "slides/compound-probability.html#probability-trees-1",
    "title": "Compound Probability",
    "section": "Probability Trees",
    "text": "Probability Trees\n\nImagine the future as a series of branching paths.\nEach branch represents a different path that the universe could take.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBorn\n\n\n\nGo to UGA\n\n\n\nGo somewhere else\n\n\n\nEnroll in POLS 3220\n\n\n\nStudy biochem instead\n\n\n\nAttend class today\n\n\n\nSkip class today"
  },
  {
    "objectID": "slides/compound-probability.html#rules-for-tree-construction",
    "href": "slides/compound-probability.html#rules-for-tree-construction",
    "title": "Compound Probability",
    "section": "Rules for Tree Construction",
    "text": "Rules for Tree Construction\n\nBranches originating from a single point must be mutually exclusive and collectively exhaustive.\n\nMutually Exclusive: if one event happens, the others cannot happen\n\ne.g. “Georgia beats Texas” and “Texas beats Georgia”.\n\nCollectively Exhaustive: covers every possible outcome\n\ne.g. “I drink coffee this afternoon” and “I don’t drink coffee this afternoon”."
  },
  {
    "objectID": "slides/compound-probability.html#rules-for-tree-construction-1",
    "href": "slides/compound-probability.html#rules-for-tree-construction-1",
    "title": "Compound Probability",
    "section": "Rules for Tree Construction",
    "text": "Rules for Tree Construction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStart of Season\n\n\n\nUGA beats Alabama\n\n\n\nAlabama beats UGA\n\n\n\nUGA beats Texas\n\n\n\nTexas beats UGA\n\n\n\nUGA beats Texas\n\n\n\nTexas beats UGA"
  },
  {
    "objectID": "slides/compound-probability.html#practice",
    "href": "slides/compound-probability.html#practice",
    "title": "Compound Probability",
    "section": "Practice",
    "text": "Practice\nThink about a sequence of 2-3 choices you expect to make after class is over (e.g. where you’ll eat dinner). Draw a tree illustrating all the possible paths your evening could take."
  },
  {
    "objectID": "slides/compound-probability.html#rules-for-tree-construction-2",
    "href": "slides/compound-probability.html#rules-for-tree-construction-2",
    "title": "Compound Probability",
    "section": "Rules for Tree Construction",
    "text": "Rules for Tree Construction\nNext, we will assign a probability to each branch. When doing so, remember two rules:\n\nIf events \\(A\\) and \\(B\\) are mutually exclusive, then \\(P(A \\text{ or } B) = P(A) + P(B)\\).\nIf events \\(A\\), \\(B\\), and \\(C\\) are mutually exclusive and collectively exhaustive, then \\(P(A) + P(B) + P(C) = 1\\).\n\n\nThese two rules are called the axioms of probability."
  },
  {
    "objectID": "slides/compound-probability.html#rules-for-tree-construction-3",
    "href": "slides/compound-probability.html#rules-for-tree-construction-3",
    "title": "Compound Probability",
    "section": "Rules for Tree Construction",
    "text": "Rules for Tree Construction\n\n\n\n\n\n\n\n\n\n0.6\n\n\n\n0.4\n\n\n\n0.6\n\n\n\n0.4\n\n\n\n0.5\n\n\n\n0.5\n\n\n\nStart of Season\n\n\n\nUGA beats Alabama\n\n\n\nAlabama beats UGA\n\n\n\nUGA beats Texas\n\n\n\nTexas beats UGA\n\n\n\nUGA beats Texas\n\n\n\nTexas beats UGA\n\n\n\n\n\n\n\n\n\nThe second set of probabilities are called conditional probabilities."
  },
  {
    "objectID": "slides/compound-probability.html#conditional-probability",
    "href": "slides/compound-probability.html#conditional-probability",
    "title": "Compound Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nWe denote the probability of event \\(A\\) conditional on event \\(B\\) as:\n\\[\nP(A|B)\n\\]\n\nAnd the probability of \\(A\\) conditional on \\(B\\) not happening is:\n\\[\nP(A|\\neg B)\\]"
  },
  {
    "objectID": "slides/compound-probability.html#independence",
    "href": "slides/compound-probability.html#independence",
    "title": "Compound Probability",
    "section": "Independence",
    "text": "Independence\nTwo events are independent if the outcome of one doesn’t affect the probability of the other. Formally:\n\\[P(A|B) = P(A|\\neg B)\\]"
  },
  {
    "objectID": "slides/compound-probability.html#independence-1",
    "href": "slides/compound-probability.html#independence-1",
    "title": "Compound Probability",
    "section": "Independence",
    "text": "Independence\nCoin flips are a classic example of independent events:\n\n\n\n\n\n\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\n0.5\n\n\n\nStart flipping a coin\n\n\n\nHeads\n\n\n\nTails\n\n\n\nHeads\n\n\n\nTails\n\n\n\nHeads\n\n\n\nTails\n\n\n\nHeads\n\n\n\nTails\n\n\n\nHeads\n\n\n\nTails\n\n\n\nHeads\n\n\n\nTails\n\n\n\nHeads\n\n\n\nTails"
  },
  {
    "objectID": "slides/compound-probability.html#practice-1",
    "href": "slides/compound-probability.html#practice-1",
    "title": "Compound Probability",
    "section": "Practice",
    "text": "Practice\nAssign probabilities to each branch of your evening tree. Make sure you adhere to the axioms. And consider whether your choices are independent, or if one event might affect the probabilities of subsequent events."
  },
  {
    "objectID": "slides/compound-probability.html#joint-probability",
    "href": "slides/compound-probability.html#joint-probability",
    "title": "Compound Probability",
    "section": "Joint Probability",
    "text": "Joint Probability\nNow we’re ready to tackle joint probability. What is the probability of event \\(A\\) and event \\(B\\) both happening?\n\n\n\n\n\n\n\n\n\n0.6\n\n\n\n0.4\n\n\n\n0.6\n\n\n\n0.4\n\n\n\n0.5\n\n\n\n0.5\n\n\n\nStart of Season\n\n\n\nUGA beats Alabama\n\n\n\nAlabama beats UGA\n\n\n\nUGA beats Texas\n\n\n\nTexas beats UGA\n\n\n\nUGA beats Texas\n\n\n\nTexas beats UGA"
  },
  {
    "objectID": "slides/compound-probability.html#joint-probability-1",
    "href": "slides/compound-probability.html#joint-probability-1",
    "title": "Compound Probability",
    "section": "Joint Probability",
    "text": "Joint Probability\nTo find the probability of ending up at any node of the tree, multiply the probabilities of all the branches that feed into it.\n\n\n\n\n\n\n\n\n\n\n0.6\n\n\n\n0.4\n\n\n\n0.6\n\n\n\n0.4\n\n\n\n0.5\n\n\n\n0.5\n\n\n\nStart of Season\n\n\n\nUGA beats Alabama\n\n\n\nAlabama beats UGA\n\n\n\nUGA beats Texas (36%)\n\n\n\nTexas beats UGA (24%)\n\n\n\nUGA beats Texas (20%)\n\n\n\nTexas beats UGA (20%)"
  },
  {
    "objectID": "slides/compound-probability.html#practice-2",
    "href": "slides/compound-probability.html#practice-2",
    "title": "Compound Probability",
    "section": "Practice",
    "text": "Practice\nCalculate joint probabilities for every node in your evening tree. To check your work, make sure everything satisfies the axioms of probability.\n\nIf two events are mutually exclusive, then \\(P(A \\text{ or } B) = P(A) + P(B)\\)\nIf a set of events are collectively exhaustive, then their probabilities should sum to 100%."
  },
  {
    "objectID": "slides/compound-probability.html#wrap-up",
    "href": "slides/compound-probability.html#wrap-up",
    "title": "Compound Probability",
    "section": "Wrap Up",
    "text": "Wrap Up\nThis gives us some insight into the birthday problem we started with:\n\n\n\n\n\n\n\n\n\n364/365\n\n\n\n1/365\n\n\n\n363/365\n\n\n\n2/365\n\n\n\n362/365\n\n\n\n3/365\n\n\n\nPerson A\n\n\n\nNo match\n\n\n\nPerson B shares birthday\n\n\n\nNo match\n\n\n\nPerson C shares birthday\n\n\n\nNo match\n\n\n\nPerson D shares birthday"
  },
  {
    "objectID": "slides/election-forecasting.html#todays-agenda",
    "href": "slides/election-forecasting.html#todays-agenda",
    "title": "Election Forecasting",
    "section": "",
    "text": "Today, we’ll develop a model to predict the outcome of US Senate elections.\nThis is a tough forecasting problem compared to presidential elections, for two reasons:\n\nRelatively few polls\n“Fundamentals” are weakly predictive\n\nCombining polls + fundamentals will yield much better predictions than either alone!"
  },
  {
    "objectID": "slides/election-forecasting.html#approach-1-polls-only",
    "href": "slides/election-forecasting.html#approach-1-polls-only",
    "title": "Election Forecasting",
    "section": "Approach 1: Polls Only",
    "text": "Approach 1: Polls Only"
  },
  {
    "objectID": "slides/election-forecasting.html#approach-1-polls-only-1",
    "href": "slides/election-forecasting.html#approach-1-polls-only-1",
    "title": "Election Forecasting",
    "section": "Approach 1: Polls Only",
    "text": "Approach 1: Polls Only\n\nBetween 1992 and 2016, polls “called” the correct candidate 79.7% of the time.\nAnd polling error is quite high! The average poll was off by 6.75 percentage points.\nOf course, we expect that there will be some error in polling.\n\nThink back to our lecture on Bell Curves. Polling error should look like a normal distribution."
  },
  {
    "objectID": "slides/election-forecasting.html#approach-1-polls-only-2",
    "href": "slides/election-forecasting.html#approach-1-polls-only-2",
    "title": "Election Forecasting",
    "section": "Approach 1: Polls Only",
    "text": "Approach 1: Polls Only"
  },
  {
    "objectID": "slides/election-forecasting.html#approach-1-polls-only-3",
    "href": "slides/election-forecasting.html#approach-1-polls-only-3",
    "title": "Election Forecasting",
    "section": "Approach 1: Polls Only",
    "text": "Approach 1: Polls Only\n\nThe problem is that polling error is much wider than the Central Limit Theorem would predict!\nRecall: a poll’s margin of error is the range within which one can be 95% confident the polling error will fall.\nBack-of-the-envelope approximation of a poll’s margin of error is \\(\\frac{100\\%}{\\sqrt{n}}\\)\nIn this dataset of 6,407 polls, the result only fell within the margin of error 3,119 times.\n\nThat’s 48.7% confidence, not 95%!"
  },
  {
    "objectID": "slides/election-forecasting.html#approach-1-polls-only-4",
    "href": "slides/election-forecasting.html#approach-1-polls-only-4",
    "title": "Election Forecasting",
    "section": "Approach 1: Polls Only",
    "text": "Approach 1: Polls Only\n\nFor most polls, the margin of error gives us a false sense of security.\nReal-world polling is much more volatile than theory would predict.\nMany different reasons why:\n\nNon-response bias\nPeople change their minds\nPeople lie"
  },
  {
    "objectID": "slides/election-forecasting.html#approach-2-fundamentals-only",
    "href": "slides/election-forecasting.html#approach-2-fundamentals-only",
    "title": "Election Forecasting",
    "section": "Approach 2: Fundamentals Only",
    "text": "Approach 2: Fundamentals Only\n\nMaybe a fundamentals model will work better?\nThe problem here is a lack of good data.\nFor example, there’s no dataset on “Senator approval” like there was on presidential approval\nBut there are still some useful predictor variables we can include:\n\nThe Cook Political Report’s measure of a state’s partisan lean.\nWhether each Senate candidate has previous political experience (Carson et al. 2010)."
  },
  {
    "objectID": "slides/election-forecasting.html#approach-2-fundamentals-only-1",
    "href": "slides/election-forecasting.html#approach-2-fundamentals-only-1",
    "title": "Election Forecasting",
    "section": "Approach 2: Fundamentals Only",
    "text": "Approach 2: Fundamentals Only"
  },
  {
    "objectID": "slides/election-forecasting.html#approach-2-fundamentals",
    "href": "slides/election-forecasting.html#approach-2-fundamentals",
    "title": "Election Forecasting",
    "section": "Approach 2: Fundamentals",
    "text": "Approach 2: Fundamentals\nThe fundamentals model is off by 12.05 percentage points on average. Even worse than the polls!"
  },
  {
    "objectID": "slides/election-forecasting.html#approach-3-polls-fundamentals",
    "href": "slides/election-forecasting.html#approach-3-polls-fundamentals",
    "title": "Election Forecasting",
    "section": "Approach 3: Polls + Fundamentals",
    "text": "Approach 3: Polls + Fundamentals\n\nA polls-only approach had way higher error than we would expect from the Central Limit Theorem.\nThe fundamentals-only approach was weak sauce.\nWhat if we combined the two?\nOur final approach: use both polls and fundamentals as predictor variables in a linear model."
  },
  {
    "objectID": "slides/election-forecasting.html#approach-3-polls-fundamentals-1",
    "href": "slides/election-forecasting.html#approach-3-polls-fundamentals-1",
    "title": "Election Forecasting",
    "section": "Approach 3: Polls + Fundamentals",
    "text": "Approach 3: Polls + Fundamentals\nPredictor variables include:\n\nCook Partisan Voting Index (PVI)\nWhether the Democratic candidate has previous political experience (1 if yes, 0 if no).\nWhether the Republican candidate has previous political experience (1 if yes, 0 if no).\nThe average vote margin in polls conducted within 1 month of the election."
  },
  {
    "objectID": "slides/election-forecasting.html#approach-3-polls-fundamentals-2",
    "href": "slides/election-forecasting.html#approach-3-polls-fundamentals-2",
    "title": "Election Forecasting",
    "section": "Approach 3: Polls + Fundamentals",
    "text": "Approach 3: Polls + Fundamentals\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  -0.012\n                \n                \n                  polling_average\n                  0.911\n                \n                \n                  pvi\n                  0.004\n                \n                \n                  dem_experienced\n                  0.012\n                \n                \n                  rep_experienced\n                  0.001\n                \n                \n                  Num.Obs.\n                  223"
  },
  {
    "objectID": "slides/election-forecasting.html#approach-3-polls-fundamentals-3",
    "href": "slides/election-forecasting.html#approach-3-polls-fundamentals-3",
    "title": "Election Forecasting",
    "section": "Approach 3: Polls + Fundamentals",
    "text": "Approach 3: Polls + Fundamentals"
  },
  {
    "objectID": "slides/election-forecasting.html#approach-3-polls-fundamentals-4",
    "href": "slides/election-forecasting.html#approach-3-polls-fundamentals-4",
    "title": "Election Forecasting",
    "section": "Approach 3: Polls + Fundamentals",
    "text": "Approach 3: Polls + Fundamentals\n\nThe combined model has an average error of 3.88 percentage points.\nIt correctly calls the winner in 91.9% of elections from 1992-2016.\nBut the true test of the model is whether it can accurately forecast out-of-sample.\n\nThe model was fit using 1992-2016 election data. Can we use the same model to predict the outcomes in 2018?"
  },
  {
    "objectID": "slides/election-forecasting.html#out-of-sample-prediction",
    "href": "slides/election-forecasting.html#out-of-sample-prediction",
    "title": "Election Forecasting",
    "section": "Out-of-Sample Prediction",
    "text": "Out-of-Sample Prediction"
  },
  {
    "objectID": "slides/election-forecasting.html#out-of-sample-prediction-1",
    "href": "slides/election-forecasting.html#out-of-sample-prediction-1",
    "title": "Election Forecasting",
    "section": "Out-of-Sample Prediction",
    "text": "Out-of-Sample Prediction"
  },
  {
    "objectID": "slides/election-forecasting.html#out-of-sample-prediction-2",
    "href": "slides/election-forecasting.html#out-of-sample-prediction-2",
    "title": "Election Forecasting",
    "section": "Out-of-Sample Prediction",
    "text": "Out-of-Sample Prediction"
  },
  {
    "objectID": "slides/election-forecasting.html#out-of-sample-prediction-3",
    "href": "slides/election-forecasting.html#out-of-sample-prediction-3",
    "title": "Election Forecasting",
    "section": "Out-of-Sample Prediction",
    "text": "Out-of-Sample Prediction\n\nThe polls + fundamentals model correctly predicted all but two of the 2018 Senate elections, with an average error of 2.55 percentage points.\nBy comparison, if you had been looking at the polls alone, you would have incorrectly called three elections.\n\nIn Missouri 2018, the polls narrowly favored the Democratic incumbent, but the fundamentals were skeptical."
  },
  {
    "objectID": "slides/election-forecasting.html#takeaways",
    "href": "slides/election-forecasting.html#takeaways",
    "title": "Election Forecasting",
    "section": "Takeaways",
    "text": "Takeaways\n\nIt’s somewhat surprising that combining two terrible models (polls: 6.7% error, fundamentals: 12% error) yields a pretty good model (polls + fundamentals: 3.8% error)!\nBut it follows the basic logic of the wisdom of crowds.\nBoth polls and fundamentals provide an imperfect glimpse at the state of a race.\nCombining the two perspectives yields a better forecast than either alone (Chen, Garnett, and Montgomery 2023)."
  },
  {
    "objectID": "slides/election-forecasting.html#references",
    "href": "slides/election-forecasting.html#references",
    "title": "Election Forecasting",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nCarson, Jamie, Gregory Koger, Matthew J. Lebo, Jamie L, and Matthew J. Lebo. 2010. “The Electoral Costs of Party Loyalty in Congress.” American Journal of Political Science, 598616.\n\n\nChen, Yehu, Roman Garnett, and Jacob M. Montgomery. 2023. “Polls, Context, and Time: A Dynamic Hierarchical Bayesian Forecasting Model for US Senate Elections.” Political Analysis 31 (1): 113–33. https://doi.org/10.1017/pan.2021.42."
  },
  {
    "objectID": "slides/information-theory.html#lets-play-another-game",
    "href": "slides/information-theory.html#lets-play-another-game",
    "title": "Information Theory",
    "section": "Let’s Play Another Game!",
    "text": "Let’s Play Another Game!\n\nI’m going to write a sequence of three numbers on the board.\nYour goal is to guess what “rule” these numbers follow (e.g. “all the numbers must be positive”).\nYou can shout any sequence of three numbers at me, and I’ll tell you if it satisfies the rule.\nWhen you think you know the rule, write it down on a slip of paper and bring it up front.\n\nFirst person to guess correctly wins."
  },
  {
    "objectID": "slides/information-theory.html#discuss",
    "href": "slides/information-theory.html#discuss",
    "title": "Information Theory",
    "section": "Discuss",
    "text": "Discuss\nWhat did we learn about ourselves?"
  },
  {
    "objectID": "slides/information-theory.html#confirmation-bias",
    "href": "slides/information-theory.html#confirmation-bias",
    "title": "Information Theory",
    "section": "Confirmation Bias",
    "text": "Confirmation Bias\n\nConfirmation bias is the tendency of people to seek out information that confirms their pre-existing beliefs.\nThis is particularly true in when we’re arguing about politics (Taber and Lodge 2006).\n\n\n\n“So convenient a thing is it to be a rational creature, since it enables us to find or make a reason for everything one has a mind to.”\n- Benjamin Franklin"
  },
  {
    "objectID": "slides/information-theory.html#confirmation-bias-1",
    "href": "slides/information-theory.html#confirmation-bias-1",
    "title": "Information Theory",
    "section": "Confirmation Bias",
    "text": "Confirmation Bias\n\nThis tendency makes playing our opening game difficult.\nThe most common strategy is to focus in on a particular rule, then keep guessing number sequences that satisfy the rule.\nBut the optimal strategy is to guess sequences that narrow down the set of possible rules.\n\nIf you think the rule could be “double each number” or “all numbers are even”, don’t guess 4-8-16!"
  },
  {
    "objectID": "slides/information-theory.html#active-open-mindedness",
    "href": "slides/information-theory.html#active-open-mindedness",
    "title": "Information Theory",
    "section": "Active Open-Mindedness",
    "text": "Active Open-Mindedness\n\nThe opposite of confirmation bias is what psychologists call active open-minded thinking (AOMT)."
  },
  {
    "objectID": "slides/information-theory.html#active-open-minded-thinking",
    "href": "slides/information-theory.html#active-open-minded-thinking",
    "title": "Information Theory",
    "section": "Active Open-Minded Thinking",
    "text": "Active Open-Minded Thinking\nI’m skeptical that asking these questions is a great way to measure AOMT."
  },
  {
    "objectID": "slides/information-theory.html#active-open-minded-thinking-1",
    "href": "slides/information-theory.html#active-open-minded-thinking-1",
    "title": "Information Theory",
    "section": "Active Open-Minded Thinking",
    "text": "Active Open-Minded Thinking\nNot many people disagree with the statement: “A person should always consider new information.”"
  },
  {
    "objectID": "slides/information-theory.html#information",
    "href": "slides/information-theory.html#information",
    "title": "Information Theory",
    "section": "Information",
    "text": "Information\n\nWhat is “information”, anyway?\nLet’s briefly dip our toes into information theory, a branch of mathematics that deals with, well, information.\nIntuitively, information is something that shifts our beliefs.\nThe more surprising a piece of information is, the more it should shift our beliefs.\nIn a deep sense, information = surprise (Shannon 1948)."
  },
  {
    "objectID": "slides/information-theory.html#surprise",
    "href": "slides/information-theory.html#surprise",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise\n\nHow surprised would you be to learn that an event happened?\nDepends on how probable you thought the event was!"
  },
  {
    "objectID": "slides/information-theory.html#surprise-1",
    "href": "slides/information-theory.html#surprise-1",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise"
  },
  {
    "objectID": "slides/information-theory.html#surprise-2",
    "href": "slides/information-theory.html#surprise-2",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise\nIf you were 100% sure an event would happen, you wouldn’t be surprised at all to learn that it happened.\n\n“The sun rose in the east today.”"
  },
  {
    "objectID": "slides/information-theory.html#surprise-3",
    "href": "slides/information-theory.html#surprise-3",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise"
  },
  {
    "objectID": "slides/information-theory.html#surprise-4",
    "href": "slides/information-theory.html#surprise-4",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise\nIf you thought there was a 75% chance of it happening, you still wouldn’t be very surprised.\n\n“I found my shoes on the shoe rack this morning!”"
  },
  {
    "objectID": "slides/information-theory.html#surprise-5",
    "href": "slides/information-theory.html#surprise-5",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise"
  },
  {
    "objectID": "slides/information-theory.html#surprise-6",
    "href": "slides/information-theory.html#surprise-6",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise\nIf you thought there was a 50-50 chance, you’d be kinda surprised.\n\n“The coin landed on heads!”"
  },
  {
    "objectID": "slides/information-theory.html#surprise-7",
    "href": "slides/information-theory.html#surprise-7",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise"
  },
  {
    "objectID": "slides/information-theory.html#surprise-8",
    "href": "slides/information-theory.html#surprise-8",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise\nIf you thought there was only a 25% chance, now this would be surprising information."
  },
  {
    "objectID": "slides/information-theory.html#surprise-9",
    "href": "slides/information-theory.html#surprise-9",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise"
  },
  {
    "objectID": "slides/information-theory.html#surprise-10",
    "href": "slides/information-theory.html#surprise-10",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise\nIf you thought there was only a 5% chance, you’ve reached what statisticians would call “statistical significance”.\n\nYou’ve observed an event you really didn’t expect to see!\nMaybe your theory was wrong!"
  },
  {
    "objectID": "slides/information-theory.html#surprise-11",
    "href": "slides/information-theory.html#surprise-11",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise"
  },
  {
    "objectID": "slides/information-theory.html#surprise-12",
    "href": "slides/information-theory.html#surprise-12",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise\nIf you thought there was a 1-in-1,000 chance, now it’s downright shocking!\n\nLearning this information should change how you think about the world."
  },
  {
    "objectID": "slides/information-theory.html#surprise-13",
    "href": "slides/information-theory.html#surprise-13",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise"
  },
  {
    "objectID": "slides/information-theory.html#surprise-14",
    "href": "slides/information-theory.html#surprise-14",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise\n1-in-a-million chance?\n\nThis is, like, winning the lottery level of surprise.\nHeart-attack-inducing surprise."
  },
  {
    "objectID": "slides/information-theory.html#surprise-15",
    "href": "slides/information-theory.html#surprise-15",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise"
  },
  {
    "objectID": "slides/information-theory.html#surprise-16",
    "href": "slides/information-theory.html#surprise-16",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise\nNotice the shape of the curve we’re drawing here. Surprise increases exponentially as we get closer to \\(P(x) = 0\\)."
  },
  {
    "objectID": "slides/information-theory.html#surprise-17",
    "href": "slides/information-theory.html#surprise-17",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise"
  },
  {
    "objectID": "slides/information-theory.html#surprise-18",
    "href": "slides/information-theory.html#surprise-18",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise\n\nThis looks an awful lot like the charts I was showing you in the lecture on Long Tails.\nWhat I’m arguing is that the amount of information revealed by an event is a logarithmic function of how probable you thought it was."
  },
  {
    "objectID": "slides/information-theory.html#surprise-19",
    "href": "slides/information-theory.html#surprise-19",
    "title": "Information Theory",
    "section": "Surprise",
    "text": "Surprise"
  },
  {
    "objectID": "slides/information-theory.html#information-1",
    "href": "slides/information-theory.html#information-1",
    "title": "Information Theory",
    "section": "Information",
    "text": "Information\n\nThis idea motivates the mathematical definition of information content.\n\n\n\\[\nI(x) = -\\text{log}(P(x))\n\\]\n\nThis function tells you, in essence, how much you learn from observing a piece of information.\nA unit of information is called a bit."
  },
  {
    "objectID": "slides/information-theory.html#information-entropy",
    "href": "slides/information-theory.html#information-entropy",
    "title": "Information Theory",
    "section": "Information Entropy",
    "text": "Information Entropy\n\nThis leads us to a related concept, called entropy (“average surprise”).\n\n“On average, how much information do you expect to learn by observing an event?”\n\nAn event with probability 100% has zero entropy.\n\nBecause you get 0 bits of information when it happens. And it always happens.\n\nAn event with probability \\(\\frac{1}{1,000,000}\\) also has no entropy.\n\nYou get about 20 bits of information if it happens, but it basically never happens!"
  },
  {
    "objectID": "slides/information-theory.html#entropy",
    "href": "slides/information-theory.html#entropy",
    "title": "Information Theory",
    "section": "Entropy",
    "text": "Entropy"
  },
  {
    "objectID": "slides/information-theory.html#entropy-1",
    "href": "slides/information-theory.html#entropy-1",
    "title": "Information Theory",
    "section": "Entropy",
    "text": "Entropy\n\nWhen seeking out information, your goal should be to decrease entropy as much as possible.\nThis idea will be particularly useful after the midterm, when we start discussing machine learning.\nBut we can also apply it to the game we played at the start of class."
  },
  {
    "objectID": "slides/information-theory.html#guess-the-rule",
    "href": "slides/information-theory.html#guess-the-rule",
    "title": "Information Theory",
    "section": "Guess The Rule",
    "text": "Guess The Rule\n\nI observe that “2-4-8” satisfies the rule.\nA lot of potential rules here: (1) numbers go up, (2) second number goes up, (3) third number goes up, (4) all evens, (5) all positives, (6) sums to 14\nIf I think these six rules are equally likely, then my entropy is \\(-\\text{log}(\\frac{1}{6}) \\approx 2.6\\).\nThe move that decreases entropy the most is one that splits the set of hypotheses in half, like “8-4-2”. No matter what happens, entropy will decrease to \\(-\\text{log}(\\frac{1}{3}) \\approx 1.6\\)."
  },
  {
    "objectID": "slides/information-theory.html#wisdom-of-crowds",
    "href": "slides/information-theory.html#wisdom-of-crowds",
    "title": "Information Theory",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds\n\nHere’s a final perspective on what’s happening with the “wisdom of crowds”.\nIndividuals are prone to confirmation bias. We tend to look for information that confirms our theories (“4-8-16”).\nBut in a large enough crowd of people, you’ll end up with a diverse set of theories.\nIf everyone looks for information that confirms their theory, then the group ends up finding a bunch of information that collectively decreases entropy."
  },
  {
    "objectID": "slides/information-theory.html#for-more-information-theory",
    "href": "slides/information-theory.html#for-more-information-theory",
    "title": "Information Theory",
    "section": "For more information theory…",
    "text": "For more information theory…"
  },
  {
    "objectID": "slides/information-theory.html#references",
    "href": "slides/information-theory.html#references",
    "title": "Information Theory",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nShannon, C E. 1948. “A Mathematical Theory of Communication.” The Bell System Technical Journal 27: 379–423.\n\n\nTaber, Charles S., and Milton Lodge. 2006. “Motivated Skepticism in the Evaluation of Political Beliefs.” American Journal of Political Science 50 (3): 755–69. https://doi.org/10.1111/j.1540-5907.2006.00214.x."
  },
  {
    "objectID": "slides/linear-models.html#road-map",
    "href": "slides/linear-models.html#road-map",
    "title": "Linear Models",
    "section": "Road Map",
    "text": "Road Map\n\nIn the first half of the semester, we learned the fundamentals of probabilistic forecasting.\nWe also showed how “crowds” of forecasters tend to outperform individuals.\nOver the next three weeks, we’ll apply these insights to learn the basics of machine learning."
  },
  {
    "objectID": "slides/linear-models.html#machine-learning",
    "href": "slides/linear-models.html#machine-learning",
    "title": "Linear Models",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nThe term machine learning refers to any computer algorithm that:\n\nidentifies patterns in data, and\nuses those patterns to make predictions.\n\nThis is a really broad definition!"
  },
  {
    "objectID": "slides/linear-models.html#machine-learning-1",
    "href": "slides/linear-models.html#machine-learning-1",
    "title": "Linear Models",
    "section": "Machine Learning",
    "text": "Machine Learning"
  },
  {
    "objectID": "slides/linear-models.html#machine-learning-2",
    "href": "slides/linear-models.html#machine-learning-2",
    "title": "Linear Models",
    "section": "Machine Learning",
    "text": "Machine Learning"
  },
  {
    "objectID": "slides/linear-models.html#machine-learning-3",
    "href": "slides/linear-models.html#machine-learning-3",
    "title": "Linear Models",
    "section": "Machine Learning",
    "text": "Machine Learning"
  },
  {
    "objectID": "slides/linear-models.html#todays-agenda",
    "href": "slides/linear-models.html#todays-agenda",
    "title": "Linear Models",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nWe’ll start with perhaps the simplest machine learning approach: the linear model.\nSimple \\(\\neq\\) bad!\n\nSimple models frequently outperform complicated models at prediction.\n\nWe’ll describe how linear models works, and discuss three potential dangers associated with using them."
  },
  {
    "objectID": "slides/linear-models.html#motivating-example",
    "href": "slides/linear-models.html#motivating-example",
    "title": "Linear Models",
    "section": "Motivating Example",
    "text": "Motivating Example\n\nSuppose it is June in a presidential election year, and you want to predict which candidate will win.\nCan you make a reasonably accurate prediction about November’s election based only on information you have in June?\nLet’s start with an “Outside View”. Are there variables that, historically, have been predictive of presidential election results five months in advance?\nDiscuss: what information would you want to know before making your prediction?"
  },
  {
    "objectID": "slides/linear-models.html#presidential-election-forecast",
    "href": "slides/linear-models.html#presidential-election-forecast",
    "title": "Linear Models",
    "section": "Presidential Election Forecast",
    "text": "Presidential Election Forecast"
  },
  {
    "objectID": "slides/linear-models.html#presidential-election-forecast-1",
    "href": "slides/linear-models.html#presidential-election-forecast-1",
    "title": "Linear Models",
    "section": "Presidential Election Forecast",
    "text": "Presidential Election Forecast"
  },
  {
    "objectID": "slides/linear-models.html#presidential-election-forecast-2",
    "href": "slides/linear-models.html#presidential-election-forecast-2",
    "title": "Linear Models",
    "section": "Presidential Election Forecast",
    "text": "Presidential Election Forecast\n\nThe equation describing that line is 3.27 + 0.37 \\(\\times \\text{June Approval}\\)\nFor every 1-unit increase in presidential approval, we would predict an additional 0.37 in vote margin for the incumbent’s party.\nBut where did those numbers come from? What is the “line of best fit”?\nThe line of best fit is the line that minimizes average squared error.\n\nSound familiar?"
  },
  {
    "objectID": "slides/linear-models.html#presidential-election-forecast-3",
    "href": "slides/linear-models.html#presidential-election-forecast-3",
    "title": "Linear Models",
    "section": "Presidential Election Forecast",
    "text": "Presidential Election Forecast"
  },
  {
    "objectID": "slides/linear-models.html#presidential-election-forecast-4",
    "href": "slides/linear-models.html#presidential-election-forecast-4",
    "title": "Linear Models",
    "section": "Presidential Election Forecast",
    "text": "Presidential Election Forecast"
  },
  {
    "objectID": "slides/linear-models.html#presidential-election-forecast-5",
    "href": "slides/linear-models.html#presidential-election-forecast-5",
    "title": "Linear Models",
    "section": "Presidential Election Forecast",
    "text": "Presidential Election Forecast\n\nWe can make predictions using this model by plugging in values for future elections.\nIn June 2016, the incumbent president’s net approval rating was +5%.\nSo the model would predict 3.27 + 0.37 \\(\\times 5 = 5.12\\%\\) vote margin for the incumbent party."
  },
  {
    "objectID": "slides/linear-models.html#presidential-election-forecast-6",
    "href": "slides/linear-models.html#presidential-election-forecast-6",
    "title": "Linear Models",
    "section": "Presidential Election Forecast",
    "text": "Presidential Election Forecast"
  },
  {
    "objectID": "slides/linear-models.html#presidential-election-forecast-7",
    "href": "slides/linear-models.html#presidential-election-forecast-7",
    "title": "Linear Models",
    "section": "Presidential Election Forecast",
    "text": "Presidential Election Forecast\n\nThe average forecast error is 4.7 percentage points.\nAnd we have some particularly large errors in 1972 (11.9 points) and 1984 (8.5 points).\nMaybe we’d do better if we added more predictor variables to the model?"
  },
  {
    "objectID": "slides/linear-models.html#presidential-election-forecast-8",
    "href": "slides/linear-models.html#presidential-election-forecast-8",
    "title": "Linear Models",
    "section": "Presidential Election Forecast",
    "text": "Presidential Election Forecast"
  },
  {
    "objectID": "slides/linear-models.html#presidential-election-forecast-9",
    "href": "slides/linear-models.html#presidential-election-forecast-9",
    "title": "Linear Models",
    "section": "Presidential Election Forecast",
    "text": "Presidential Election Forecast\n\nCombining the presidential approval and economic growth into the same linear model yields this equation:\n\n0.33 + 0.3 \\(\\times \\text{June Approval}\\) + 0.94 \\(\\times \\text{Q2 GDP Growth}\\)\n\nIn 2016, GDP growth was a sluggish 1.2% in the second quarter.\nSo the revised model would predict an incumbent margin of 2.96 percentage points. Much better!"
  },
  {
    "objectID": "slides/linear-models.html#the-plane-of-best-fit",
    "href": "slides/linear-models.html#the-plane-of-best-fit",
    "title": "Linear Models",
    "section": "The “Plane of Best Fit”",
    "text": "The “Plane of Best Fit”"
  },
  {
    "objectID": "slides/linear-models.html#presidential-election-forecast-10",
    "href": "slides/linear-models.html#presidential-election-forecast-10",
    "title": "Linear Models",
    "section": "Presidential Election Forecast",
    "text": "Presidential Election Forecast\n\nThe kind of forecast model we’ve been building here is called a fundamentals model.\nNotice that it doesn’t use polling at all! \nIt makes predictions purely based on “fundamentals”, historical patterns in the data (Abramowitz 2021) .\nIn a future lecture, we’ll show how combining polls + fundamentals yields better predictions than either alone."
  },
  {
    "objectID": "slides/linear-models.html#three-dangers",
    "href": "slides/linear-models.html#three-dangers",
    "title": "Linear Models",
    "section": "Three Dangers",
    "text": "Three Dangers\n\nSimple linear models can be a surprisingly useful tool for making predictions.\nBut when using a linear model, keep in mind three dangers that could ruin your forecasts:\n\nNonlinearity\nExtrapolation\nStructural Stability"
  },
  {
    "objectID": "slides/linear-models.html#danger-1-nonlinearity",
    "href": "slides/linear-models.html#danger-1-nonlinearity",
    "title": "Linear Models",
    "section": "Danger 1: Nonlinearity",
    "text": "Danger 1: Nonlinearity\n\nThe linear model faithfully gives you the line of best fit…\n\n\n\n\n\n\n\n\n\n\n\n…even when a straight line is a terrible model!\n\nDiagnostic: are there patterns in the errors?"
  },
  {
    "objectID": "slides/linear-models.html#danger-2-extrapolation",
    "href": "slides/linear-models.html#danger-2-extrapolation",
    "title": "Linear Models",
    "section": "Danger 2: Extrapolation",
    "text": "Danger 2: Extrapolation\n\nBe cautious of making predictions with a linear model if the current situation lies far outside the historical data.\n\n\n\n\n\n\n\n\nPlugging these numbers into our model would predict the incumbent losing by 35.2 percentage points! (Actual margin was -4.5 points)."
  },
  {
    "objectID": "slides/linear-models.html#danger-3-structural-stability",
    "href": "slides/linear-models.html#danger-3-structural-stability",
    "title": "Linear Models",
    "section": "Danger 3: Structural Stability",
    "text": "Danger 3: Structural Stability\n\nPredicting with a linear model assumes that the relationships observed in your data will be stable over time.\nIn other words, you’re assuming that the future will follow the same rules as the past.\nMaybe as voters have become more polarized, the relationship between economic growth and voting behavior has gotten weaker?\n\nIf so, our fundamentals model will get worse and worse at predicting.\n\nThis is closely related to a problem we’ll discuss in more detail next lecture: overfitting."
  },
  {
    "objectID": "slides/linear-models.html#looking-forward",
    "href": "slides/linear-models.html#looking-forward",
    "title": "Linear Models",
    "section": "Looking Forward",
    "text": "Looking Forward\n\nNext time, I’ll show you a slightly more complex machine learning approach that’s better at handling nonlinearity: classification and regression trees (CART).\nWe’ll discuss the balance between overfitting and underfitting machine learning models, and how it relates to the bias-variance tradeoff we introduced previously."
  },
  {
    "objectID": "slides/linear-models.html#references",
    "href": "slides/linear-models.html#references",
    "title": "Linear Models",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nAbramowitz, Alan I. 2021. “It’s the Pandemic, Stupid! A Simplified Model for Forecasting the 2020 Presidential Election.” PS: Political Science & Politics 54 (1): 52–54. https://doi.org/10.1017/S1049096520001389."
  },
  {
    "objectID": "slides/madness-of-crowds.html#wisdom-of-crowds",
    "href": "slides/madness-of-crowds.html#wisdom-of-crowds",
    "title": "Madness of Crowds",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds"
  },
  {
    "objectID": "slides/madness-of-crowds.html#wisdom-of-crowds-1",
    "href": "slides/madness-of-crowds.html#wisdom-of-crowds-1",
    "title": "Madness of Crowds",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds"
  },
  {
    "objectID": "slides/madness-of-crowds.html#wisdom-of-crowds-2",
    "href": "slides/madness-of-crowds.html#wisdom-of-crowds-2",
    "title": "Madness of Crowds",
    "section": "Wisdom of Crowds",
    "text": "Wisdom of Crowds"
  },
  {
    "objectID": "slides/madness-of-crowds.html#todays-agenda",
    "href": "slides/madness-of-crowds.html#todays-agenda",
    "title": "Madness of Crowds",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nIf you can engineer conditions so that you have a large group of independent and competent individuals, then your crowd will be wise.\n\nCongratulations!\n\nBut in most real-world groups, those assumptions are rarely met.\nToday, a few violations of the independence assumption, and how it can affect group decision-making."
  },
  {
    "objectID": "slides/madness-of-crowds.html#lets-play-a-game",
    "href": "slides/madness-of-crowds.html#lets-play-a-game",
    "title": "Madness of Crowds",
    "section": "Let’s Play A Game",
    "text": "Let’s Play A Game\n\nI need 10 volunteers to play a little game.\nI have two bags, and will randomly choose one to play with:\n\nBag A has 2 red poker chips and 1 blue poker chip.\nBag B has 1 red poker chip and 2 blue poker chips.\n\nOn your turn, secretly draw a chip from the bag, put it back, then write your name on the board.\n\nUse blue marker if you think the bag is mostly blue.\nUse red marker if you think the bag is mostly red.\n\n1 bonus point on the midterm if you predict correctly."
  },
  {
    "objectID": "slides/madness-of-crowds.html#information-cascades",
    "href": "slides/madness-of-crowds.html#information-cascades",
    "title": "Madness of Crowds",
    "section": "Information Cascades",
    "text": "Information Cascades\n\nAn information cascade occurs whenever individuals ignore their private information and go with the group consensus.\nThis can be a perfectly rational decision from a Bayesian perspective.\n\nIf the first two students draw blue chips, posterior odds are \\(1:1 \\times 2:1 \\times 2:1 = 4:1\\) that it’s the blue bag.\nIf the third student draws a blue chip, posterior odds are \\(8:1\\). Guess blue!\nIf the third student draws a red chip, posterior odds are \\(2:1\\). Guess blue!"
  },
  {
    "objectID": "slides/madness-of-crowds.html#information-cascades-1",
    "href": "slides/madness-of-crowds.html#information-cascades-1",
    "title": "Madness of Crowds",
    "section": "Information Cascades",
    "text": "Information Cascades\n\nInformation cascades don’t always cause the group to get the wrong answer.\nBut they do illustrate how hard it is to make a truly independent prediction in a group setting.\nIf we can observe the predictions that other people make, there’s a strong rational basis for ignoring our individual information and just going with the group."
  },
  {
    "objectID": "slides/madness-of-crowds.html#social-influence",
    "href": "slides/madness-of-crowds.html#social-influence",
    "title": "Madness of Crowds",
    "section": "Social Influence",
    "text": "Social Influence\nEven when there is no rational basis for doing so, we still might face strong pressures to conform with the group…"
  },
  {
    "objectID": "slides/madness-of-crowds.html#groupthink",
    "href": "slides/madness-of-crowds.html#groupthink",
    "title": "Madness of Crowds",
    "section": "Groupthink",
    "text": "Groupthink\nWhen individuals suppress their private information for the sake of group harmony/cohesion, we call it groupthink.\n\n\nDiscuss: Have you ever been part of a group like this? How can we design our teams to avoid groupthink?\n\nhttps://mediationworksfl.com/going-with-the-flow-the-influence-of-groupthink-on-mediation-outcomes/"
  },
  {
    "objectID": "slides/madness-of-crowds.html#groupthink-1",
    "href": "slides/madness-of-crowds.html#groupthink-1",
    "title": "Madness of Crowds",
    "section": "Groupthink",
    "text": "Groupthink\n\nHistory is full of examples of catastrophic predictions / decisions made by groups that valued harmony over accuracy:\n\nBay of Pigs invasion (1961)\nPearl Harbor attacks (1941)\nSpace Shuttle Challenger disaster (1986)\n\nThese are just a few prominent cases where people kept their private doubts quiet to conform with the group consensus."
  },
  {
    "objectID": "slides/madness-of-crowds.html#takeaways",
    "href": "slides/madness-of-crowds.html#takeaways",
    "title": "Madness of Crowds",
    "section": "Takeaways",
    "text": "Takeaways\n\nWe don’t naturally design our groups in a way that promotes independence.\n\nWhen people share their predictions but not their reasoning, a common result is information cascades.\nWe are social creatures, and so face strong pressures towards group conformity.\n\nIf you want to harness the wisdom of crowds, give individuals the freedom and motivation to work independently and report their information honestly."
  },
  {
    "objectID": "slides/tree-models.html#todays-agenda",
    "href": "slides/tree-models.html#todays-agenda",
    "title": "Tree Models",
    "section": "",
    "text": "Last time, we introduced linear models as a machine learning tool.\n\nThe key idea was to make predictions based on a linear combination of predictor variables.\n\nToday, we’ll discuss an approach that combines variables in a nonlinear fashion, called classification and regression trees (CART).\n\nIt looks exactly like the probability trees we worked with in the first half of the semester.\nExcept we let computer to build them for us!"
  },
  {
    "objectID": "slides/tree-models.html#motivating-problem",
    "href": "slides/tree-models.html#motivating-problem",
    "title": "Tree Models",
    "section": "Motivating Problem",
    "text": "Motivating Problem\n\n\nCan we predict a movie’s Rotten Tomatoes rating?\nPredictor Variables: Content Rating, Genre, Release Date, Distributor, Runtime, Number of Audience Reviews"
  },
  {
    "objectID": "slides/tree-models.html#outside-view",
    "href": "slides/tree-models.html#outside-view",
    "title": "Tree Models",
    "section": "Outside View",
    "text": "Outside View\nIf we had no other information, what would we predict based solely on historical base rates?\n. . .\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Rating\n                Total\n                %\n              \n        \n        \n        \n                \n                  \n                  3,259\n                  18.4\n                \n                \n                  \n                  6,844\n                  38.7\n                \n                \n                  \n                  7,565\n                  42.8\n                \n        \n      \n    \n\n\n\n. . .\nWe can do better!"
  },
  {
    "objectID": "slides/tree-models.html#classification-trees",
    "href": "slides/tree-models.html#classification-trees",
    "title": "Tree Models",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nA classification tree works by recursively partitioning the dataset (i.e. repeatedly splitting the data into smaller subsets).\n\n. . .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll Movies\n\n\n\n\n\n\n\nCriterion/Searchlight/A24\n\n\n\n\n\n\n\nOther Distributor\n\n\n\n\n\n\n\nAudience Count &gt;= 10,000\n\n\n\n\n\n\n\nAudience Count &lt; 10,000\n\n\n\n\n\n\n\nPG\n\n\n\n\n\n\n\nOther Rating\n\n\n\n\n\n\n\nPG-13 or R\n\n\n\n\n\n\n\nOther Rating"
  },
  {
    "objectID": "slides/tree-models.html#classification-trees-1",
    "href": "slides/tree-models.html#classification-trees-1",
    "title": "Tree Models",
    "section": "Classification Trees",
    "text": "Classification Trees\nWith each partition, your prediction becomes less biased.\n. . .\n\n\nCriterion, Searchlight, or A24\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Rating\n                Total\n                %\n              \n        \n        \n        \n                \n                  \n                  118\n                  37.2\n                \n                \n                  \n                  135\n                  42.6\n                \n                \n                  \n                  64\n                  20.2\n                \n        \n      \n    \n\n\n\n\nOther Distributors\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Rating\n                Total\n                %\n              \n        \n        \n        \n                \n                  \n                  3,105\n                  18.4\n                \n                \n                  \n                  6,396\n                  37.9\n                \n                \n                  \n                  7,357\n                  43.6"
  },
  {
    "objectID": "slides/tree-models.html#classification-trees-2",
    "href": "slides/tree-models.html#classification-trees-2",
    "title": "Tree Models",
    "section": "Classification Trees",
    "text": "Classification Trees\nWith each partition, your prediction becomes less biased.\n. . .\n\n\nAudience Count &lt; 10,000\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Rating\n                Total\n                %\n              \n        \n        \n        \n                \n                  \n                  1,293\n                  12.0\n                \n                \n                  \n                  5,052\n                  46.9\n                \n                \n                  \n                  4,427\n                  41.1\n                \n        \n      \n    \n\n\n\n\nAudience Count &gt;= 10,000\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Rating\n                Total\n                %\n              \n        \n        \n        \n                \n                  \n                  1,811\n                  30.8\n                \n                \n                  \n                  1,193\n                  20.3\n                \n                \n                  \n                  2,870\n                  48.9"
  },
  {
    "objectID": "slides/tree-models.html#choosing-splits",
    "href": "slides/tree-models.html#choosing-splits",
    "title": "Tree Models",
    "section": "Choosing Splits",
    "text": "Choosing Splits\n\nWhat is the “best” way to partition the data?\nUp to now, we’ve been choosing our splits based purely on vibes.\nA more principled way is to partition the data based on how much information it reveals."
  },
  {
    "objectID": "slides/tree-models.html#information-theory-review",
    "href": "slides/tree-models.html#information-theory-review",
    "title": "Tree Models",
    "section": "Information Theory: Review",
    "text": "Information Theory: Review\nRecall the definitions of information and entropy.\n\nInformation is how surprised we are when we learn an outcome. Equal to \\(-log(p)\\).\nEntropy is “expected surprise”: how much surprise we experience on average when we learn the outcome.\nIf we could perfectly predict the Rotten Tomatoes rating a of movie, then entropy would be zero. (We would never be surprised).\nTherefore, our optimal strategy is to seek out information that reduces entropy as much as possible."
  },
  {
    "objectID": "slides/tree-models.html#information-theory-review-1",
    "href": "slides/tree-models.html#information-theory-review-1",
    "title": "Tree Models",
    "section": "Information Theory: Review",
    "text": "Information Theory: Review\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Rating\n                Total\n                p\n              \n        \n        \n        \n                \n                  \n                  3,259\n                  0.184\n                \n                \n                  \n                  6,844\n                  0.387\n                \n                \n                  \n                  7,565\n                  0.428"
  },
  {
    "objectID": "slides/tree-models.html#information-theory-review-2",
    "href": "slides/tree-models.html#information-theory-review-2",
    "title": "Tree Models",
    "section": "Information Theory: Review",
    "text": "Information Theory: Review\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Rating\n                Total\n                p\n                -log(p)\n              \n        \n        \n        \n                \n                  \n                  3,259\n                  0.184\n                  2.44\n                \n                \n                  \n                  6,844\n                  0.387\n                  1.37\n                \n                \n                  \n                  7,565\n                  0.428\n                  1.22\n                \n        \n      \n    \n\n\n\n. . .\n\\(\\text{Entropy} = -\\sum p\\times log(p)=\\) 1.5 bits."
  },
  {
    "objectID": "slides/tree-models.html#information-gain",
    "href": "slides/tree-models.html#information-gain",
    "title": "Tree Models",
    "section": "Information Gain",
    "text": "Information Gain\n\nEach time you partition the dataset, it reduces entropy.\nThis is because you become more certain about your prediction, therefore less likely to be surprised!\nThe information gain from a partition equals how much it reduces entropy."
  },
  {
    "objectID": "slides/tree-models.html#information-gain-1",
    "href": "slides/tree-models.html#information-gain-1",
    "title": "Tree Models",
    "section": "Information Gain",
    "text": "Information Gain\n\n\nAudience Count &lt; 9,000\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Rating\n                Total\n                %\n              \n        \n        \n        \n                \n                  \n                  1,328\n                  12.1\n                \n                \n                  \n                  5,161\n                  47.1\n                \n                \n                  \n                  4,462\n                  40.7\n                \n        \n      \n    \n\n\n\nEntropy: 1.41 bits\n\nAudience Count &gt;= 9,000\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Rating\n                Total\n                %\n              \n        \n        \n        \n                \n                  \n                  1,894\n                  31.5\n                \n                \n                  \n                  1,219\n                  20.3\n                \n                \n                  \n                  2,899\n                  48.2\n                \n        \n      \n    \n\n\n\nEntropy: 1.5 bits"
  },
  {
    "objectID": "slides/tree-models.html#classification-trees-3",
    "href": "slides/tree-models.html#classification-trees-3",
    "title": "Tree Models",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nTrying to find the best partition by hand would be tremendously tedious.\nBut computers are great at it.\nWhen creating a classification tree, the computer will check thousands of possible partitions, see how much it reduces entropy, and pick the best one."
  },
  {
    "objectID": "slides/tree-models.html#classification-trees-4",
    "href": "slides/tree-models.html#classification-trees-4",
    "title": "Tree Models",
    "section": "Classification Trees",
    "text": "Classification Trees"
  },
  {
    "objectID": "slides/tree-models.html#how-complex-should-the-tree-get",
    "href": "slides/tree-models.html#how-complex-should-the-tree-get",
    "title": "Tree Models",
    "section": "How Complex Should The Tree Get?",
    "text": "How Complex Should The Tree Get?"
  },
  {
    "objectID": "slides/tree-models.html#bias-variance-tradeoff",
    "href": "slides/tree-models.html#bias-variance-tradeoff",
    "title": "Tree Models",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\nIt’s our old friend, the bias-variance tradeoff!\nOutside View (zero partitions) is biased.\nInside View (lots of partitions) has higher variance, because your predictions are based on less data.\nSweet spot is somewhere in the middle."
  },
  {
    "objectID": "slides/tree-models.html#bias-variance-tradeoff-1",
    "href": "slides/tree-models.html#bias-variance-tradeoff-1",
    "title": "Tree Models",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\nIn machine learning, this tradeoff is called overfitting vs. underfitting.\n\n. . ."
  },
  {
    "objectID": "slides/tree-models.html#next-time",
    "href": "slides/tree-models.html#next-time",
    "title": "Tree Models",
    "section": "Next Time",
    "text": "Next Time\n\nNext time, we’ll talk about how to hit that “sweet spot” between overfitting and underfitting.\nAnd we’ll show how you can harness the wisdom of crowds with machine learning models.\nSpoiler alert: we’re going to take a bunch of different classification trees and ask them to “vote” on the best prediction.\nThe resulting machine learning model is called random forest."
  },
  {
    "objectID": "slides/random-forests.html#recap",
    "href": "slides/random-forests.html#recap",
    "title": "Random Forests",
    "section": "Recap",
    "text": "Recap\n\nPreviously, we introduced classification trees as a machine learning approach.\n\nA flowchart-like prediction model.\nSplit the data into smaller and smaller subgroups based on predictor variables.\nThe “best” tree is the one that minimizes information entropy at each split.\n\nThe problem with trees is that they can get too complex.\n\nSuch trees overfit to patterns in the dataset, and may do a poor job predicting out-of-sample."
  },
  {
    "objectID": "slides/random-forests.html#todays-agenda",
    "href": "slides/random-forests.html#todays-agenda",
    "title": "Random Forests",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nWe’ll introduce an approach that solves this overfitting problem in an clever and surprising way.\nIt’s called random forest (Breiman 2001).\n\nAn ensemble approach.\nCreates a large number of classification trees and then asks them to “vote” on the correct prediction.\n\nA few reasons I want to teach you this machine learning method:\n\nNearly state-of-the-art prediction performance in machine learning competitions.\nWorks very well “out-of-the-box”. Can implement it with just a few lines of code.\nIf you understand how classification trees work, then it’s straightforward to grasp what random forest is doing."
  },
  {
    "objectID": "slides/random-forests.html#references",
    "href": "slides/random-forests.html#references",
    "title": "Random Forests",
    "section": "References",
    "text": "References\n\n\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. https://doi.org/10.1023/A:1010933404324."
  },
  {
    "objectID": "slides/random-forests.html#ensemble-models",
    "href": "slides/random-forests.html#ensemble-models",
    "title": "Random Forests",
    "section": "Ensemble Models",
    "text": "Ensemble Models"
  },
  {
    "objectID": "slides/random-forests.html#ensemble-models-1",
    "href": "slides/random-forests.html#ensemble-models-1",
    "title": "Random Forests",
    "section": "Ensemble Models",
    "text": "Ensemble Models\n\nWhen would we expect an ensemble approach—a large number of models voting on the correct prediction—to yield better predictions than a single model?\nCondorcet Jury Theorem strikes again! Majority rule yields good predictions when:\n\nNumber of models is large\n\\(P(\\text{correct}) &gt; 0.5\\)\nPredictions are made independently\n\nAs always, independence is the trickiest criterion to satisfy.\n\nIf you train 500 classification trees on the same dataset, they won’t be independent at all.\nIn fact, they’ll be identical!\nThe problem is that they’re making predictions on the basis of the exact same set of information."
  },
  {
    "objectID": "slides/random-forests.html#the-rotten-tomatoes-random-forest",
    "href": "slides/random-forests.html#the-rotten-tomatoes-random-forest",
    "title": "Random Forests",
    "section": "The Rotten Tomatoes Random Forest",
    "text": "The Rotten Tomatoes Random Forest\n\n\nPredictor variables included: release date, genre, content rating, director, actors, distributor, runtime, and audience count.\nI built 500 classification trees, each trained on a random subset of the data.\nI asked each tree to predict the probability that a movie would fall into one of the three rating categories (Certified Fresh, Fresh, or Rotten).\nFollowing best practices, I hid 20% of the data from the model as a test set, to assess how well it performs at out-of-sample prediction."
  },
  {
    "objectID": "slides/random-forests.html#out-of-sample-prediction",
    "href": "slides/random-forests.html#out-of-sample-prediction",
    "title": "Random Forests",
    "section": "Out-of-Sample Prediction",
    "text": "Out-of-Sample Prediction"
  },
  {
    "objectID": "slides/random-forests.html#out-of-sample-prediction-1",
    "href": "slides/random-forests.html#out-of-sample-prediction-1",
    "title": "Random Forests",
    "section": "Out-of-Sample Prediction",
    "text": "Out-of-Sample Prediction"
  },
  {
    "objectID": "slides/random-forests.html#out-of-sample-prediction-2",
    "href": "slides/random-forests.html#out-of-sample-prediction-2",
    "title": "Random Forests",
    "section": "Out-of-Sample Prediction",
    "text": "Out-of-Sample Prediction"
  },
  {
    "objectID": "slides/random-forests.html#but-what-about-roofman",
    "href": "slides/random-forests.html#but-what-about-roofman",
    "title": "Random Forests",
    "section": "But What About Roofman???",
    "text": "But What About Roofman???"
  },
  {
    "objectID": "slides/random-forests.html#but-what-about-roofman-1",
    "href": "slides/random-forests.html#but-what-about-roofman-1",
    "title": "Random Forests",
    "section": "But What About Roofman???",
    "text": "But What About Roofman???\nAccording to the Rotten Tomatoes Random Forest model, Roofman had a 69% chance of earning a Certified Fresh rating, and a 14% chance of being just regular Fresh."
  },
  {
    "objectID": "slides/random-forests.html#random-forest",
    "href": "slides/random-forests.html#random-forest",
    "title": "Random Forests",
    "section": "Random Forest",
    "text": "Random Forest\n\nRandom forest uses a clever trick to train a large number of independent trees.\nEach classification tree is trained on a random subset of the data, and can only use a random selection of predictor variables at each split.\n\nThis is why it’s called “random” forest.\n\nThe result is a forest of classification trees, each of which is flawed in some way.\nBut as long as each individual tree is more likely than not to get the right answer, the forest as a whole willl be wise."
  },
  {
    "objectID": "slides/random-forests.html#advantages-of-random-forest",
    "href": "slides/random-forests.html#advantages-of-random-forest",
    "title": "Random Forests",
    "section": "Advantages of Random Forest",
    "text": "Advantages of Random Forest\n\nIndividual classification trees are almost certainly flawed in some way (either underfit or overfit).\nBut taking the average judgment of a large number of trees, trained on slightly different sets of data and predictor variables, will tend to cancel out their idiosyncratic errors.\nNearly every state-of-the-art machine learning technique today incorporates this insight.\nEnsemble models almost always outperform a single complicated model."
  },
  {
    "objectID": "slides/random-forests.html#limitations",
    "href": "slides/random-forests.html#limitations",
    "title": "Random Forests",
    "section": "Limitations",
    "text": "Limitations\n\nRequires a lot of data.\nNot very transparent.\n\nUnlike the linear model, where it’s clear why it’s making a prediction, models like random forest are too complicated to get a good sense of what’s going on inside.\nI don’t really have a clear sense of why the model gave Roofman such a high probability of critical acclaim.\n\nEvery machine learning approach rests on a crucial assumption:\n\n“The future will be similar to the past”\nFor a machine learning model to make good predictions, we must assume that the patterns we observe in historical data will continue to hold true going forward.\nMachine learning struggles to extrapolate beyond its training data.\nIf you encounter a truly unprecedented situation unlike anything in the training data, be skeptical of model-based predictions."
  },
  {
    "objectID": "slides/random-forests.html#next-time",
    "href": "slides/random-forests.html#next-time",
    "title": "Random Forests",
    "section": "Next Time",
    "text": "Next Time\n\nBefore class next time, follow the instructions here to download R and RStudio on your computer.\nBring it to class, and we’ll start learning some basics of working with datasets and fitting machine learning models!"
  },
  {
    "objectID": "index.html#week-9-1014",
    "href": "index.html#week-9-1014",
    "title": "Fall 2025 Predictions",
    "section": "Week 9 (10/14)",
    "text": "Week 9 (10/14)"
  },
  {
    "objectID": "slides/tree-models.html",
    "href": "slides/tree-models.html",
    "title": "Tree Models",
    "section": "",
    "text": "Last time, we introduced linear models as a machine learning tool.\n\nThe key idea was to make predictions based on a linear combination of predictor variables.\n\nToday, we’ll discuss an approach that combines variables in a nonlinear fashion, called classification and regression trees (CART).\n\nIt looks exactly like the probability trees we worked with in the first half of the semester.\nExcept we let computer to build them for us!"
  },
  {
    "objectID": "slides/election-forecasting.html",
    "href": "slides/election-forecasting.html",
    "title": "Election Forecasting",
    "section": "",
    "text": "Today, we’ll develop a model to predict the outcome of US Senate elections.\nThis is a tough forecasting problem compared to presidential elections, for two reasons:\n\nRelatively few polls\n“Fundamentals” are weakly predictive\n\nCombining polls + fundamentals will yield much better predictions than either alone!"
  }
]