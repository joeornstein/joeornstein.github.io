[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Joseph T. Ornstein",
    "section": "",
    "text": "Blasingame, Elise N., Christina L. Boyd, Robert F. Carlos, Joseph T. Ornstein (2023). “How the Trump Administration’s Quota Policy Transformed Immigration Judging”. American Political Science Review. [DOI] [PDF] [Dataverse]\nOrnstein, Joseph T. (2023). “Zone Defense: Why Liberal Cities Build Too Few Homes.” Journal of Theoretical Politics 6(2): 189-216. [DOI] [PDF]\nEinstein, Katherine Levine, Joseph T. Ornstein, & Maxwell Palmer (2022). “Who Represents the Renters?” Housing Policy Debate. [DOI] [PDF]\nOrnstein, Joseph T., Jude C. Hays and Robert J. Franzese (2022). “The Interest Premium for Left Government: Regression Discontinuity Estimates”. Economics & Politics 34(3): 429-443. [PDF] [Replication Materials]\nOrnstein, Joseph T., Hammond, Ross. A., Padek, Maggie, Mazzucca, Stephanie, & Brownson, Ross. C. (2020). “Rugged landscapes: complexity and implementation science”. Implementation Science, 15, 1-9. [PDF] [Technical Appendix]\nOrnstein, Joseph T. (2020). “Stacked Regression and Poststratification”. Political Analysis 28: 293-301. [DOI] [PDF] [Appendix] [Replication Materials] [R Package]\nLuke, Douglas A., Joseph T. Ornstein, Todd B. Combs, Lisa Henriksen, & Maggie Mahoney (2020). “Moving From Metrics to Mechanisms to Evaluate Tobacco Retailer Policies: Importance of Retail Policy in Tobacco Control”. American Journal of Public Health. 110: 431-433. [DOI] [PDF] [Dashboard]\nOrnstein, Joseph T. & Ross A. Hammond (2017). “The Burglary Boost: A Note On Detecting Contagion Using the Knox Test”. Journal of Quantitative Criminology, 33(1): 65-75. [DOI] [PDF] [NetLogo Code] [R Code]\nOrnstein, Joseph T. & Robert Z. Norman (2014). “Frequency of monotonicity failure under Instant Runoff Voting: estimates based on a spatial model of elections”. Public Choice, 161:1-9. [DOI] [PDF] [NetLogo Code] [Post-Publication Comments]\nHammond, Ross A. & Joseph T. Ornstein (2014). “A Model of Social Influence on Body Mass Index” Annals of the New York Academy of Sciences 1331:34-42. [PDF]\nHammond, Ross A. et al. (2012). “A Model of Food Reward Learning with Dynamic Reward Exposure”. Frontiers in Computational Neuroscience 6:82. [PDF]"
  },
  {
    "objectID": "publications.html#book-chapters",
    "href": "publications.html#book-chapters",
    "title": "Joseph T. Ornstein",
    "section": "Book Chapters",
    "text": "Book Chapters\n\n“Getting the Most Out of Surveys: Multilevel Regression And Poststratification” (Chapter 5). In Causality in Policy Studies (ed. Alessia Damonte & Fedra Negri), Springer Nature, 2023. [DOI]\n“Agent-Based Models in the Social Sciences” (Chapter 4) and “Agent-Based Models in Public Health” (Chapter 5), In New Horizons in Modeling and Simulation for Social Epidemiology and Public Health (ed. Daniel Kim). Wiley Press, 2021. [DOI]"
  },
  {
    "objectID": "publications.html#working-papers",
    "href": "publications.html#working-papers",
    "title": "Joseph T. Ornstein",
    "section": "Working Papers",
    "text": "Working Papers\n\nOrnstein, Joseph T., Elise N Blasingame, and Jake S. Truscott. “How to Train Your Stochastic Parrot: Large Language Models for Political Texts,” 2022. [PDF] [R Package]\nOrnstein, Joseph T. & JBrandon Duck-Mayr. “Gaussian Process Regression Discontinuity”. [PDF]\nOrnstein, Joseph T. “Election Timing and the Politics of Housing”. [PDF]"
  },
  {
    "objectID": "software/SRP/index.html",
    "href": "software/SRP/index.html",
    "title": "The SRP Package",
    "section": "",
    "text": "SRP is an R package that contains useful functions for implementing multilevel regression and poststratification (MRP) and stacked regression and poststratification (SRP)."
  },
  {
    "objectID": "software/SRP/index.html#motivation",
    "href": "software/SRP/index.html#motivation",
    "title": "The SRP Package",
    "section": "Motivation",
    "text": "Motivation\nSuppose we want to know how some public opinion varies by subnational unit. But we don’t have surveys that were conducted at the unit-level, only a national-level survey. How can we use the information from the national survey to make inferences about the subnational level? This vignette walks through three techniques for doing so: disaggregation, multilevel regression and poststratification (MRP), and stacked regression and poststratification (SRP). It concludes with an introduction to synthetic poststratification."
  },
  {
    "objectID": "software/SRP/index.html#installation",
    "href": "software/SRP/index.html#installation",
    "title": "The SRP Package",
    "section": "Installation",
    "text": "Installation\nThe SRP package is currently available on GitHub. You can install it using the devtools package. You’ll also want to load the tidyverse library for this vignette.\n\ndevtools::install_github('joeornstein/SRP')\nlibrary(SRP)\nlibrary(tidyverse)"
  },
  {
    "objectID": "software/SRP/index.html#the-data",
    "href": "software/SRP/index.html#the-data",
    "title": "The SRP Package",
    "section": "The Data",
    "text": "The Data\nThe dataset (trainset) contains individual-level data on public opinion and demographic characteristics. It is simulated data, generated from the Monte Carlo in Ornstein (2020). It contains the following variables:\n\ntrainset <- SRP::vignetteData\n\ntrainset\n#> # A tibble: 3,000 × 8\n#>         ID      y    x1    x2  unit latitude longitude unit_covariate\n#>      <int>  <dbl> <int> <int> <int>    <dbl>     <dbl>          <dbl>\n#>  1  638233 -2.68      2     1    43    0.663    0.140            2.22\n#>  2 1909280  2.01      3     3   128    0.987    0.504            2.64\n#>  3 2513825 -4.36      1     2   168    0.752    0.746            2.84\n#>  4 2968824 -0.989     2     3   198    0.764    0.190            3.27\n#>  5  258017  4.18      1     1    18    0.545    0.897            2.00\n#>  6 1252503 -4.54      3     3    84    0.770    0.195            2.43\n#>  7 1414783 -3.63      2     2    95    0.518    0.0937           2.47\n#>  8  602889  3.56      3     3    41    0.162    0.559            2.20\n#>  9  157609  0.271     2     2    11    0.431    0.554            1.93\n#> 10 2610832  5.99      4     4   175    0.898    0.932            2.91\n#> # ℹ 2,990 more rows\n\nThe variable \\(y\\) is our outcome of interest, \\(x_1\\) and \\(x_2\\) are individual-level covariates, unit is the subnational unit ID, and latitude, longitude, and unit_covariate are characteristics of the subnational unit."
  },
  {
    "objectID": "software/SRP/index.html#disaggregation",
    "href": "software/SRP/index.html#disaggregation",
    "title": "The SRP Package",
    "section": "Disaggregation",
    "text": "Disaggregation\nDisaggregation is the most straightforward method to estimate. Simply take the unit-level means from the national survey. Note, however, that the number of observations within each unit is fairly small. As a result, disaggregation is unlikely to yield good estimates. This is why we adopt a model-based approach.\n\ndisag_estimates <- trainset %>% \n  group_by(unit) %>% \n  summarise(disag_estimate = mean(y),\n            num = n())\n\ndisag_estimates\n#> # A tibble: 200 × 3\n#>     unit disag_estimate   num\n#>    <int>          <dbl> <int>\n#>  1     1          -6.15    14\n#>  2     2           2.61    14\n#>  3     3          -4.10    16\n#>  4     4          -3.45    15\n#>  5     5          -3.63    10\n#>  6     6          -5.40    21\n#>  7     7          -2.74    15\n#>  8     8          -2.95    18\n#>  9     9          -1.61    15\n#> 10    10          -1.48    16\n#> # ℹ 190 more rows"
  },
  {
    "objectID": "software/SRP/index.html#mrp",
    "href": "software/SRP/index.html#mrp",
    "title": "The SRP Package",
    "section": "MRP",
    "text": "MRP\nMultilevel regression and poststratification (MRP) was introduced by Gelman and Little (1997) and refined by Park, Gelman, and Bafumi (2004). It proceeds in two steps:\n\nEstimate a multilevel regression, predicting opinion using observed individual-level and unit-level covariates.\nPoststratify by taking the mean of each group’s prediction weighted by their frequency in the subnational unit.\n\nWe can estimate the first-stage regression using the lme4 package.\n\nlibrary(lme4)\n\nmodel1 <- lmer(y ~ (1|x1) + (1|x2) + unit_covariate + (1|unit), data = trainset)\n\nFor the second stage, we need a poststratification frame. For the SRP package, it should come in the following format.\n\nPSFrame <- SRP::vignettePSFrame\n\nPSFrame\n#> # A tibble: 3,200 × 7\n#>     unit    x1    x2  freq unit_covariate latitude longitude\n#>    <int> <int> <int> <int>          <dbl>    <dbl>     <dbl>\n#>  1     1     1     1  5482           1.54    0.623    0.0647\n#>  2     1     1     2  2497           1.54    0.623    0.0647\n#>  3     1     1     3   499           1.54    0.623    0.0647\n#>  4     1     1     4    43           1.54    0.623    0.0647\n#>  5     1     2     1  2418           1.54    0.623    0.0647\n#>  6     1     2     2  1817           1.54    0.623    0.0647\n#>  7     1     2     3   603           1.54    0.623    0.0647\n#>  8     1     2     4    58           1.54    0.623    0.0647\n#>  9     1     3     1   557           1.54    0.623    0.0647\n#> 10     1     3     2   590           1.54    0.623    0.0647\n#> # ℹ 3,190 more rows\n\nEach row reports the empirical frequency for each unique combination of individual-level characteristics, repeated for each subnational unit. For example, the first row reports that there are 5482 individuals with \\(x_1 = 1\\) and \\(x_2 = 1\\) in Unit 1.\nOnce we have both pieces of information – the predictions and the frequencies – poststratification simply requires taking a weighted average, using the poststratify function. Note that this function requires your poststratification frame to have two particular variables:\n\nunit: the identity of the subnational unit.\nfreq: the empirical frequency for each cell.\n\n\npred <- predict(model1, PSFrame, allow.new.levels = T)\n\nmrp_estimates <- poststratify(pred, PSFrame)\n\nmrp_estimates\n#> # A tibble: 200 × 2\n#>     unit poststratifiedEstimate\n#>    <int>                  <dbl>\n#>  1     1                  -3.54\n#>  2     2                  -1.38\n#>  3     3                  -2.72\n#>  4     4                  -2.49\n#>  5     5                  -2.39\n#>  6     6                  -2.96\n#>  7     7                  -2.06\n#>  8     8                  -2.27\n#>  9     9                  -1.76\n#> 10    10                  -1.66\n#> # ℹ 190 more rows"
  },
  {
    "objectID": "software/SRP/index.html#srp",
    "href": "software/SRP/index.html#srp",
    "title": "The SRP Package",
    "section": "SRP",
    "text": "SRP\nStacked regression and poststratification (SRP) proceeds in the same fashion as MRP, but the first-stage predictions come from an ensemble model average generated through stacking. See Ornstein (2020) for technical details.\nTo start, we must tune and estimate each of the component models separately. The following code estimates a hierarchical linear regression model, LASSO, random forest, KNN, and gradient boosting.\n\nlibrary(glmnet)\nlibrary(ranger)\nlibrary(kknn)\nlibrary(xgboost)\nlibrary(caret)\n\n\n#Estimate HLM\nhlmFormula <- y ~ (1|x1) +  (1|x2) + unit_covariate + (1|unit) \nhlmModel <- lmer(hlmFormula, data = trainset)\n\n#Tune LASSO\nlasso_vars <- c(\"x1\",\"x2\",\"unit\",\"unit_covariate\")\nlasso_factors <- c('x1', 'x2', 'unit') #which variables to convert to factors\ntrainset_lasso <- cleanDataLASSO(trainset, lasso_vars, lasso_factors)$trainset\n\nlassoModel <- cv.glmnet(trainset_lasso, trainset$y, \n                        type.measure = \"mse\") \n\n#Tune KNN\nknnFormula <- y ~ x1 + x2 + latitude + longitude + unit_covariate\nknn_train <- train.kknn(knnFormula, data=trainset, kmax = 201) #Find best k (LOOCV)\nk_best <- knn_train$best.parameters$k\n\n#Tune Random Forest\nforestFormula <- y ~ x1 + x2 + latitude + longitude + unit_covariate\nforestModel <- ranger(forestFormula, data = trainset)\n\n#Tune GBM\ngbm_vars <- c(\"x1\",\"x2\",\"latitude\",\"longitude\",\"unit_covariate\")\ntrainset_gbm <- cleanDataGBM(trainset=trainset, gbm_vars=gbm_vars)$trainset\n#Create a custom 'xgb.DMatrix'. Faster computation\ndtrain <- xgb.DMatrix(trainset_gbm, label = trainset$y)\n\n#5-fold cross-validation; pick nrounds that minimizes RMSE\nxgb.tune <- xgb.cv(data = dtrain, \n                   booster = \"gbtree\",\n                   objective = \"reg:squarederror\",\n                   eval_metric = \"rmse\",\n                   eta = 0.02,\n                   nrounds = 50 / 0.02, #Lower eta -> more trees\n                   nfold = 5, \n                   verbose = F,\n                   early_stopping_rounds = 20)\n\ngbmModel <- xgboost(data = dtrain, \n                    booster = \"gbtree\",\n                    objective = \"reg:squarederror\",\n                    eval_metric = \"rmse\",\n                    eta = 0.02,\n                    verbose = F,\n                    nrounds = xgb.tune$best_iteration)\n\nNext, we will use the getStackWeights() function to estimate the optimal ensemble model average weights using 5-fold cross-validation.\n\nstackWeights <- getStackWeights(trainset = trainset,\n                                hlmFormula = hlmFormula,\n                                lasso_vars = lasso_vars,\n                                lasso_factors = lasso_factors,\n                                forestFormula = forestFormula,\n                                knnFormula = knnFormula, k_best = k_best,\n                                gbm_vars = gbm_vars, gbm_factors = NULL, \n                                gbm_params = list(eta = 0.02), gbm_tune = xgb.tune, \n                                nfolds = 5)\n\nstackWeights %>% round(3)\n#> [1] 0.322 0.000 0.098 0.000 0.581\n\nThen we can poststratify as before.\n\nPSFrame_lasso <- cleanDataLASSO(PSFrame, lasso_vars = lasso_vars, lasso_factors = lasso_factors,\n                                new_vars_lasso = colnames(trainset_lasso))$trainset\nPSFrame_gbm <- cleanDataGBM(PSFrame, gbm_vars = gbm_vars)$trainset\n\nM1 <- predict(hlmModel, PSFrame, allow.new.levels = T)\nM2 <- predict(lassoModel, newx = PSFrame_lasso, s = lassoModel$lambda.min)\nM3 <- kknn(knnFormula, train = trainset, test = PSFrame, k = k_best)$fitted.values\nM4 <- predict(forestModel, PSFrame)$predictions\nM5 <- predict(gbmModel, PSFrame_gbm)\nM <- cbind(M1,M2,M3,M4,M5) %>% as.matrix\n\npred <- M %*% stackWeights\n\n#Poststratify\nsrp_estimates <- poststratify(pred, PSFrame)\n\nhead(srp_estimates)\n#> # A tibble: 6 × 2\n#>    unit poststratifiedEstimate\n#>   <int>                  <dbl>\n#> 1     1                 -0.180\n#> 2     2                  0.780\n#> 3     3                 -3.13 \n#> 4     4                 -2.95 \n#> 5     5                  0.236\n#> 6     6                 -2.09"
  },
  {
    "objectID": "software/SRP/index.html#results",
    "href": "software/SRP/index.html#results",
    "title": "The SRP Package",
    "section": "Results",
    "text": "Results\nBecause the data came from a simulation, we also know the true unit-level means. Let’s see how our estimates compare."
  },
  {
    "objectID": "software/SRP/index.html#synthetic-poststratification",
    "href": "software/SRP/index.html#synthetic-poststratification",
    "title": "The SRP Package",
    "section": "Synthetic Poststratification",
    "text": "Synthetic Poststratification\nWhat if you do not have the joint frequency distribution for all your predictor variables at the subnational level? Leemann and Wasserfallen (2017) propose a method that instead uses marginal frequency distributions called synthetic poststratification. The approach proceeds by multiplying the marginal probabilities to create a synethtic joint distribution, assuming that the predictor variables are statistically independent.\nNote that this is a strong assumption. However, if the first-stage model is additively-separable, then both classical and synthetic poststratification produce identical results (see Appendix A in Ornstein (2020) for the proof). This implies that Multilevel Regression and Synthetic Poststratification (MrsP) can produce strictly superior estimates when the first-stage model is linear-additive, because one can include more predictor variables.\nThe SRP package provides a function that can generate synthetic poststratification frames, called getSyntheticPSFrame().\n\nUsing the Function\nSuppose you have two (non-synthetic) frequency distributions describing the same population.\n\nPSFrame1 <- SRP::race\nPSFrame2 <- SRP::education\n\nPSFrame1\n#> # A tibble: 8 × 3\n#>    unit  race  freq\n#>   <int> <int> <int>\n#> 1     1     1   100\n#> 2     1     2   200\n#> 3     1     3   300\n#> 4     1     4   400\n#> 5     2     1   200\n#> 6     2     2   100\n#> 7     2     3   300\n#> 8     2     4   400\nPSFrame2\n#> # A tibble: 6 × 3\n#>    unit education  freq\n#>   <int>     <int> <int>\n#> 1     1         1   250\n#> 2     1         2   300\n#> 3     1         3   450\n#> 4     2         1   450\n#> 5     2         2   350\n#> 6     2         3   200\n\nTo get the synthetic joint distribution, just call getSyntheticPSFrame(). Note that the resulting output is consistent with the marginal frequency distributions; add up all the observations with race == 1 in and it should yield the same frequency from PSFrame1.\n\nPSFrame <- getSyntheticPSFrame(PSFrame1, PSFrame2)\n\nPSFrame\n#> # A tibble: 24 × 4\n#>     unit  race education  freq\n#>    <int> <int>     <int> <dbl>\n#>  1     1     1         1    25\n#>  2     1     1         2    30\n#>  3     1     1         3    45\n#>  4     1     2         1    50\n#>  5     1     2         2    60\n#>  6     1     2         3    90\n#>  7     1     3         1    75\n#>  8     1     3         2    90\n#>  9     1     3         3   135\n#> 10     1     4         1   100\n#> # ℹ 14 more rows\n\nIf you want to generate a synthetic poststratification frame from more than one marginal distribution, simply repeat the process.\n\nPSFrame3 <- SRP::sex\n\nPSFrame3\n#> # A tibble: 4 × 3\n#>    unit   sex  freq\n#>   <int> <int> <int>\n#> 1     1     1   600\n#> 2     1     2   400\n#> 3     2     1   500\n#> 4     2     2   500\n\nPSFrame <- getSyntheticPSFrame(PSFrame, PSFrame3)\n\nPSFrame\n#> # A tibble: 48 × 5\n#>     unit  race education   sex  freq\n#>    <int> <int>     <int> <int> <dbl>\n#>  1     1     1         1     1    15\n#>  2     1     1         1     2    10\n#>  3     1     1         2     1    18\n#>  4     1     1         2     2    12\n#>  5     1     1         3     1    27\n#>  6     1     1         3     2    18\n#>  7     1     2         1     1    30\n#>  8     1     2         1     2    20\n#>  9     1     2         2     1    36\n#> 10     1     2         2     2    24\n#> # ℹ 38 more rows"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Joseph T. Ornstein",
    "section": "",
    "text": "text2data : Functions for formatting and completing prompts to Large Language Models. Useful for text classification and topic modeling.\ngprd (work in progress): Regression discontinuity estimates using Gaussian Process\nSRP : Stacked regression and poststratification (vignette)\nrdviz : Regression discontinuity visualizations"
  },
  {
    "objectID": "software.html#misc",
    "href": "software.html#misc",
    "title": "Joseph T. Ornstein",
    "section": "Misc",
    "text": "Misc\n\nDraw A Random Sample: An interactive Shiny app that does exactly what it says on the tin. Very useful for teaching about probability and sampling. Hard to believe I couldn’t find something like this anywhere else on the Internet!"
  }
]